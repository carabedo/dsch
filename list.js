clases=[`    Esta clase va a ser
grabad
  a
      Clase 19. DATA SCIENCE 
     Algoritmos y 
    Validación de 
Modelos de Machine 
       Learning
Temario
              18                   19                    20
       Introducción al        Algoritmos y             Stack 
        ML y a la IA          validación de         tecnológico I
                             modelos de ML
        ✓ Introducción         ✓ Conceptos 
        ✓ Tipos de IA             básicos           ✓ Base de 
        ✓ Aplicaciones         ✓ Aprendizaje           datos 
           de la                  y validación      ✓ Lenguajes 
           industria           ✓ Métricas y            DS
        ✓ Riesgos                 Evaluación        ✓ Visualización 
           asociados
Objetivos de la clase
         Reconocer los conceptos básicos asociados a 
         Machine Learning.
         Identificar las principales métricas para 
         evaluar la performance de un modelo.
MAPA DE CONCEPTOS
                            Historia de la 
                            IA
                            Clasificación 
          ML e 
        Inteligencia        IA y Robótica   RPA
         Artificial
                            IA en industrias
                             Programas y 
                              Algoritmos
                               GPT - 3
Repaso 
      MAPA DE CONCEPTOS                            Algoritmos y 
                                                  Validación de 
                                               Modelos de Machine 
                                                     Learning
           Conceptos                Aprendizaje y               Aprendizaje y                Métricas y 
            básicos                   Validación                  Validación                modelos de 
                                                                                             regresión
         Dataset                    Overfitting                Matriz de                   RMSE
                                                               confusión
         Registro                   Underfitting               Exactitud                   MAE
         Atributo                                              Precisión                   R2
         Objetivo                                              Sensibilidad
         Ingeniería de                                         Especificidad
         Factores
         Outliers                                              F1 - Score
PARA RECORDAR
Inteligencia Artificial
La evaluación de modelos es un aspecto 
fundamental y crítico en todo flujo de Data 
Science. Pero antes de hablar de métricas 
de performance, resulta importante 
entender algunos conceptos básicos y 
esenciales del Machine Learning. 
Empecemos������
Conceptos básicos
Dataset, instancia, 
característica y 
variable objetivo
Dataset: 
conjunto 
✓ Materia prima del sistema de 
de datos
predicción. 
✓ Histórico de datos que se usa para 
entrenar al sistema que detecta los 
patrones. 
✓ El conjunto de datos se compone de 
instancias de factores, 
características o propiedades.
Instancia
✓ Cada uno de los datos de los que se 
disponen para hacer un análisis. 
✓ Cada instancia a su vez, está 
compuesta de características que la 
describen.
✓ En una hoja de cálculo, las 
instancias serían las filas; las 
características, las columnas.
Características
✓ Atributos que describen 
cada una de las instancias 
del conjunto de datos. 
✓ En una hoja de cálculo, 
serían las columnas.
Variable objetivo
Atributo o factor que queremos predecir, el 
objetivo de la predicción, como puede ser la 
probabilidad de reingreso de un paciente tras 
una intervención quirúrgica.
Ingeniería de 
factores
(Feature 
Engineering)
Ingeniería de Factores
✓ Proceso previo a la creación del 
modelo en el que se hace análisis, 
limpieza y estructuración de los datos. 
✓ El objetivo es eliminar los campos que 
no sirven para hacer la predicción y 
organizarlos adecuadamente para que 
el modelo no reciba información que 
no le es útil y que podría provocar 
predicciones de poca calidad o 
confianza. 
������ Este proceso es uno de los más 
importantes y más costosos del proceso de 
predicción. 
Datos perdidos
 ✓ Es muy habitual encontrarnos con valores      Pero ¿qué podemos hacer en estos 
    perdidos en estos procesos.                  casos? Existen múltiples técnicas para 
 ✓ Pueden aparecer de distintas formas:          tratar los valores missing, lo veremos 
    como un signo de interrogación, o N/A,       más adelante en curso ������
    como un 0 o simplemente como una celda 
    en blanco, pero en su mayoría nos lo 
    encontramos representado como NaN que 
    se refiere a “no un número”. 
¿Qué es un outlier?
Valores extremos
Valor que no se corresponde con el patrón 
general de nuestros datos. Puede ser bueno, 
malo o simplemente un error de datos pero en 
todos esos casos tenemos que realizar un 
análisis.
¿Por qué es 
importante tratar los 
outliers?
En términos generales, tratar los outliers suele 
mejorar los modelos de ML. 
Muchos modelos avanzados son sensibles a los 
valores extremos y además, siempre es preferible 
realizar una buena preparación de datos antes 
que complejizar los modelos.
Para pensar
¿Cómo podemos evaluar si nuestro modelo 
está aprendiendo correctamente de 
nuestros datos?¿Por qué es necesario usar 
nuevas instancias y no limitarse a aquellas 
con las que se entrenó el modelo? 
Contesta mediante el chat de Zoom 
Evaluación del modelo
¿Cómo podemos evaluar si 
nuestro modelo está aprendiendo 
correctamente de nuestros datos?
������ Una respuesta posible sería, que para 
evaluar si nuestro modelo aprendió o no 
de nuestro datos, observemos su 
desempeño o performance frente a 
nuevas instancias es decir, frente a datos 
que nunca vio. 
Nuevas instancias
¿Por qué es necesario usar nuevas 
instancias y no sólo aquellas con las que 
se entrenó el modelo?
La respuesta es sencilla, por que no se puede 
ser “Juez y parte” al mismo tiempo. ������
A partir de este concepto, surgen las 
siguientes características en ML: 
“Entrenamiento” y “Validación” para luego 
hablar del “Sobreajuste” o “Sub-ajuste”. 
Ejemplo en vivo
¿Cómo podríamos utilizar el feature engineering para 
encontrar las variables más relevantes en los precios 
de inmuebles?
Estudiaremos un ejemplo aplicado donde podremos 
ver el uso de feature selection enfocado en Wrapper 
Methods (forward, backward y stepwise)
Utilizaremos el notebook  Clase_19.ipynb 
dentro de la carpeta de clase.
Aprendizaje y 
Validación   
Entrenamiento y 
validación
Nuevas instancias
¿Por qué es necesario usar nuevas 
instancias y no sólo aquellas con las que 
se entrenó el modelo?
La respuesta es sencilla, por que no se puede 
ser “Juez y parte” al mismo tiempo. ������
A partir de este concepto, surgen las 
siguientes características en ML: 
“Entrenamiento” y “Validación” para luego 
hablar del “Sobreajuste” o “Sub-ajuste”. 
Aprendizaje o 
Entrenamiento
✓ Proceso en el que se detectan los 
patrones de un conjunto de datos, 
es decir, es el corazón del machine 
learning. 
✓ Cuando identificamos los patrones, se 
pueden hacer predicciones con 
nuevos datos que se incorporen al 
sistema.
Para pensar
¿Cómo podemos utilizar la información de 
la compra de libros, por ejemplo, respecto 
al comportamiento de los clientes para 
mejorar las utilidades del negocio? 
Contesta mediante el chat de Zoom 
Ejemplo
Los datos de las compras de libros 
online se pueden usar para analizar el 
comportamiento de los clientes en sus 
procesos de compra (títulos visitados, 
categorías, historial de compras, etc) 
agruparlos en patrones de 
comportamiento y hacer 
recomendaciones de compra.
Validación
✓ Proceso de evaluar un modelo 
entrenado sobre un conjunto de datos 
de prueba. Esto proporciona la capacidad 
de generalización de un modelo de ML. 
✓ Para poder evaluarlo correctamente, hay 
que realizar “split de datos” es decir, 
separar nuestro dataset original en 
“Datos de Entrenamiento”, que serán 
usados justamente para entrenar a nuestro 
modelo y en “Datos de Test o de 
Testing” que serán aquellos datos que 
utilizaremos para evaluar la performance 
de nuestro modelo. 
¿Qué porcentaje se usa 
para train y test? 
������ No existe una única respuesta, en términos generales se suele utilizar un 70 % 
de nuestros datos para el training y un 30 % para el testing.  
¿Qué porcentaje se 
usa para train y 
test? 
Training : Datos para ajustar el modelo
Validation: Datos para proporcionar una 
evaluación imparcial de un modelo que se 
ajusta al conjunto de datos de entrenamiento 
mientras se ajustan los hiper parámetros del 
modelo. 
Test: Datos para proporcionar una evaluación 
imparcial de un modelo final que se ajusta al 
conjunto de datos de entrenamiento.
Validación Cruzada
✓ También conocida como Cross-
Validation, separa los datos en 
diferentes particiones y obtiene la media 
de las evaluaciones de las distintas 
particiones. 
✓ Ayuda a evaluar los resultados que 
devuelve el modelo y garantizar la 
independencia de las particiones que 
hacemos, con lo cual se evita el 
sobreajuste.  
Overfitting y 
Underfitting
Para pensar
Considerando los siguientes 3 escenarios, 
¿qué modelo parece ser el mejor?
Contesta mediante el chat de Zoom 
Overfitting y Underfitting
El modelo a es muy simple y no reproduce correctamente la frontera 
entre las clases. Esto se conoce como Underfitting o Sub-Ajuste.
Overfitting y Underfitting
 El modelo a es muy simple  El modelo b tiene la 
 y no reproduce               complejidad suficiente para 
 correctamente la frontera    encontrar una frontera que 
 entre las clases. Esto se    parece ser la apropiada 
 conoce como Underfitting     en base al dataset 
 o Sub-Ajuste.                analizado.
Overfitting y Underfitting
 El modelo a es muy simple  El modelo b tiene la              El modelo c se adaptó 
 y no reproduce                 complejidad suficiente para   demasiado a los datos con los 
 correctamente la frontera      encontrar una frontera que    que fue entrenado. Esto se 
 entre las clases. Esto se      parece ser la apropiada       conoce como Overfitting o 
 conoce como Underfitting       en base al dataset            Sobre – Ajuste.
 o Sub-Ajuste.                  analizado.
Overfitting y Underfitting
 ✓ Las principales causantes de obtener        ✓ Tanto el Over como el Under – Fitting, 
     malos resultados en Machine Learning         se relacionan al fallo de nuestro 
     son el Overfitting o el Underfitting         modelo al generalizar -encajar- el 
     de los datos. Dado que cuando                conocimiento que pretendíamos que 
     entrenamos nuestro modelo intentamos         adquieran.
     “hacer encajar” -fit en inglés- los datos 
     de entrada entre ellos y con la salida. 
¿Cómo prevenir el 
✓ Sucede cuando nuestro modelo aprende los datos de train 
Overfitting?
perfectamente, por lo que no es capaz de generalizar y 
cuando le lleguen nuevos datos obtiene pésimos resultados. 
������
✓ Existen diferentes formas de prevenir el Overfitting: 
○ Dividir nuestros datos en training, validación y testing.
○ Obtener un mayor número de datos.
○ Ajustar los parámetros de nuestros modelos.
○ Utilizar modelos más simples en caso de ser posible 
 (PARSIMONIA).
¿Y el Underfitting?
✓ Sucede cuando nuestro modelo no es capaz de 
identificar patrones. Por lo que tendrá siempre 
pésimos resultados. ������
✓ Existen diferentes formas de prevenir el 
Underfitting:
○ Tratar los datos correctamente, 
eliminando outliers y variables 
innecesarias.
○ Utilizar modelos más complejos.
○ Ajustar los parámetros de nuestros 
modelos.
           Diferencias
                REEMPLAZAR 
                 POR VIDEO
Overfitting y 
Underfitting
En el Machine Learning
Métricas y 
evaluación
Métricas y 
Evaluación
Resulta importante comenzar a hablar acerca 
de las diferentes métricas que existen 
dentro del Machine Learning para 
evaluar la performance de nuestro 
modelo. 
Simplemente realizaremos una primera 
aproximación a la temática, en próximas 
clases el tema de: Validación de resultados 
del Modelo y Tuneo se verá y tratará de 
manera detallada. ������
Métricas para 
Algoritmos 
de Clasificación 
Matriz de Confusión
✓ Herramienta que permite visualizar el 
desempeño de un algoritmo  de 
aprendizaje supervisado. 
✓ Cada columna de la matriz representa el 
número de predicciones de cada clase, 
mientras que cada fila representa a las 
instancias en la clase real. 
En términos prácticos entonces, nos 
permite ver qué tipos de aciertos y 
errores está teniendo nuestro modelo.
Matriz de Confusión
                    Interpretación:
                     ✓ Verdadero Positivo (TP): Predije que era 
                        positivo y lo era.
                     ✓ Verdadero Negativo (TN): Predije que era 
                        falso y lo era.
                     ✓ Falso Positivo (FP): Predije que era positivo 
                        pero resultó ser negativo.
                     ✓ Falso Negativo (FN): Predije que era negativo 
                        pero resultó siendo positivo.
  Los Verdaderos Positivos como Negativos son aciertos. Los Falsos Negativos como Positivos son 
  errores.
 Ejemplo Titanic
                                           Clase Predicha
       a                        No Sobrevivieron    Sobrevivieron
       r
     e e
     s d     No Sobrevivieron         513                110
     a a
     l d
       r
     C e
       V     Sobrevivieron            103                283
Matriz de confusión 
y sus métricas
Métricas para evaluación 
de clasificadores
                                   MÉTRICAS
Exactitud            Precisión          Sensibilidad       Especificidad          F1 Score
La Exactitud o Accuracy
              ������ se refiere a lo cerca que está el resultado 
              de una medición del valor verdadero. En 
              términos estadísticos, la exactitud está 
              relacionada con el sesgo de una estimación. Se 
              representa por la proporción entre los positivos 
              reales predichos por el algoritmo y todos los 
              casos positivos.
              En forma práctica la Exactitud es  el % total de 
              elementos clasificados correctamente.
              (VP+VN)/(VP+FP+FN+VN) * 100
Precisión (Positive 
Predictive rate)
             ������ Se refiere a la dispersión del conjunto de 
             valores obtenidos a partir de mediciones 
             repetidas de una magnitud. Cuanto menor es la 
             dispersión mayor la precisión. Es una proporción 
             entre el número de predicciones correctas (tanto 
             positivas como negativas) y el total de 
             predicciones. En forma práctica, es  el porcentaje 
             de casos positivos detectados y nos sirve para 
             medir la calidad del modelo de ML en tareas de 
             clasificación.
             Se calcula como:  VP/(VP+FP)
Sensibilidad o Tasa 
de Verdaderos 
Positivos ������ Es la proporción de casos positivos que fueron 
              correctamente identificadas por el algoritmo.
              En términos prácticos sería la capacidad de una 
              prueba para identificar correctamente a las 
              personas con la característica (e.g. enfermedad) 
              Se calcula:  VP/(VP+FN) o lo que sería igual en 
              términos de salud:  Verdaderos positivos 
Especificidad - Tasa de 
Verdaderos Negativos
          ������ Se trata de los casos negativos que el algoritmo ha clasificado 
          correctamente.  Expresa cuán bien puede el modelo detectar esa 
          clase.
          En términos prácticos es la capacidad de la prueba para identificar 
          correctamente a las personas sin la característica (e.g enfermedad) 
          Se calcula:  VN/(VN+FP) o en términos de salud:  Verdaderos 
          Negativos 
En resumen
F1 – Score
������ Esta es otra métrica muy empleada porque        Los valores típicos están entre 0 y 1.
nos resume la Precisión (Precisión) y 
Sensibilidad (Recall) en una sola métrica.         Se calcula:  
Es una medida general del desempeño de un          2 * (Recall * Precision) / (Recall + Precision)
modelo combinando Precisión y Sensibilidad.
Un valor alto indica pocos Falsos Positivos y 
pocos Falsos Negativos, identificando las 
amenazas reales. 
Algunas 
consideraciones de 
F1 - Score                                      ✓ Baja precisión y bajo recall ������ El modelo 
✓ Alta precisión y alto recall ������ el modelo 
   maneja perfectamente esa clase.              no logra clasificar la clase correctamente.
✓ Alta precisión y bajo recall ������ el modelo no 
   detecta la clase muy bien, pero cuando lo 
   hace es altamente confiable.
✓ Baja precisión y alto recall ������ El modelo 
   detecta bien la clase,  pero también 
   incluye muestras de la otra clase.
PARA RECORDAR
Inteligencia Artificial
Por último hablamos sobre algunas métricas para 
evaluación de Modelos de Regresión. Recordemos que 
aquí, predecimos o estimamos el valor numérico de una 
cantidad desconocida, de acuerdo con unas 
características dadas. 
La diferencia entre la predicción y el valor real es 
el Error, este es una variable aleatoria, que puede 
depender de las características dadas. ������
Para pensar
Si tuviéramos que cuantificar el 
desempeño de diferentes pruebas para 
detectar  COVID ¿Qué métrica sería la 
apropiada y por qué? 
Contesta mediante el chat de Zoom 
Métricas para Algoritmos 
de Regresión 
Métricas para algoritmos 
de Regresión
 En la actualidad hay muchas formas para        Existen varias métricas más como ser 
estimar el rendimiento y evaluar el ajuste      por ejemplo, el R cuadrado ajustado (R²), 
     del modelo de regresión, las más           MSPE – Error de porcentaje cuadrático 
            importantes son:                    medio, entre otras.
 ✓ Error Cuadrático Medio (RMSE, por 
    sus  siglas  en  inglés,  Root  Mean 
    Squared Error).
 ✓ Error  Absoluto  Medio  (MAE,  Mean 
    Absolute Error). 
 ✓ R-Cuadrado.
Error cuadrático medio 
(RMSE)
������ Es la métrica más comúnmente utilizada 
para las tareas de regresión y representa a 
la raíz cuadrada de la distancia 
cuadrada promedio entre el valor real 
y el valor pronosticado.
Indica el ajuste absoluto del modelo a los 
datos, cuán cerca están los puntos de 
datos observados de los valores 
predichos del modelo.
Error absoluto 
medio (MAE)
������ Es la diferencia absoluta entre el valor 
objetivo y el valor predicho por el modelo. Es 
más robusto para los valores atípicos y no penaliza 
los errores tan extremadamente como el MSE.  
Este tipo de métrica, no es adecuada para 
aplicaciones en las que desea prestar más atención a 
los valores atípicos.
R2
������ indica la bondad o la aptitud del modelo, a 
menudo se utiliza con fines descriptivos y muestra 
que también las variables independientes 
seleccionadas explican la variabilidad en sus 
variables dependientes. 
R-cuadrado tiene la propiedad útil de que su 
escala es intuitiva, va de 0 a 1, con 0 indicando 
que el modelo propuesto no mejora la predicción 
sobre el modelo medido y 1 indica una 
predicción perfecta. 
Para pensar
Imaginemos que estamos esperando un mail 
importante y se categoriza como spam ¿Cuál es el 
problema? ¿Cómo podemos utilizar las métricas para 
reducir el margen de error? 
Contesta mediante el chat de Zoom 
Ejemplo en vivo
Exploremos cómo se obtienen las diversas métricas 
para evaluar el desempeño de un modelo de 
clasificación y regresión.
Utilizaremos el notebook  Clase_19.ipynb 
dentro de la carpeta de clase.
        Recordemos…
                  Generamos recomendaciones 
                  basados en insights obtenidos
 Clase 17         Definimos Objetivo, Contexto y 
Estructurando un    Problema comercial
Proyecto DS- Parte II
                  Contexto analítico, Limpieza 
                  de datos y EDA 
                  Obtenemos conclusiones y 
                  puntos importantes a resaltar
                    5
 Práctica integradora 
Deberás entregar el quinto avance de tu proyecto 
final. Continuaremos hablando sobre lo trabajado en 
el desafío “Estructurando un proyecto de DS Parte II”. 
     DESAFÍO 
     ENTREGABLE
Estructurando un proyecto de 
DS-parte III
Consigna                                               Aspectos a incluir
✓ Crearás un notebook que complemente el              ✓ El código debe estar hecho en un 
  trabajo realizado en los siguientes                   notebook y debe estar probado.
  apartados:                                       Formato
    -   i) elegir un método de feature              ✓ Entregar un archivo con formato .ipynb. 
        selection para reducir la                       Debe tener el nombre 
        dimensionalidad del dataset,                    “Proyecto_ParteIII_+Apellido.ipynb”  
    -   ii) elegir un algoritmo de regresión o 
        clasificación para entrenar con los 
        datos elegidos,                            Sugerencias
    -    iii) cálculo de métricas para validar      ✓ Preparar el código y probar los 
        el modelo                                       resultados con subconjuntos del 
    -   iv) generar conclusiones con base en            conjunto original.
        los resultados obtenidos.                   ✓ Video explicativo
Evaluando modelos 
         ML
Se propone complementar el análisis desarrollado 
 hasta el momento del proyecto final. 
     DESAFÍO 
     COMPLEMENTARIO
Evaluando modelos ML
Consigna
 ✓ Continuaremos trabajando con base              Aspectos a incluir
     en lo realizado en el Desafío                  ✓ El código debe estar hecho en un notebook 
     entregable: Estructurando un                       y debe estar probado.
     proyecto de DS-Parte II y III, en esta       Formato
     oportunidad deberás complementar 
     con lo siguiente:                              ✓ Entregar un archivo con formato .ipynb. 
 ✓ Generar una evaluación de modelos                    Debe tener el nombre 
     apropiados para el problema de                     “Proyecto_ComplementarioI_+Apellido.ipyn
     interés                                            b”  
 ✓ Identificar por medio de las métricas          Sugerencias
     generadas si se puede tener una                ✓ Preparar el código y probar los resultados 
     situación de overfitting (sobreajuste)             con subconjuntos del conjunto original.
     o underfitting (subajuste), 
     discutiendo posibles formas de 
     mejora
Para pensar
¿Cómo podemos utilizar la información de la compra 
de libros, por ejemplo, respecto al comportamiento de 
los clientes para mejorar las utilidades del negocio? 
Contesta mediante el chat de Zoom 
CLASE N°19                                    Conjunto de validación: fracción de datos 
                                             (usualmente 20-30%) que se utiliza para validar 
                                             algoritmos de Machine Learning supervisado con 
Glosario                                      el fin de identificar si el modelo aprendió 
                                             correctamente
Instancia : unidad fundamental que representa 
a los individuos u objetos que conforman un       Overfitting: cuando un modelo obtiene muy 
dataset                                           buenas métricas en el conjunto de 
                                             entrenamiento pero muy malas en el conjunto de 
Característica o feature : variables que          test
representan los atributos de las instancias de 
un dataset                                        Underfitting: Cuando el modelo no es capaz de 
                                             reproducir correctamente los patrones y 
Entrenamiento: fase donde se detectan las         relaciones fundamentales del fenómeno de 
asociaciones y tendencias de un dataset           interés.
Conjunto de entrenamiento: fracción de            Matriz de confusión:matriz que se construye 
datos (usualmente 70-80%) que se utiliza para     para validar el performance de un modelo de 
entrenar algoritmos de Machine Learning           clasificación, contiene información sobre el 
supervisado con el fin de entender patrones y     accuracy, precisión, exactitud, sensibilidad y 
tendencias                                        especificidad del algoritmo
                                             Dataset: conjunto de filas y columnas que 
                                             guardan información histórica 
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
MATERIAL AMPLIADO
Recursos multimedia
✓ Hoja de referencia de consejos y trucos sobre Aprendizaje 
Automático
| Stanford Edu
✓ Selección de Métricas para aprendizaje automático | 
Fayrix
✓ Aprendizaje automático y métricas de regresión | 
Sitiobigdata.com
✓ Error Cuadrático Medio para Regresión | Iartificial.net
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Conceptos básicos de ML
      ✓ Métricas de Clasificación
      ✓ Regresión en Machine Learning
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 40. DATA SCIENCE
Introducción a Machine 
        Learning
Temario
                  39                         40                         41
              En foco...             Introducción a             Algoritmos de 
                                         Machine                 clasificación
                                         Learning
            ✓ Data                   ✓  Marco CRISP-DM y la fase 
               Acquisition              de Machine Learning y 
                                        modelado                 ✓ KNN
            ✓ Data Wrangling         ✓  Scikit Learn             ✓ Random Forest
            ✓ EDA                    ✓  Técnicas de Encoding     ✓ Regresión 
                                     ✓  Feature Engineering          Logística 
            ✓ Análisis 
               estadístico           ✓  Flujo de trabajo 
Objetivos de la clase
         Introducir el concepto de Machine Learning
         Enfatizar la importancia del encoding de 
         variables y la ingeniería de atributos
MAPA DE CONCEPTOS
  Scikit Learn           Marco CRISP-           Ingeniería de 
                            DM                    atributos
                      Introducción a 
                         Machine 
                         Learning
  Encoding                                      Flujo de trabajo
    Repaso
             Les proponemos tomarse unos minutos 
             para realizar un repaso de los conceptos 
              aprendidos en Kahoot, ¿están listos?
                  Profe, puedes compartir el 
                   PIN o link de acceso al 
                        juego
Actividad colaborativa
Recolectando datos
Simularemos pertenecer al equipo de marketing de 
una empresa de cosméticos e intentaremos 
encontrar patrones demográficos que indican a qué 
persona se le podría vender delineador de ojos
A través de la siguiente encuesta recolectamos 
datos para su posterior análisis 
Duración: 5 minutos
Marco CRISP-DM y la 
fase de Machine 
Learning y modelo 
Marco CRISP-DM
Definición
CRISP-DM, que son las siglas de Cross-Industry Standard 
Process for Data Mining, es un método probado para 
orientar sus trabajos de minería de datos.
● Como metodología, incluye descripciones de las 
fases normales de un proyecto, las tareas 
necesarias en cada fase y una explicación de las 
relaciones entre las tareas.
● Como modelo de proceso, CRISP-DM ofrece un 
resumen del ciclo vital de minería de datos.
Contiene 6 fases importantes que se encuentran 
relacionadas. A continuación explicaremos cada una de 
ellas
Fases CRISP-DM
Fases CRISP-DM
 1           2           3            4           5           6
Comprensión Comprensión Preparación   Modelado    Evaluación  Despliegue
del negocio  de datos    de datos
Comprensión del negocio
Es recomendable dedicar tiempo a explorar las 
expectativas de la organización con respecto al análisis 
de datos. Siempre es bueno involucrar a equipos 
interdisciplinarios para entablar discusiones sobre cual 
es el mejor camino a elegir.
Conocer el contexto y las razones por las cuales se 
realizan diferentes procesos hace que se pueda optimizar 
el tiempo y recursos valiosos en la compañía. 
Es bueno recopilar información acerca de la situación 
comercial actual y qué objetivos tiene la compañía a 
corto, mediano y largo plazo
Comprensión de datos
Esta fase involucra estudiar más de cerca los datos 
disponibles, esto es vital para evitar problemas durante la 
siguiente fase: Preparación de datos 
Es importante en esta fase acceder a los datos y 
explorarlos con ayuda de tablas y gráficos. Consiste de 4 
procesos fundamentales:
1. Recopilación
2. Descripción
3. Exploración 
4. Verificación de calidad 
Preparación de datos
Es un aspecto fundamental que requiere de bastante 
tiempo (entre 50-70% del tiempo total invertido). Algunas 
de las tareas involucradas en esta fase son:
1. Fusión de conjuntos y registros de datos
2. Selección de muestras de subconjuntos de datos
3. Agregación de registros
4. Derivación de nuevos atributos
5. Clasificación de los datos para modelado
6. Eliminación o sustitución de valores en blanco o 
perdidos
7. División en entrenamiento y prueba
Modelado
En esta fase se incorporan herramientas analiticas para 
lograr resolver las preguntas de interes
El modelado se hace en varias iteraciones, los analistas de 
datos usualmente ejecutan varios modelos utilizando los 
parámetros predeterminados y ajustan los parámetros o 
vuelven a la fase de preparación de datos para las 
manipulaciones necesarias 
Existen muchas formas de resolver un problema e incluso 
de pueden utilizar diversos modelos para resolver el 
mismo problema
Evaluación
Luego de implementar modelos se debe evaluar los 
resultados a través de criterios de rendimiento comercial 
que se deben establecer al inicio del proyecto. 
Esta fase es clave para asegurar que su organización 
puede usar los resultados que ha obtenido 
Las conclusiones o inferencias obtenidas de los modelos y 
del proceso se entienden como descubrimientos
Despliegue
Es el proceso que consiste en utilizar sus nuevos 
conocimientos para implementar las mejoras en su 
organización. 
En general la fase de despliegue CRISP-DM incluye dos 
tipos de actividades:
1. Planificación y control del despliegue de los 
resultados
2. Finalización de tareas de presentación como la 
producción de un informe final y la revisión de un 
proyecto (La presentación final se puede llevar a 
cabo a través de un dashboard)
    Implementando 
 metodología CRISP-DM
Evaluaremos un caso donde podremos implementar la 
metodología CRISP-DM con el fin de resolver un 
         problema
      Duración: 15-20 min
ACTIVIDAD EN CLASE
Implementando 
metodología CRISP-DM
Descripción de la actividad. 
Tenemos el siguiente escenario: El dueño de un 
concesionario de autos recientemente ha escuchado 
que los negocios que no implementen técnicas de 
análisis de datos no podrán hacer parte de ciertos 
beneficios que proporciona el gobierno. Por esto 
contrata a un grupo de personas dedicadas a la ciencia 
de datos para que le ayuden a implementar una 
metodología CRISP-DM en su compañía con el fin de 
maximizar las ventas
Importante: El concesionario lleva todos sus registros 
de manera manual desde su creación en 1975
Tiempo estimado 15-20 min
    ACTIVIDAD EN CLASE
Implementando 
metodología CRISP-DM
 Descripción de la actividad. 
 Rellenar la siguiente tabla con propuestas para el 
 dueño del concesionario
            Fase                 Sugerencias de 
                                 implementación
     Comprensión del negocio
      Comprensión de datos
       Preparación de datos
         Modelamiento
      Evaluación y despliegue
 Tiempo estimado 15-20min
Scikit-learn
Scikit-learn
Scikit-learn es probablemente la librería más útil para 
Machine Learning en Python, es de código abierto y es 
reutilizable en varios contextos. Proporciona además 
una gama de algoritmos de aprendizaje 
supervisados y no supervisados en Python.
Fue diseñado por David Cournapeau como un código de 
un proyecto en 2007, luego este trabajo fue 
complementado por Matthieu Brucher en 2010 con su 
     primera versión beta 1.0
Librerías 
involucradas
Scikit-learn
Este librería está construida sobre SciPy (Scientific Python) e incluye las siguientes 
              librerías o paquetes:
Scikit-learn
        Ipython:                                             NumPy: 
        consola interactiva mejorada                         librería de matriz n-dimensional 
                                                             base
         SciPy:                                              Matplotlib: 
         librería fundamental para la informática            trazado completo 2D
         científica
         SymPy:                                              Pandas: 
         matemática simbólica                                estructura de datos y análisis
¿Por qué elegir Scikit-
Learn?
¿Por qué elegir Scikit-
Learn?
• Clustering.
• Ensemble  methods,  es  decir,  algoritmos  de 
aprendizaje supervisados y no supervisados.
• Validación cruzada, es decir, dispone de varios 
métodos  para  verificar  la  precisión  de  los 
modelos supervisados.
• Varios conjuntos de datos para prueba 
• Reducción de dimensionalidad
• Feature selection
• Ajuste de hiperparametros
 Ventajas
    1.  Interfaz consistente sin importar el 
        entorno
    2.  Variedad de modulos y algoritmos de ML 
        y DL
    3.  Posibilidad de extraer datos de varios 
        repositorios (Github, Gitlab entre otro)
    4.  Parametros de configuracion (parametros 
        e hiperparametros)
    5.  Documentación bien estructurada y fácil 
        de seguir 
    6.  Gran comunidad que brinda soporte y 
        ayuda en diferentes foros y repositorios en 
        la web
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Encoding: One Hot, 
Label Encoder, 
Getdummies
One Hot Encoding
Toman variables numéricas para medir la distancia. Sin embargo, existe la manera de 
trabajar con variables categóricas haciéndolas variables dummy, a través del uso de la 
técnica de transformación de datos One Hot Encoding (OHE). No permite convertir 
               strings directamente.
Label Encoder
Sin embargo One Hot Encoding es la única técnica para transformar variables 
categóricas, existe una alternativa para reducir el problema de multidimensionalidad 
cuando tenemos muchas categorías en una variable a través del uso de la técnica de 
        transformación de datos Label Encoder (LE).
Getdummies()
Es un método similar a One Hot Encoding el cual transforma variables categórica en 
dummies. La diferencia es que One Hot Encoding almacena la transformación en un 
objeto. Una vez que se tiene la instancia OneHotEncoder(), se puede guardar para ser 
   usada más tarde en las siguientes fases de manipulación de datos
Ventajas y 
desventajas de 
Getdummies()
Ventajas y Desventajas 
Getdummies()
                  Ventajas                                  Desventajas
   Es un método sencillo para convertir        Agrega muchas columnas binarias si 
   categorías a números                        tienes muchas categorías en una 
                                               variable
   No requiere que las categorías sean         Necesita aplicarse individualmente a 
   mayores o iguales a cero como One Hot       cada columna o variable categórica
   Encoding
   Fue la primera versión de                   Requiere de mayor tiempo de ejecución
   estandarización de variables categóricas
Ingeniería de 
atributos
Definición
Ingeniería de factores
También se le conoce como Feature Engineering
 ● Proceso previo a la creación del modelo en el que 
    se  hace  análisis,  limpieza  y  estructuración 
    de los datos. 
 ● El objetivo es eliminar los campos que no sirven 
    para   hacer    la  predicción   y   organizarlos 
    adecuadamente para que el modelo no reciba 
    información que no le es útil y que podría 
    provocar  predicciones  de  poca  calidad  o 
    confianza. 
Procesos involucrados
La  ingeniería  de  factores  consiste  en  diferentes 
procesos:
● Feature Creation
● Transformaciones
● Feature Extraction
● Exploratory Data Analysis
● Benchmark
A  continuación  explicaremos  un  poco  cada  uno  de 
estos procesos
Feature Creation
Es el proceso de crear nuevas variables con el 
fin de desarrollar modelos más eficientes. 
Esto se puede hacer añadiendo o removiendo 
algunas    variables,  incluso   condensando 
diferentes variables en una sola por medio de 
un indicador
Es  recomendable  no  añadir  muchas  más 
variables  de  las  que  se  tienen  en  cualquier 
dataset.
Transformaciones
Es el proceso de convertir variables por medio 
de algún proceso de escalamiento o encoding
El  objetivo  de  esta  tarea  es  poder  utilizar 
algunas  visualizaciones  para  identificar  si  es 
necesario incluir o remover ciertas variables del 
dataset original.
Se pueden utilizar diversas metodologías con el 
fin  de  optimizar  tiempos  de  ejecución  en 
algoritmos   como  One  Hot  Encoding  o 
LabelEncoder
Feature Extraction
Es el proceso de extraer variables o features de 
los  datasets  con  el  fin  de  identificar  qué 
información es útil.
Sin  distorsionar  las  relaciones  originales  o 
significativas, este proceso comprime la cantidad 
de  información  en  cantidades  manejables  para 
que  los  algoritmos  los  procesen  de  manera 
eficiente.
Se asocia a lo que se conoce como reducción de 
dimensionalidad como el PCA por ejemplo.
Exploratory Data Analysis
Es el proceso de obtener gráficos y resúmenes 
descriptivos de los datos con el fin de mejorar el 
entendimiento  de  la  data  explorando  sus 
propiedades
Esta técnica se aplica a menudo con el fin de 
poder testear hipótesis y encontrar patrones en 
los datos
Cobra  mayor  relevancia  en  datos  de  tipo 
cuantitativo  o  cualitativos  que  no  han  sido 
analizados previamente.
Benchmark
 Es el proceso de crear un modelo de aprendizaje 
 automático        fácil     de      utilizar,    confiable, 
 transparente e interpretable. 
 Usualmente  se  pueden  utilizar  diferentes  tipos 
 de algoritmos para ver el desempeño individual 
 de cada uno en los datos analizados  (eg redes 
 neuronales,  máquinas  de  soporte  vectorial, 
 clasificadores        lineales      y      no      lineales, 
 bagging/boosting entre otros)
Técnicas más 
utilizadas
Imputación
Esta  técnica  busca  manejar  los  datos  nulos, 
existen dos tipos de imputación:
1. Numérica: para este caso se puede utilizar 
interpolación o alguna técnica de regresión, 
incluso se puede reemplazar por valores por 
defecto
2. Categórica:  una  alternativa  puede  ser 
utilizar la moda para reemplazar, también es 
posible crear una nueva categoría llamada 
“Desconocido”
Manejo de outliers
Esta  técnica  busca  remover  valores  atípicos  de  los 
datos. Existen diversas formas de poder hacerlo:
1. Remover: es algo no muy recomendable aunque 
depende  del  contexto  ya  que  puede  dejar  muy 
pocos datos para ser analizados
2. Reemplazar: se pueden reemplazar por la media 
o mediana entre otras opciones
3. Capping:  utilizar  valores  arbitrarios  de  una 
distribución
4. Discretización: convertir variables numéricas a 
categóricas  con  el  fin  de  evitar  la  influencia  de 
outliers
Transformaciones 
Existen muchas técnicas para realizar el procesos de 
numéricas
escalamiento   de    los  datos,   a   continuación 
mencionaremos las más utilizadas:
1. Logarítmica
2. StandardScaler
3. Min Max Scaler
4. Quantile Transformer
5. Robust Scaling
6. Absolute Maximum Scaling
Transformaciones 
categóricas
Para este proceso tenemos tres alternativas:
1. One Hot Encoding
2. Label Encoder
3. Conversión a variables dummies
Es  recomendable  que  cuando  se  tienen  pocas 
categorías  (<=4)  utilizar  One  Hot  Encoding  o  el 
método getdummies. Por otro lado, cuando se tienen 
muchas categorías (>4) se recomienda reducirlas y 
aplicar LabelEncoder 
Importancia
Importancia
El  Feature  Engineering  es  un  proceso  muy 
importante  para  Machine  y  Deep  Learning, 
donde es posible extraer, limpiar, filtrar y crear 
variables  artificiales  que  permiten  mejorar  el 
performance de modelos con el fin de obtener 
mejores resultados
Los  Data  Scientists  pasan  mucho  tiempo 
limpiando  y  organizando  datos  (~60%  del 
tiempo),  por  ende  es  importante  que  sean 
capaces de crear modelos reproducibles y con 
altos niveles de precisión
Importancia
Cuando  las  tareas  asociadas  a  Feature 
Engineering  se  realizan  de  forma  correcta,  el 
dataset resultante es óptimo y contiene todos los 
factores importantes que afectan el problema de 
negocio
Como resultado estos datasets se convierten en 
el  insumo  para  lograr  obtener  modelos  con 
mayor precisión de los cuales se pueden obtener 
más insights 
Flujo de trabajo
Flujo de trabajo
   1             2              3             4              5              6
Definir     Recolectar      Preparar     Seleccionar    Entrenar y      Integrar
objetivo        datos       los datos      algoritmo       validar        modelo
Actividad colaborativa
Analizando datos de marketing
Utilizaremos los resultados de la encuesta que realizamos al 
inicio de la clase. Los estudiantes se dividirán en grupos (no más 
de 5 personas) y copiaran los resultados de la encuesta.
Se deben generar reglas para identificar si una persona usa o no 
delineador de ojos en base a las respuestas obtenidas
Las reglas las deben discutir como equipo y luego generarlas 
como funciones “if” en google sheets. Tener en cuenta el 
problema del “sesgo” (e.g mujer=delineador) y otros tipos de 
errores
Duración: 15 minutos
Ejemplo en vivo
Analizaremos el notebook llamado 
Clase_1.ipynb donde procesaremos la encuesta 
y entrenaremos un árbol de decisión que 
generará las reglas para determinar si alguien 
comprara o no delineador de ojos. Además 
evaluaremos qué tan bien funciona dicho arbol de 
decision
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
 MATERIAL AMPLIADO
Recursos multimedia
Scikit-Learn
✓ Machine Learning in Python | Scikit-Learn | Enlace
Disponible en nuestro repositorio.
¿Preguntas?
CLASE N°40
Glosario
CRISP-DM: Cross-Industry Standard           Label Encoder: proceso mediante el cual 
Process for Data Mining, es un método       se realiza el encoding de variables 
probado para orientar trabajos de minería   categóricas de manera secuencial ideal 
de datos                                    para variables ordinales
Scikit-Learn: librería más útil para        Feature Creation: proceso en el cual se 
Machine Learning en Python, es de código    crean nuevas variables con el fin de 
abierto y es reutilizable en varios         desarrollar modelos más eficientes.
contextos                                   Feature Extraction: proceso de extraer 
One Hot Encoding: proceso mediante el       variables o features de los datasets con el 
cual se realiza el encoding de variables    fin de identificar qué información es útil.
categóricas en números utilizando 
variables dummys
Opina y valora 
esta clase
           Resumen 
       de la clase hoy
      ✓ Marco CRISP-DM y la fase de Machine Learning
      ✓ Scikit Learn
      ✓ Encoding: OneHot encoder, 
        Labelencoder, .getdummies()
      ✓ Ingeniería de atributos: concepto, ventajas y 
        relevancia
      ✓ Flujo de trabajo
Muchas 
gracias.
   ¡Les damos la 
    bienvenida!
       ¿Comenzamos?
Esta clase va a ser
grabad
  a
      Clase 0. DATA SCIENCE
  Introducción a la 
  Ciencia de Datos
Temario
                                       00                     01
                                Introducción a        La necesidad de 
                                 la Ciencia de         Información en 
                                     Datos             la Industria 4.0
                                ✓ Ciencia de             ✓ Industria 4.0
                                    datos                ✓ Transformación 
                                                            digital
                                ✓ Ciclo de vida          ✓  Ciclo de vida de 
                                    de los                  un proyecto de 
                                    proyectos               ciencia de datos
                                                         ✓  Valor y retorno 
                                ✓ Tipos de Data             de la Ciencia de 
                                    Science                 Datos
                                                         ✓ Estrategia data-
                                ✓ Importancia               driven
Objetivos de la clase
         Comprender la definición de Data Science.
         Identificar los roles que cumplen las personas 
         con el rol de Data Scientist.
         Identificar qué habilidades son importantes 
         para poder cumplir con este cargo.
MAPA DE CONCEPTOS
                                         ¿Qué hacen los 
                                         DS?
                      Ciencia de datos   Habilidades 
                                         necesarias
                                         Características de 
                      Transformación     la industria 4.0
                      Digital
Introducción a la 
ciencia de datos
                      Ciclo de vida de un 
                      proyecto de ciencia 
                      de datos
                      Valor y retorno de la 
                      ciencia de datos
                      Estrategia Data 
                      Driven
Ciencia de
Datos
Definición
Data Science
Es un campo interdisciplinario que utiliza        Es un campo que combina dominio del 
métodos científicos, procesos, algoritmos         tema, habilidades de programación y 
y sistemas con el fin de extraer                  conocimiento de matemáticas y 
conocimientos e insights de datos                 estadística
estructurados, no estructurados y 
semiestructurados. 
Fuente: Cambridge
                REEMPLAZAR 
                POR IMAGEN
Data Science
                             Existen muchas 
                             creencias de que es 
                             lo que realmente 
                             hace un Data 
                             Scientist, es por esto 
                             que a continuación 
                             aclararemos los roles 
                             que implica este 
                             cargo.
¿Qué hacen los 
científicos de Datos?
¿Qué hacen los 
científicos de 
datos?
Un flujo de trabajo típico para los Data Scientist es 
el siguiente:
✔ Entender el negocio
✔ Recolectar y Explorar los datos
✔ Preparar y procesar los datos
✔ Crear y validar modelos
✔ Desplegar y monitorear performance de 
algoritmos
¿Qué habilidades 
requieren los 
científicos de Datos?
              ¿Qué habilidades 
              requiere un 
              científico de 
              datos?
              1. Inquisitivo: es una persona curiosa y algunas 
              veces escéptica
              2. Conocimiento sólidos: en Machine Learning, 
              computación, matemáticas, estadística y probabilidad
              3. Método científico: crea hipótesis, las pone a 
              prueba y actualiza su entendimiento de los 
              problemas
              4.  Habilidades en programación: es bueno 
              realizando códigos, hacking y en la programación en 
              general
              5. Productos orientados: sabe como desarrollar 
              productos asociados a data y visualizaciones para 
              hacer los datos más entendibles para cualquier 
              persona
              6. Conocimiento del dominio: entiende los 
              negocios y cómo contar historias interesantes 
              (Storytelling), es capaz de responder preguntas. 
Data Team
               Data Team
REEMPLAZAR      Es el grupo de roles dentro de una organización que se 
POR IMAGEN      encargan de todo el proceso de manipulación, 
               estructuración y generación de insights a partir de los 
               datos disponibles. Está compuesto usualmente de 3 
               roles importantes:
               ✔ Data Scientist
               ✔ Data Engineers
               ✔ Data Analysts
Data Scientist
Es capaz de tomar proyectos de Data Science desde el                 REEMPLAZAR 
inicio al fin. Pueden almacenar grandes cantidades de                 POR IMAGEN
información, crear modelos predictivos y presentar 
resultados. 
Skills: Matemáticas, Programación y Comunicación
Software comúnmente usado: SQL, Python, R
               Data Engineers
               Son personas versátiles capaces de usar la ciencia de 
               computación para procesar grandes cantidades de 
               datos. Se enfocan en procesos de código, limpieza de 
               datos e implementar solicitudes de los data Scientists
               Skills: Matemáticas, Programación y Big Data
               Software comúnmente usado: Hadoop, NoSQL, 
               Python
Data Analysts
Son personas que ayudan a otras personas dentro de la 
compañía a entender solicitudes específicas por medio 
de gráficas y resúmenes numéricos.
Skills: Estadística, Comunicación y Entendimiento del 
negocio
Software comúnmente usado: Excel, Tableau, SQL
Ciclo de vida de los 
proyectos en
Ciencia de Datos
Ciclo de vida de proyectos 
Existen 9 etapas fundamentales:                ✔ Evaluación
  ✔ Entendimiento del problema                 ✔ Generación de insights y reportes
  ✔ Recolección de datos                       ✔ Despliegue
  ✔ Preprocesamiento de datos                  ✔ Toma de decisiones 
  ✔ Analizando los datos
  ✔ Modelamiento                             Explicaremos brevemente cada etapa 
                                             a continuación
1. 
Entendimiento 
Se requiere tener claro el contexto de negocio que 
del problema
se analiza para poder responder las preguntas 
relevantes que puedan existir. Usualmente, 
aplicamos Data Science para resolver 5 preguntas:
✔ ¿Cuánto o cuántos? (Regresión)
✔ ¿Cuál categoría? (Clasificación)
✔ ¿Qué grupo? (Clustering)
✔ ¿Es raro? (Detección anomalías)
✔ ¿Qué opción deberíamos tomar 
(Recomendaciones)?
             1. 
             Entendimiento 
             del problema
             Algunas preguntas que se han hecho algunos negocios 
             útiles por ejemplo son:
             Uber: ¿Qué porcentaje del tiempo los conductores 
             realmente conducen? ¿Qué tan estable es su ingreso?
             Oyo Hotels: ¿Cuál es el promedio de ocupación de 
             hoteles mediocres?
             Alibaba: ¿Cuáles son las ganancias por pie cuadrado 
             de nuestros almacenes?
2. Recolección 
de datos
Los datos recolectados deben permitir resolver las 
preguntas y pueden venir de diferentes fuentes, 
algunos ejemplos de fuentes de datos son:
✔ Formato plano (Excel, CSV, Texto, XML, JSON)
✔ Bases de datos relacionales
✔ Bases de datos no relacionales
✔ Data de Web Scraping
El concepto de Big Data implica Volumen 
(Terabytes), Velocidad (Data en Streaming) y 
Variedad (Estructurado, No estructurado y semi 
estructurada)
             3. 
             Preprocesamiento 
             de datos
             Se conoce como Data Wrangling y es la tarea que 
             usualmente toma más tiempo.  Aquí se entienden 
             mejor los datos y se preparan para análisis posteriores. 
             Limpiar datos esencialmente implica remover 
             discrepancias de tus datos (nulos, outliers, duplicados).
             Es la etapa más importante en todo el ciclo, ya que los 
             modelos usualmente son tan buenos como los datos 
             con los que son entrenados. 
4. Análisis de 
datos
Se conoce como Exploratory Data Analysis (EDA) y 
no hay reglas exactas de como hacerlo.
Se necesita de conocimientos en estadística para 
presentar resúmenes numéricos y gráficas 
apropiadas de acuerdo a la naturaleza de las 
variables analizadas
Existen diversos tipos de analítica de datos que se 
pueden aplicar de acuerdo con los datos y el 
problema a resolver.
               4. Análisis de 
               datos
                1. Analítica descriptiva: que ha pasado en el 
                 pasado y tiene carácter meramente exploratorio
                2. Analítica predictiva: que podría pasar en el 
                 futuro, se pueden usar técnicas estadísticas o de 
                 Machine Learning para estimar el futuro
                3. Analítica Prescriptiva: que deberíamos hacer, 
                 podemos usar métodos de optimización o 
                 simulación para tomar decisiones y describir 
                 posibles resultados 
5. 
Es una de las etapas más interesantes. Se usa 
Modelamiento
para encontrar patrones y comportamientos en 
los datos. Esto se puede lograr de dos formas 
usualmente
1. Modelamiento descriptivo (No 
Supervisado): que nos permite encontrar 
grupos y patrones ocultos
2. Modelamiento predictivo (supervisado): 
obtener predicciones futuras con base en 
información del pasado
               6. Evaluación
               En esta etapa se cuantifica el desempeño del modelo 
               creado previamente. Para esto se dividen los datos 
               en dos partes: train/entrenamiento (70%) que 
               permiten calibrar los modelos y test/validación 
               (30%) que permiten obtener las métricas 
               correspondiente
               Las métricas elegidas varían de acuerdo al algoritmo 
               elegido, existen medidas tanto para clasificación 
               como para regresión
7. Generación 
de insights y 
Se presentan los resultados a diferentes tipos de 
reportes
audiencias a través de reportes o tableros. Existen 
diferentes herramientas para esto, por ejemplo:
✔ Tableau
✔ Power BI
✔ R- ggplot2, lattice, Shiny
✔ Python- Matplotlib, Seaborn, Plotly, Dash
✔ Kibana
✔ Grafana
✔ Spotfire
8. Despliegue
Se pone en producción (a disposición de, por ejemplo, el equipo de 
ventas) la herramienta desarrollada en las etapas previas. Algunos 
Frameworks útiles son:
✓ Flask
✓ Django
✓ FastAPI
Algunos proveedores en la nube son:
✓ AWS
✓ Azure
✓ Google Cloud
9. Toma de 
decisiones
En esta etapa es posible la toma de decisiones con 
base en insights. De igual forma al realizar el 
proceso podemos aprender de resultados positivos o 
negativos que puedan ocurrir.
Con toda esta información es posible tomar 
decisiones operativas con el fin de mejorar los 
diferentes procesos dentro de cualquier 
organización.
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Tipos de 
Data Science
Clasificación
Clasificación
De acuerdo a la naturaleza de las tareas 
desarrolladas podemos tener dos tipos de 
Data Science, los cuales son:
✔ Data Science para humanos
✔ Data Science para máquinas
Data Science para 
humanos
               Data Science 
               para humanos
               La cual se refiere al uso de la información por parte de 
               tomadores de decisiones como ejecutivos o managers.
               El rol del Data Scientist es por ende diseñar, definir e 
               implementar métricas además de desarrollar e 
               interpretar experimentos, crear dashboards y obtener 
               inferencias causales para poder generar sistemas de 
               recomendación 
Data Science 
para humanos
Procesos que permiten el buen desarrollo de este 
proceso:
✔ Data analysis
✔ Data visualization
✔ Data Storytelling
✔ Entendimiento de negocio
✔ Capacidad de presentar
✔ Predicción de resultados deseados
Data Science para 
máquinas
               Data Science 
               para máquinas
               Donde los consumidores finales son máquinas que se 
               alimentan de datos, modelos y algoritmos.
               Dependiendo del nivel de dificultad detrás se puede 
               hablar de productos que se pueden desplegar en el 
               sistema de producción o también pueden ser prototipos 
               que se pueden optimizar 
Data Science 
para máquinas
Procesos que permiten el buen desarrollo de este 
proceso:
✔ Modelamiento automático
✔ Inteligencia artificial
✔ ETL
✔ Data Engineering
✔ Software Engineering
✔ Arquitecturas de optimización
Importancia de
Data Science
¿Por qué la ciencia 
de datos es 
importante?
Cada negocio tiene datos pero su valor         Nos permite conocer mejor a nuestros 
comercial depende de qué tanto conocen         clientes, y puede ayudar a optimizar 
esos datos.                                    nuestros procesos con el fin de tomar 
                                              mejores decisiones.
Data Science ha ganado importancia en 
tiempos recientes porque ayuda a 
incrementar el valor comercial de los 
datos disponibles y cómo se pueden 
utilizar para tomar ventaja respecto a los 
competidores.
Ejemplos de la vida 
 real
LYNA
Recientemente, han desarrollado una herramienta 
llamada LYNA para identificar cáncer de mama. 
Estos tumores pueden ser difíciles de detectar por el 
ojo humano especialmente cuando apenas se está 
desarrollando el tumor y es pequeño. 
El algoritmo desarrollado en LYNA tienen un 
accuracy de cerca del 99% a la hora de detectar 
este tipo de cáncer y a pesar de que se requiere de 
mejoras ya en algunos hospitales se usa.
               Clue
               Esta es una aplicación desarrollada en Alemania que 
               utiliza Data Science con el fin de pronosticar los ciclos 
               menstruales registrando diversas variables 
               importantes. 
               Los usuarios se les notifica cuando son fértiles en la 
               cúspide de un periodo o en su defecto cuando tienen 
               un riesgo elevado de padecer afecciones como 
               embarazos ectópicos (óvulo fecundado crece fuera de 
               cavidad principal del útero). 
UPS
Utilizan Data Science para optimizar el transporte 
de paquetes. Para esto utilizan Herramientas de 
Planeación interconectada (NPT) que incorpora 
Machine Learning e IA para poder superponerse a 
las diferentes dificultades de la logística y ante 
adversidades climáticas
A través de este sistema se sugieren rutas para la 
entrega de los diferentes paquetes, usando esta 
plataforma la compañía ha ahorrado entre 100 a 
200 millones de USD en 20210
               Moneyball-ING 
               El club de fútbol Liverpool FC el cual es conocido por su 
               exitoso presente ha utilizado Data Science para su 
               beneficio. Al igual que el equipo Oakland A’s ha logrado 
               conseguir muy buenos jugadores antes de que otros 
               equipos ricos se den cuenta de que existen.
               Para esto utilizan un modelo que es capaz de 
               cuantificar el desempeño de cada jugador teniendo en 
               cuenta pases, velocidad, distancia recorrida e 
               influencia general en asistencias y goles, así como su 
               influencia en victorias.
Airbnb
La Ciencia de datos ayudó a renovar por completo la 
función de búsqueda de Airbnb. antes se priorizaron 
los alquileres mejor calificados ubicados a cierta 
distancia del dentro de las ciudades, eso implicaba 
conseguir alquileres buenos pero no siempre en los 
mejores vecindarios
Se resuelve este problema con un truco, el cual fue 
dar prioridad a los alquileres en lugares que tienen 
una alta densidad de reservas Airbnb, aunque aún 
hay algunas dificultades por mejorar 
               Uber Eats
               El objetivo principal de este servicio es llevar la comida 
               caliente lo más rápido posible. Para cumplir con esto 
               utilizan Machine Learning, modelos estadísticos junto 
               con un staff meteorológico. 
               Con el fin de optimizar el proceso de delivery el equipo 
               tiene que predecir como cada posible variable (desde 
               tormentas hasta cumpleaños) impactan en el tráfico y 
               el tiempo de cocina. 
Instagram
Instagram utiliza la ciencia de datos para orientar 
sus publicaciones patrocinadas. Los científicos de 
datos extraen información de Instagram y Facebook 
que tienen una estructura de seguimiento web 
exhaustiva sobre muchos usuarios.
A partir de esto el equipo elabora algoritmos que 
convierten los me gusta y comentarios, uso de otras 
aplicaciones e historial web para generar 
predicciones de productos que podrían comprar 
               Meta
               Meta usa la ciencia de datos de varias maneras, pero 
               una de sus funciones más populares es la barra lateral 
               "Personas que quizás conozcas", que aparece en la 
               pantalla de inicio de la red social. 
               Se basa en la lista de amigos, las personas con las que 
               han sido etiquetados en las fotos y dónde han 
               trabajado y estudiado. También se basa en 
               "matemáticas" en donde la ciencia de redes es usada 
               para el crecimiento de la red social de un usuario en 
               función del crecimiento de las redes de usuarios 
               similares.
CLASE N°0
Glosario
Data Science: campo interdisciplinario       Data Engineers: personas versátiles 
que utiliza métodos científicos, procesos,   capaces de usar la ciencia de 
algoritmos y sistemas con el fin de          computación para procesar grandes 
extraer conocimientos e insights de datos    cantidades de datos. Se enfocan en 
estructurados, no estructurados y            procesos de código, limpieza de datos e 
semiestructurados                            implementar solicitudes de los data 
Data Scientist: persona que es capaz de      Scientists.
tomar proyectos de Data Science desde el     Data Analysts: personas que ayudan a 
inicio al fin. Pueden almacenar grandes      otras personas dentro de la compañía a 
cantidad de información, crear modelos       entender solicitudes específicas por 
predictivos y presentar resultados.          medio de gráficas y resúmenes 
                                            numéricos.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Ciencia de datos
      ✓  Ciclo de vida de los proyectos de ciencia de 
        datos
      ✓ Tipos de Data Science
      ✓ Importancia de Data Science
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 44. DATA SCIENCE
     Algoritmos de 
     Agrupación II
Temario
                43                      44                       45
          Algoritmos de           Algoritmos de             Selección de 
           agrupación I            Agrupación II            Algoritmo y 
                                                         Entrenamiento del 
                                                              Modelo I
        ✓ K means                 ✓ PCA                   ✓ Matriz de confusión
        ✓ DBSCAN                  ✓ Reducción de          ✓ Accuracy
                                     dimensionalida       ✓ Precisión 
                                     d                    ✓ Sensibilidad 
                                  ✓ Detección de          ✓ Especificidad 
                                     outliers             ✓ F1 score  y curva ROC
Objetivos de la clase
         Profundizar en el aprendizaje supervisado
         Conocer el funcionamiento de los modelos de 
         Clustering, PCA y detección de anomalías
  MAPA DE CONCEPTOS
                                                   K Means
                                                   DBSCAN
                                                  HDBSCAN
     Algoritmos de                                Reducción de 
      Agrupación                                 dimensionalidad
                                                                 Clase 
                                                     PCA         44
                                                  Detección de 
                                                   outliers
    Repaso
             Les proponemos tomarse unos minutos 
             para realizar un repaso de los conceptos 
              aprendidos en Kahoot, ¿están listos?
                  Profe, puedes compartir el 
                   PIN o link de acceso al 
                        juego
 PCA
Para pensar
Si tenemos una situación en la cual tenemos 
variables categóricas solamente respecto a los 
atributos de diversos automóviles, ¿podríamos 
aplicar la técnica de PCA?

Verdadero o Falso
Contesta en el chat de Zoom 
¿Qué hacía el PCA?
• El método gira los datos de forma que, desde 
un punto de vista estadístico, no exista una 
correlación entre las características rotadas 
pero que conserven la mayor cantidad 
posible de la varianza de los datos 
originales. 
• Reduce la dimensionalidad de un conjunto 
de datos proyectándose sobre un subespacio 
de menor dimensionalidad.
Definición
PCA (Principal Component 
Analysis)
Es una técnica utilizada para la 
identificación de un número más 
pequeño de variables no 
correlacionadas conocidas como 
componentes principales de un conjunto 
más grande de datos. Permite enfatizar 
la variación y capturar patrones fuertes 
en un conjunto de datos.
Ejemplo aplicado
PPCACA  ((PPririnncciipalpal  CCoommponponenentt  
AAnnalalyyssiiss))
Es una técnica utilizada para la 
identificación de un número más 
pequeño de variables no 
correlacionadas conocidas como 
componentes principales de un 
conjunto más grande de datos. 
Permite enfatizar la variación y 
capturar patrones fuertes en un 
conjunto de datos.
  Imaginemos que tenemos el siguiente conjunto de datos que representa dos variables para 6 
  individuos (ratones). Si queremos representar a los individuos en una dimensión a traves de una 
  variable podemos hacer una grafica com la de la derecha
PCA (Principal Component 
Analysis)
También podemos graficar dos               Si agregamos una tercera variable 
dimensiones como se observa en la          podríamos hacer un gráfico 3D. 
figura de arriba generando un              Aunque si ponemos una cuarta 
diagrama de dispersión                     dimensión el análisis ya no es tan 
                                           sencillo
PCA (Principal Component 
Analysis)
Si calculamos la media de la primera        De igual forma si calculamos la media 
variable tenemos lo siguiente               de la segunda variable tenemos lo 
                                            siguiente
PCA (Principal Component 
Analysis)
Si juntamos los dos punto tenemos el        Si hacemos que este punto sea el 
centro de gravedad o la media global        nuevo centro (0,0) no afecta la 
de los individuos                           posición relativa de los puntos
PCA (Principal Component 
Analysis)
Podemos proceder a ajustar una línea       Y el resultado luego del ajuste es el 
que mejor se ajuste a los datos como       siguiente
en regresión
PCA (Principal Component 
Analysis)
La medida de bondad de ajuste de la 
línea se hace proyectando cada             De esta manera las distancia respecto 
individuo en la línea                      al origen se maximizan
PCA (Principal Component 
Analysis)
                 Entonces el PCA minimiza la 
                 distancia hasta la linea de 
                 proyeccion o maximiza la distancia 
                 del punto proyectado al origen
                 Aunque preferiblemente lo hace 
                 maximizando la distancia c en el 
                 triángulo
PCA (Principal Component 
Analysis)
Así que de esta manera tenemos maximizadas las distancias hasta el 
origen y minimizadas las distancias de los puntos a la línea de proyección
PCA (Principal Component 
Analysis)
                 Esta línea se llama PC1 (si tiene 
                 una pendiente de 0.25 por 
                 ejemplo) por cada 4 unidades que 
                 aumente el gen 1 el gen 2 
                 aumenta en 1 unidad
                 Por eso el PC1 así como los 
                 demás componentes principales 
                 on una combinación lineal de 
                 variables
PCA (Principal Component 
Analysis)
Una propiedad de este método es que el PC1    También es de suma importancia que 
siempre recoge mayor porcentaje de            los datos esten escalados o 
variabilidad explicada                        normalizados
PCA (Principal Component 
Analysis)
Ejemplo en vivo
Dentro de la carpeta de clase explorar el 
notebook “PCA - CoderHouse (Ejemplo 1)” 
con el fin de comprender el funcionamiento 
del algoritmo PCA
HANDS ON LAB
Explorar el notebook “PCA - 
CoderHouse (Ejemplo 2)” para 
identificar el funcionamiento del 
algoritmo PCA. ¿Tiene sentido 
en este algoritmo separar 
train/test?
Cualquier inquietud pueden 
consultar a su tutor o profesor
10-15 min
Reducción de 
dimensionalidad
Para pensar
Si bien hemos visto que la Reducción de 
dimensionalidad es uno de los problemas que se 
pueden resolver con el aprendizaje no 
supervisado, ¿Han escuchado de algún otro 
método similar al PCA? ¿Si es así como funciona?
Contesta en el chat de Zoom 
Definición
Reducción de dimensionalidad
Es una técnica de de aprendizaje no 
supervisado que consiste en la 
transformación de datos desde un espacio 
de alta dimensión a uno de baja dimensión 
para que la representación de baja 
dimensión retenga algunas propiedades 
significativas de los datos originales.
Importancia
Importancia
● Menor número de dimensiones implica 
menor tiempo de entrenamiento y menor 
recurso computacional y mayor 
performance
● Permite evitar el problema del overfitting
● Es extremadamente util para la tarea de 
“Data Visualization”
Importancia
● Puede ser usado para comprimir imágenes 
con  el  fin  de  evitar  consumir  mucho 
espacio a la hora de generar algoritmos y 
entrenarlos
● Puede ser usado para transformar datos no 
lineales en una forma lineal separable (e.g 
Kernel PCA es una aplicación de este caso)
Importancia
● Tiene en cuenta el problema de la 
multicolinealidad 
● Es bastante útil para el análisis factorial 
(encontrar variables latentes que no se 
pueden medir directamente en una 
variable)
● Remueve el ruido en los datos quedándose 
con las variables más importantes
Clasificación
Clasificación
Se tienen dos grandes categorías:
1. Feature Selection: tiene como objetivo 
encontrar un subconjunto de variables (las 
más importantes) de todas las variables. 
Tenemos 3 estrategias distintas (Filtro, 
Wrapper, Embedded)
2. Feature Extraction: extrae variables 
relevantes como combinación lineal de las 
variables originales con menor número de 
dimensiones
Clasificación
                         La metodología Feature 
                         Extraction también tiene una 
                         clasificación adicional ya que 
                         se pueden utilizar técnicas 
                         lineales o no 
                         lineales )Manifold) para 
                         encontrar las mejores 
                         combinaciones 
                          
                         Fuente: Clasificación 
                         Reducción Dimensionalidad
Principales técnicas
Principales técnicas
Algunas de las principales técnicas más aplicadas 
son:
  1. Principal Component Analysis (PCA)
  2. Análisis Factorial
  3. Análisis Discriminante
  4. Kernel PCA
  5. t-SNE  (t-distributed   Stochastic   Neighbor 
      Embedding)
  6. Escalamiento multidimensional (MDS)
  7. Isomap (Isometric mapping)
Exploraremos algunas técnicas distintas del PCA
Análisis factorial
Este método no solo busca reducir la 
dimensionalidad como el PCA, sino que también 
permite encontrar variables latentes que no se 
pueden medir directamente con una sola 
variable, estas variables latentes se les llama 
factores. 
Es una tećnica principalmente estadística que 
busca explicar las correlaciones entre las 
variables en términos de un menor número de 
variables
Análisis Discriminante
Conocido como LDA se usa para problemas de 
clasificación multi clase usualmente. Esta técnica 
separa o discrimina las instancias por sus clases. A 
diferencia del PCA el LDA encuentra una 
combinación lineal de variables que optimice la 
separación de clases mientras que el PCA trata de 
encontrar componentes no correlacionados. 
Otra diferencia es que el PCA es un algoritmo no 
supervisado mientras que el LDA es supervisado
Kernel PCA
Es una técnica de reducción de dimensionalidad 
no lineal que utiliza diferentes funciones kernel 
(Radial basis-function kernel) y es la versión 
no lineal del PCA. Funciona bastante bien en 
datasets no lineales donde el PCA normal no 
funciona eficientemente
Se necesitan especificar 3 hiper parámetros 
importantes: a) componentes a retener, b) tipo de 
kernel (linear, poly, rbf, sigmoid, cosine) y c) 
coeficiente kernel llamado gamma
t-SNE
Es una técnica de reducción de dimensionalidad 
no lineal que principalmente se usa para “Data 
Visualization”.
Se basa en técnicas probabilísticas sobre parejas 
de muestras en el espacio original
Se recomienda utilizar ya sea PCA o 
descomposicion SVD truncada antes de t-SNE si el 
numero de variables es mayor a 50. 
Escalamiento 
multidimensional
MDS es una técnica de reducción de 
dimensionalidad no lineal que trata de preservar 
las distancias entre las instancias mientras 
reduce la dimensionalidad de los datos no 
lineales.
Existen dos tipos de algoritmos de MDS con 
métrica y sin métrica. Dentro de la clase MDS() 
de Scikit Learn e puede definir cuál de las dos 
se desea utilizar 
Isomap
Es una técnica de reducción de dimensionalidad 
no lineal que funciona como una extensión del 
MDS o del Kernel PCA. Conecta cada instancia 
calculando una distancia curva o geodésica con 
base a los vecinos más cercanos. 
El número de vecinos a considerar para cada 
punto se puede específicas con n_neighbors 
dentro de la clase Isomap 
Ejemplo en vivo
Dentro de la carpeta de clase exploraremos 
los notebooks “Reduccion de 
dimensionalidad - Ejemplo 3.ipynb” con el 
fin de comprender el funcionamiento de los 
algoritmos: Análisis factorial, Análisis 
Discriminante, Kernel PCA, t-SNE, 
Escalamiento Multidimensional e Isomap
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Detección de Outliers
Para pensar
Como hemos visto en clases anteriores los 
outliers son un problema bastante común en 
cualquier tipo de datos, ¿Conocen algún método 
que permita encontrarlos de manera automática? 
¿Podríamos utilizar métodos automáticos para 
detectarlos?
Contesta en el chat de Zoom 
Definición
Definición
Se define como el proceso de detectar 
y excluir outliers de un dataset dado. 
Recordemos que un outlier es una 
observación que difiere drásticamente 
de los demás datos. Existen diversas 
técnicas estadísticas y de aprendizaje 
automático para poder detectarlos
Clasificación
Definición
Existen diversas técnicas de las cuales 
podemos resaltar:
1. Isolation Forest
2. Minimum Covariance Determinant
3. Local Outlier Factor (LOF)
4. One Class SVM
Isolation Forest
Es un algoritmo de detección de anomalías, el 
proceso de detección lo realiza por medio de 
aislamiento (isolation). Fue desarrollado por 
Fei Tony Liu en 2007 en sus estudios de 
doctorado.
Es un método no supervisado (sin etiquetas). 
Eta inspirado en algoritmo de clasificación 
Random Forest, formado por la combinación 
de múltiples arboles de decision llamados 
isolation trees 
Isolation Forest
El método toma ventaja de dos propiedades 
cuantitativas de las anomalías: 
1) Son una minoría de todas las instancias
2) Tienen atributos bastante diferentes a las 
instancias normales
A la hora de implementar el algoritmo es 
importante tener en cuenta el hiper parámetro 
llamado contaminación que se encuentra entre 
0 y 0.5 por defecto es 0.1
Minimum Covariance Determinant
Llamado MCD por sus siglas en inglés. Es un 
algoritmo de detección de anomalías bastante 
robusto teniendo en cuenta estructuras 
multivariadas creando estructuras elipsoidales 
que siguen la distribución normal 
multivariada. Todas las observaciones que 
queden por fuera de la elipsoide se consideran 
como anomalías/outliers
Minimum Covariance Determinant
Su objetivo principal es encontrar 
observaciones (de un total de n) en las cuales 
la matriz de covarianza tiene el determinante 
más bajo.
El parámetro “contamination” se define como 
la proporción de outliers y se puede encontrar 
el valor óptimo con prueba y error o con algun 
metodo de busqueda de hiper parámetros
Minimum Covariance Determinant
Llamado MCD por sus siglas en inglés. Es un 
algoritmo de detección de anomalías bastante 
robusto teniendo en cuenta estructuras 
multivariadas creando estructuras elipsoidales 
que siguen la distribución normal 
multivariada. Todas las observaciones que 
queden por fuera de la elipsoide se consideran 
como anomalías/outliers
Local Outlier Factor (LOF)
Es una técnica que intenta aprovechar la idea 
de los vecinos más cercanos para la detección 
de valores atípicos. A cada instancia se le 
asigna una puntuación de qué tan aislado 
(probabilidad de ser atípico) en función del 
tamaño de su vecindario más cercano. Es más 
probable que las instancias con la puntuación 
más alta sean valores atípicos. El parámetro 
“contamination” representa la proporción de 
nulos por defecto es 0.1.
One-Class SVM
Aunque el SVM se desarrolló inicialmente para 
clasificación binaria se puede usar también para 
clasificación con una sola clase. Cuando se 
modela una clase el algoritmo captura la 
densidad de la clase mayoritaria y clasifica las 
instancias en los extremos de la función de 
densidad como outliers. 
El One-Class SVM es un algoritmo de clasificación 
aunque puede ser usado para detectar outliers 
en datos de entrada para regresión o clasificación
Ejemplo en vivo
Miraremos ejemplos de aplicación dentro 
de la carpeta de clase con el notebook “4) 
Ejemplo Técnicas detección Outliers 
(Ejemplo 4).ipynb” donde aprenderemos la 
metodología para aplicar los algoritmos: 
Isolation Forest, Minimum Covariance 
Determinant, Local Outlier Factor y One-
Class SVM
Para pensar
¿El SVM encuentra el hiperplano que 
maximiza el margen de separación entre 
clases ?
Verdadero o Falso
Contesta la encuesta de Zoom 
          Importante…
                       Si bien existen diversos métodos 
                       que funcionan de manera 
                       automática para la detección de 
                       outliers no siempre todos son 
                       eficientes a la hora de 
                       encontrarlos, es por ello que se 
                       recomienda aplicar diferentes 
                       métodos y seleccionar el mejor que 
                       se ajuste al contexto de los datos 
                       analizados.
Detección de outliers
Utilizaremos lo aprendido en clase para poder 
identificar outliers en un conjunto de datos
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Detección de 
outliers
En esta oportunidad nos reuniremos en grupos de 
máximo 4 personas. Trabajaremos sobre el dataset 
de la clase anterior: clientes
1. ¿Sobre qué variable tiene sentido analizar 
outliers? 
2. Identificar outliers en la variable charges
3. Discutir: ¿Qué harían con esos outliers?, ¿Qué 
pasa si los sacamos? ¿Cómo afectaría esto al 
análisis del problema? ¿Pueden identificar algún 
patrón en las personas con “charges” 
extremos/anómalos?
4. Opcional: Remover outliers y repetir los pasos 
del notebook de Clustering de la clase 43 y ver 
como cambia la composición de los clusters
¿Preguntas?
CLASE N°44
Glosario
PCA: técnica utilizada para la                   Detección de outliers: proceso de 
identificación de un número más pequeño          detectar y excluir outliers de un dataset 
de variables no correlacionadas conocidas        dado. Recordemos que un outlier es una 
como componentes principales de un               observación que difiere drásticamente de 
conjunto más grande de datos.                    los demás datos. Existen diversas técnicas 
                                                estadísticas y de aprendizaje automático 
Reducción de dimensionalidad:                    para poder detectarlos. Existen diversas 
técnica de aprendizaje no supervisado            técnicas como: Isolation Forest, Minimum 
que consiste en la transformación de             Covariance Determinant, Local Outlier 
datos desde un espacio de alta dimensión         Factor (LOF) y One-Class SVM
a uno de baja dimensión para que la 
representación de baja dimensión retenga 
algunas propiedades significativas de los 
datos originales.
Muchas 
gracias.
                 Resumen 
           de la clase hoy
          ✓ PCA
          ✓ Reducción de Dimensionalidad
          ✓ Detección de Outliers
Opina y valora 
esta clase
Esta clase va a ser
grabad
  a
      Clase 24. DATA SCIENCE
  Fundamentos de 
   bases de datos
Temario
               23                      24                     25
             Data               Fundamentos               Lenguaje 
                                                       estructurado de 
         Acquisition I           de bases de            consulta SQL- 
                                     datos                 Parte 1
         ✓ Comprensión del     ✓ Bases de datos          ✓ Introducción
            problema en el     ✓ Sistemas DBMS
            caso de negocio    ✓                         ✓ DDL
                                  Tipos de sistemas de 
         ✓ Lectura de datos       gestión de bases de    ✓ DML
            con Pandas            datos
         ✓ Pyspark             ✓ Backup, Conexiones, 
                                  auditoría
Objetivos de la clase
         Comprender la definición de bases de datos y 
         las mejoras que ofrecen 
         Reconocer los sistemas de gestión de bases 
         de datos. Cómo es su arquitectura y que 
         funciones permiten trabajar con bases de datos 
         Conocer algunas funciones que forman parte 
         del DBMS
         Reconocer los tipos de sistemas de bases de 
         datos
     MAPA DE CONCEPTOS
                                       Fundamentos de bases de 
                                                 datos
                                                            Principales                  Backup, 
       ¿Por qué?                    DBMS                    sistemas de                conexión y 
                                                            bases datos                 auditoría
                                                            Relacional
     Definiciones                Definición                                             Backup y 
                                                              No SQL                   conexiones
                                                            Ventajas y 
        Mejoras                Arquitectura de              desventajas
       ofrecidas                   3 capas                                              Auditoría
                                                            Funciones y 
                                                             lenguajes
Cuestionario de tarea
¿Te gustaría comprobar tus 
conocimientos de la clase anterior?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
Introducción a las 
Bases de datos
Definición de bases 
de datos
Entendimiento del caso de 
negocio
✔ El  éxito  de  las  organizaciones  que  manejan 
información  depende  de  su  habilidad  en 
obtener,  gestionar  y  utilizar  sus  datos  de 
forma oportuna, efectiva y precisa. 
✔ La cantidad de datos disponible  es  cada  vez 
mayor y más compleja y, por ello, es cada vez 
más  evidente  el  valor  de  la  información 
como activo.
Definición de 
Bases de datos
✔ Se  puede  definir  como  una  colección 
   organizada  de  datos,  con  el  objetivo  de 
   estandarizar procesos y ser más eficientes
✔ “Una  base  de  datos  es  una  colección 
   organizada  de  información  estructurada  o 
   datos,      normalmente       almacenados 
   electrónicamente en un sistema informático”. 
Fuente: Oracle
Ejemplo en vivo
Les proponemos a todos los estudiantes que creen 
columnas para capturar datos de diferente índole en un 
mismo documento de Google SpreadSheets. 
✔ ¿Qué tan caótico piensan que será el proceso de 
manipulación de estos datos? 
✔ ¿Qué eventualidades se les ocurre qué puedan 
suceder?
✔ ¿Cuál es la utilidad principal de las bases de datos?
Tipos de bases de 
datos
Tipos de Bases de datos
                                  SQL: Colección de tablas que almacena un 
                                  conjunto específico de datos estructurados.
                                  NoSQL:  variedad de modelos en datos, 
                                  incluidos los formatos de clave-valor, 
                                  documento, columna y grafos.
                                  NewSQL: NewSQL tiene una escalabilidad 
                                  como NoSQL y garantiza estructura ACID de 
                                  RDBMS 
                                  Analytical: almacena y administra big data 
                                  usando Business Intelligence (BI).
                                  As a service: servidor que utiliza una 
                                  aplicación de base de datos que proporciona 
                                  servicios de base de datos a otros 
Fuente: Bid Data DZONE              programas informático.
Tipos de Bases de datos - 
 Se basa en el modelo de datos            Algunos ejemplos de bases de 
SQL
 relacionales, que almacena datos en      datos relacionales son MySQL, 
 forma de filas (tupla) y columnas        Microsoft SQL Server, Oracle, 
 (atributos), y juntos forman una tabla   etc.
 (relación). Utiliza SQL para 
 almacenar, manipular y mantener los 
 datos. Inventado por EF Codd en 
 1970. 
Tipos de Bases de datos - 
NoSQL                                     ✔ Es un tipo de base de datos que se 
                                           utiliza para almacenar una amplia 
                                           gama de conjuntos de datos. No solo 
                                           almacena datos en forma tabular sino 
                                           de varias maneras diferentes. 
                                        ✔ Surgió cuando aumentó la demanda de 
                                           creación de aplicaciones modernas, 
                                           creado por Carlo Strozzi en 1998. 
                                        ✔ NoSQL tiene una amplia variedad de 
                                           tecnologías en respuesta a las 
                                           demandas: Key-value (Riak, Redis), 
                                           Documentos (CouchDB, Mongo 
                                           DB), Grafos (Neo4j, GraphDB) y 
                                           Wide-column (Cassandra, HBase)
Fuente: 
Architectural assessment of NoSQL and NewSQL systems
Tipos de Bases de datos - 
NewSQL                         Creado en 2011 para abordar los 
                            desafíos de en SQL, fue diseñado para 
                            los sistemas de procesamiento de 
                            transacciones en línea (OLTP), 
                            cumpliendo con la estructura (ACID). 
                            Admite de forma nativa aplicaciones que 
                            tienen una gran cantidad de 
                            transacciones, son repetitivas en sus 
                            procesos y utilizan un pequeño 
                            subconjunto de procesos de recuperación 
                            de datos.
                            VoltDB es un sistema de base de datos 
                            NewSQL que proporciona una velocidad 
                            hasta 50 veces más rápida que SQL y 
                            más de ocho veces más rápida que 
Fuente: NewSQL definición      NoSQL.
Tipos de Bases de datos - 
Analytical
Es un sistema de solo            La información se actualiza 
lectura que almacena             regularmente para incluir datos 
datos históricos sobre           de transacciones recientes de los 
métricas comerciales,            sistemas operativos de una 
como el rendimiento de           organización.
ventas y los niveles de 
inventario. Los analistas de 
negocios, ejecutivos 
corporativos y otros 
trabajadores ejecutan 
consultas e informes en 
una base de datos 
analítica.
Tipos de Bases de datos - 
As a service                    DBaaS es un modelo de servicio de 
                             computación en la nube que brinda a 
                             los usuarios acceso a una base de 
                             datos sin necesidad de configurar 
                             hardware físico, instalar software o 
                             configurar el rendimiento. 
                             Todas las tareas administrativas y el 
                             mantenimiento están a cargo del 
                             proveedor de servicios, 
                             Se puede ofrecer un mayor control 
                             sobre la base de datos, pero puede 
                             variar según el proveedor (Amazon 
                             RDS, SQL Azure, FathomDB).
Fuente: Azure Database
Una definición 
mejorada
       Una definición 
            mejorada
La base de datos es un 
conjunto de datos no 
redundantes, almacenados 
en un soporte informático, 
organizados en forma 
independiente de su 
utilización y accesibles 
simultáneamente por 
distintos usuarios y 
aplicaciones.
Características importantes
     No sean redundantes                  Se refiere a la eficiencia en su 
                                          almacenamiento
     Están almacenados en soporte         Se refiere a la necesidad de contar con 
     informático:                         medios físicos de almacenamientos 
                                          que garanticen seguridad
     Están organizados en forma           No se necesita saber cómo están 
     independiente de su uso              almacenados para poder acceder a 
                                          los datos
     Pueden ser accedidos en forma        Se refiere a la flexibilidad de poder 
     simultánea                           utilizar un sistema específico
Mejoras ofrecidas 
por Bases de Datos
                    Sistema DBMS
  Las bases de datos clásicas 
  están administradas por un 
  sistema particular adecuado                      REEMPLAZAR 
  para tal fin denominado                           POR IMAGEN
  Sistema de Gestión de 
  Bases de Datos (DBMS: 
  Data Base Management 
  System)
Algunas mejoras ofrecidas
 ✔ Independencia: de la            ✔ Integridad: se ofrecen 
    representación de los datos       diversos niveles de 
    con respecto a su                 protección frente a fallos.
    almacenamiento.
 ✔ Eficiencia: se utilizan         ✔ Seguridad: complejo 
    técnicas  se utilizan             sistema para otorgar 
    técnicas específicas para         permisos de tal forma de 
    acelerar la consulta y            garantizar el acceso a 
    utilización de los datos          quien corresponda.
    almacenados en la base de 
    datos.
Algunas mejoras ofrecidas
 ✔ Centralización:  permite          ✔ Reusabilidad: situaciones 
     que sean administrados de          comunes de acceso y uso y 
     forma centralizada.                utilizarlas a través de 
 ✔ Acceso concurrente:  el              distintas aplicaciones.
     acceso por parte de 
     múltiples usuarios es 
     complejo, y existen muchas 
     situaciones donde los datos 
     podrían corromperse. El 
     DBMS provee mecanismos 
     para evitar estas 
     situaciones.
Sistemas de gestión 
de bases de datos
Definición
Data Base Management 
System                                       Ofrece un enfoque sistemático 
 Solución tecnológica utilizada para 
 optimizar y administrar el                  para administrar bases de datos 
 almacenamiento y la recuperación            a través de una interfaz para 
 de datos de las bases de datos.             usuarios
PARA RECORDAR
Es un software específicamente diseñado 
para definir, manipular y utilizar la 
información que contienen las bases de 
datos.
Permite realizar las tareas de 
administración para garantizar su 
operatividad y además mantener las 
características de integridad y seguridad 
de los datos
Arquitectura de tres 
capas
¿Qué es una Arquitectura?
En términos de software, la arquitectura 
representa la forma en que está 
estructurado el software. Pero es un 
término muy amplio… 
Entonces podemos pensarlo como la forma 
de generar la estructura necesaria para 
cumplir con los requerimientos del 
software. 
Sistema DBMS en 3 niveles
Extern Representación de los datos de la forma que los ve el usuario. 
      Aquí se ven los datos “tal cual están” a los ojos de un determinado 
o      usuario.
      Representación abstracta del contenido total de la base de datos. 
Lógic  Contiene la definición de todos los datos y además las reglas y 
o      mecanismos  correspondientes  a  la  seguridad  e  integridad.  Está 
      orientado al perfil del administrador de bases de datos.
Intern Representación de la base de datos de forma que lo entienda la 
      computadora o soporte físico donde se almacenan los datos. Aquí 
o      se ven definiciones como el tipo de soporte donde se guardan, el 
      espacio y los mecanismos de acceso.
Sistema DBMS en 3 niveles
1. Nivel interno: estructura de 
almacenamiento FÍSICA real y 
rutas de acceso.
2. Nivel conceptual o lógico: 
estructura y restricciones para 
toda la base de datos
3. Nivel externo o de vista: 
describe varias vistas de 
usuario
Modificación en sistemas 
   ✔ Al  modificar  el  nivel  externo… 
DBMS
        podemos       alterar     los     datos     sin 
        preocuparnos  por  su  estructura  o  su 
        almacenamiento.
   ✔ Al modificar el nivel lógico… podemos 
        alterar la estructura de organización de los 
        datos sin afectar la forma en que estos se 
        guardan. 
   ✔ Al modificar el nivel interno… podemos 
        cambiar la forma de almacenamiento de 
        los  datos  sin  alterar  su  estructura  y/o 
        contenido.
Principales tipos de 
sistemas de gestión 
de Bases de datos
Tipos de bases de datos 
comunes
Relacional
Relacionales
Responden al Modelo de Datos 
relacional    propuesto     por 
Edward Frank Codd en 1970, 
tal cual como podemos observar 
en la siguiente imagen:
Relacionales
    ✔ Representan  los  datos  como  un 
       conjunto   de   tablas   en    dos 
       dimensiones, formadas por filas y 
       columnas.  Cada  fila  representa  una 
       forma  de  relación  entre  entre  un 
       conjunto de valores y está identificada 
       de forma única. 
    ✔ A  menudo,  las  bases  de  datos 
       relacionales  contienen  múltiples 
       tablas relacionadas entre sí, lo cual 
       permite  que  la  información  pueda 
       estar  almacenada de una forma más 
       compacta. 
Relacionales
Su acceso es muy simple. El lenguaje de 
consulta estructurado (denominado SQL 
por sus siglas en inglés) permite una gran 
flexibilidad a la hora de realizar 
consultas y manipulación de datos en 
estas bases de datos.
¡Veremos SQL más a detalle en la siguiente clase! 
No SQL
No Relacionales
Modelo propuesto por Carlo Strozzi en 1998, como una base de 
datos  "relacional"  de  código  abierto  y  liviana  que  no  usa  SQL, 
desarrollado en principio para datos web (no estructurados) y por 
la necesidad de un procesamiento más rápido. 
No Relacionales
Engloba muchos subtipos que 
comparten como característica 
fundamental no estar 
organizados en tablas, y 
gracias a ello, pueden ser 
accedidos mediante otras formas 
acordes a su uso específico. 
☝¡Aunque aclaramos que muchas de ellas también pueden ser accedidas por SQL!
Algunos ejemplos
  ✔ Orientadas a objetos: almacenan               ✔ Orientadas  a  documentos: 
     los  objetos  tal  cual  como  son              guardan conjuntos de texto muy 
     utilizados  en  los  lenguajes  de              grandes  (los  documentos),  y 
     programación orientados a objetos.              este texto puede estar a su vez 
  ✔ Orientadas a grafos: guardan los                 estructurado  dentro  de  cada 
     datos  como  nodos  y  relaciones,  y           documento.
     pueden  ser  consultados  de  forma 
     automatizada.
Ventajas y desventajas 
de bases de datos 
relacionales y No 
relacionales
 Ventajas y desventajas 
 bases SQL
                        Ventajas                                          Desventajas
    Simplicidad del modelo: Muy simple, no               Mantenimiento: difícil por acumulación de datos 
    requiere consultas complejas                         en el tiempo
    Fácil uso: usuarios pueden acceder/recuperar         Costo: se generan costos fijos y variables por 
    fácilmente la información requerida en segundos      mantenimiento
    sin caer en la complejidad.
    Precisión: bien definidas y organizadas, no          Almacenamiento físico: requiere mucha 
    duplicados.                                          memoria física.
Ventajas y desventajas 
bases SQL
                      Ventajas                                        Desventajas
    Integridad de datos: brindan coherencia en        Poca escalabilidad: los datos no son escalables 
    todas las tablas.                                 en diferentes servidores de almacenamiento físico
    Normalización: se divide la información en partes  Estructura compleja: solo puede almacenar 
    manejables para reducir el tamaño del             datos en forma tabular, dificultando 
    almacenamiento                                    representación compleja.
    Colaboración: muchos usuarios interactuando al    Reducción de performance en tiempo: mayor 
    tiempo                                            complejidad
    Integridad y Seguridad: Sistemas                  Menor tiempo de respuesta: muchos datos 
    medianamente confiables                           poca eficiencia
 Ventajas y desventajas 
 bases No SQL
                       Ventajas                                             Desventajas
   Modelo flexible: puede almacenar y combinar            Falta de estandarización: No existe un estándar 
   cualquier tipo de datos, tanto estructurados como no   que defina reglas y roles de las bases de datos 
   estructurados                                          NoSQL. 
   Modelo de datos en evolución: permite actualizar  Algunos problemas de backup: No está del todo 
   dinámicamente el esquema para evolucionar con los      desarrollado este ámbito en este tipo de bases de 
   requisitos cambiantes sin interrupciones.              datos.
   Fácil escalamiento: pueden escalar para                Consistencia: NoSQL prioriza la escalabilidad y el 
   adaptarse a cualquier tipo de crecimiento de datos     rendimiento, pero cuando se trata de la consistencia 
   manteniendo un bajo costo.                             de los datos no es tan eficiente.
 Ventajas y desventajas 
 bases No SQL
                       Ventajas                                               Desventajas
  Alto performance: gran rendimiento, medido en            Difícil mantenimiento: pueden llegar a ser 
  términos de rendimiento y latencia (retraso entre la     costosos y requerir de personal especializado
  solicitud y la respuesta real).
  Acceso libre: no requieren tarifas de licencia           Poco nivel de madurez: Son relativamente más 
  costosas y pueden ejecutarse en hardware                 nuevas que las bases relacionales por ende tienen 
  económico, lo que hace que su implementación sea         todavía mucho por mejorar.
  rentable.
Funciones y lenguajes
Tres tipos de funciones
básicas
Definició      Manipulación              Segurida
   n                                       d
 DD              DM              DC
  L                L               L             TCL
Data Definition 
               Data Manipulation               Transaction Control 
 Language                     Data Control Language
                  Language                        Language
DDL (Data Definition 
Language)
CREATE: crear una nueva base de          COMMENT: se utiliza para 
datos o sus objetos.                     agregar comentarios en el 
DROP: se utiliza para eliminar los       Diccionario de datos.
objetos de la base de datos.             RENAME: se utiliza para 
ALTER: se usa para reestructurar         cambiar el nombre de los 
las tablas de la base de datos           objetos de la base de datos.
(Columnas, tipos, índices)
TRUNCATE: se utiliza para vaciar 
una tabla de base de datos. (Datos 
fuera , Estructura =)
DML (Data Manipulation Language)
 SELECT: se utiliza para         DELETE: se utiliza para 
 seleccionar  valores de         eliminar los registros de una 
 columna de la base de           tabla de base de datos.
 datos y sus tablas.             LOCK: se utiliza para 
 INSERT: se utiliza para         bloquear los privilegios de 
 insertar nuevos registros       lectura o escritura en una 
 en las tablas de la base        tabla.
 de datos.                       MERGE: se utiliza para 
 UPDATE: se utiliza para         fusionar los registros de una 
 actualizar los registros de     tabla de base de datos.
 la tabla de la base de 
 datos existente.
DCL (Data Control Language)
GRANT: se utiliza para            Las tareas sobre las que se 
otorgar derechos o                pueden conceder o denegar 
privilegios particulares a los    permisos son:
usuarios de la DB
REVOKE: se utiliza para            ✔ CONNECT
retirar los derechos o             ✔ SELECT
privilegios de los usuarios 
de la DB                           ✔ INSERT
                                   ✔ UPDATE
                                   ✔ DELETE
                                   ✔ USAGE
DCL (Data Control Language)
 COMMIT: se utiliza para          Permite gestionar transacciones 
 confirmar la transacción         en la base de datos. Estos se 
 ROLLBACK: se utiliza para        utilizan para administrar los 
 rehacer los cambios realizados   cambios realizados por las 
 por una transacción por error.   declaraciones DML. También 
                                  permite que las declaraciones 
 SAVEPOINT: agrega un punto       se agrupen en transacciones 
 de control al proceso de         lógicas.
 transacción.
 SET TRANSACTION: Define 
 los diferentes caracteres de 
 una transacción
Para pensar
¿Qué casos de implementación en la 
industria conoces o has escuchado hablar, 
tanto de SQL como de NoSQL en 
empresas?
Contesta en el chat de zoom
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Ejemplo en vivo
Analicemos como se puede manipular el 
lenguaje SQL en Python usando la librería 
SQLlite. Aprenderemos cómo crear 
diagramas entidad-relación y los conceptos 
básicos para creación de tablas 
Backup, mecanismos 
de conexión y 
auditoría
Backup
             Backup
Para prevención y actuación frente a 
contingencias, disponer de una o 
varias copias, parciales o totales, de 
los datos de la base de datos, que 
pueden residir en el mismo soporte 
físico que la base de datos, o 
preferiblemente separado del lugar 
principal donde se guardan los 
datos.
Mecanismos de 
conexión
Parámetros de conexión
Las aplicaciones “hablan” con 
las bases de datos a través de 
los mecanismos de conexión 
ingresando ítems de información 
—llamados “parámetros de 
conexión”— para poder 
entenderse e interactuar 
ágilmente entre sí.
Parámetros de conexión
  ✔ Los parámetros  de  conexión  se  guardan  en 
     archivos con una estructura especial, y luego 
     son    accedidos    y   utilizados   por   las 
     aplicaciones. 
  ✔ Una vez que son accedidos, la base de datos 
     entiende  que  puede  recibir  órdenes  y 
     consultas  a  partir  de  dicha  aplicación.  Esas 
     son  las  que  permiten  al  usuario  interactuar 
     con los datos, utilizando a la aplicación como 
     interfaz para dicho acceso.
Auditoría
Auditoría
✔ Serie de registros automáticos que 
tienen en cuenta todos los detalles de 
los accesos de los usuarios a la base 
de datos.
✔ Desde la fecha, hora e identificación 
del usuario hasta el lugar y la forma 
exacta  en  que  ese  usuario  realizó 
operaciones.
✔ De esta forma, se cuenta con un nivel 
de  seguridad  mejorado  para  la 
utilización  por  parte  de  múltiples 
usuarios.
Operatividad de archivos 
       planos
Discutiremos acerca de cuáles son los pros y contras 
del uso de spreadsheets y archivos planos
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Operatividad de 
archivos planos
En esta oportunidad nos reuniremos en grupos de 
máximo 5 personas.
Discutiremos acerca de qué tan bueno es que los datos 
están descentralizados y que cualquier persona pueda 
editarlos. Podemos comentar de algunos ejemplos de la 
vida cotidiana. Se recomienda generar una lista de pros 
y contras para debatir una vez se termine la discusión. 
CLASE N°24
Glosario
Bases de datos: colección ordenada de       Backup: sistema que permite guardar 
datos con el fin de estandarizar procesos   periódica y regularmente la información 
y ser más eficientes                        almacenada en bases de datos para evitar 
                                          pérdidas 
Sistema de gestión DBMS: proceso            Mecanismos de conexión: son todos 
mediante el cual se optimiza y administra   aquellos parámetros que nos permiten 
el almacenamiento y recuperación de         interactuar con las bases de datos
datos 
                                          Auditoría: proceso mediante el cual se 
Arquitectura de 3                           mejoran las estructuras de un sistema de 
capas:representación del sistema DBMS       bases de datos y se lleva el registro de logs 
en componentes: externo, lógico e interno   y acciones de usuarios.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Bases de datos
      ✓ Sistemas de gestión de bases de datos
      ✓ Tipos de sistemas de gestión de bases de datos
      ✓ Backup, mecanismos de conexión y auditoría
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 12. DATA SCIENCE
   Introducción al 
 análisis predictivo 
    con Regresión
Temario
           11                 12                 13
     Preprocesamien     Introducción al       Modelos 
      to estadístico       análisis        analíticos para 
      de los datos      predictivo con          DS I
                           regresión       ✓
     ✓ Procesamiento      ✓ Nociones          Modelo analítico
        como concepto       básicas        ✓ Machine 
     ✓ Outliers           ✓ Aplicaciones      Learning
                            prácticas      ✓ Ciencia de 
     ✓ Datos ausentes     ✓                   datos: etapas
                            Interpretación
     ✓ Reducción de       ✓                ✓ Conceptos 
        dimensionalidad     Usar el 
                            modelo            complementario
                                              s
Objetivos de la clase
         Conocer los conceptos de correlación, 
         causalidad y dependencia y función lineal.
         Entender el modelo de regresión lineal y 
         múltiple así como condiciones a cumplir
         Conocer cómo usarlo
MAPA DE CONCEPTOS                                                 Correlación
                                 Concepto de 
                                 Regresión
                                                                  Causalidad y dependencia
                                 Repaso de funciones 
                                 lineales                         Función lineal e ideas previas
                                                                  Definición de regresión
       Análisis                  Definición y ubicación           Condiciones     para     uso    de 
       predictivo con            de la regresión en DS            regresión
       regresión
                                                                  Regresión dentro de DS
                                 Regresión lineal                 Concepto de modelo
                                 simple y múltiple
                                                                  Regresión lineal simple y múltiple
                                 Interpretación y uso             Test beta, coeficiente de 
                                 del modelo                       determinación
                                 Aplicaciones 
                                 prácticas
Introducción a las 
nociones básicas de 
regresión
Correlación, 
causalidad y 
dependencia
Punto de partida
������ Planteamos la hipótesis de que podría           ������ Si este tipo de dependencia existe, 
existir algún tipo de dependencia de               queremos ver de qué forma se 
una variable con respecto a la otra.               da esa relación. 
Supongamos entonces, que tenemos dos 
variables: x e y, ahora veamos el 
siguiente gráfico:
                     Pareciera que las variables tienen una 
                     fuerte correlación positiva, y si lo 
                     pensamos en términos de dependencia, 
                     quiere decir que cuando la variable x 
                     aumenta, entonces también lo hace la 
                     variable y, y viceversa.
                     Atención: cuando planteamos que ante 
                     un cambio en la variable x se produce un 
                     cambio en la variable y. A esto lo 
                     llamaremos dependencia de la 
                     variable y hacia la variable x.
Como una 
función 
matemática                         REEMPLAZAR 
estándar                            POR IMAGEN
✓                    donde la variable y es una función 
de x, o sea que en definitiva y depende del 
cambio de x.
✓ Otra forma de decir lo mismo es que x es una 
variable independiente, o sea que su cambio 
no depende de nuestro modelo.
Correlación no 
implica 
 ✓  La frase "correlación no implica causalidad" se                     REEMPLAZAR 
causalidad
     utiliza para señalar que la correlación entre dos                  POR IMAGEN
     variables no significa necesariamente que 
     una variable haga que ocurra la otra. 
 ✓ Correlación es una relación o conexión mutua 
     entre dos o más cosas (o variables)
 ✓ Causalidad significa que existe una relación 
     entre dos eventos en la que un evento afecta al 
     otro.
              Correlación - 
                causalidad
Fuente: Lovestats (2019). “Cartoons.” The LoveStats Blog. Disponible en: 
lovestats.wordpress.com.
              Correlación - 
                causalidad
Fuente: Data Sources U.S Census Burau and National Science Foundation
Función lineal
  Repaso
✓                         ������ donde a y b son 
números reales.
✓ Esta función genera una recta en el 
plano.
✓ El valor de a (ordenada al origen) 
muestra cuál es el valor de y 
cuando x vale 0.
✓ El valor de b (pendiente), por su 
parte, indica el grado de 
inclinación de la recta.
PARA RECORDAR
A tener en cuenta:
✓ Una recta totalmente horizontal > tiene una 
pendiente igual a cero. 
✓ Una recta inclinada en el sentido de la 
correlación positiva > tiene una pendiente 
positiva. 
✓ Una recta inclinada en el sentido de la 
correlación negativa > tiene una pendiente 
negativa.
✓ Una recta vertical > tiene pendiente infinita.
Ejemplos
El mismo valor de a con distintos           Un valor fijo de b para distintos 
valores de b, aquí cambia la pendiente o    valores de a, aquí cambia la posición 
inclinación                                 de la recta pero su inclinación 
                                           permanece igual.
Definición y ubicación 
de la regresión dentro 
de Data Science
Ideas previas
Contenido destacado
Si tenemos un conjunto de puntos en las 
variables x e y, y de alguna forma y 
depende de x, una forma es trazar una 
recta que de alguna manera puede 
representar a esos puntos, tomando un 
criterio para la representación y trazar una 
recta que cumpla con él.
Por ejemplo, una recta que pase “lo más 
al centro posible” del conjunto de 
puntos...
                     Aquí es realizamos un ajuste de la recta 
                     a los datos. A la técnica que utilizamos 
                     para realizar este ajuste a un conjunto de 
                     puntos por parte de una recta la 
                     llamaremos “método de mínimos 
                     cuadrados”.
Regresión
✓ La regresión es un método para 
determinar la relación estadística 
entre una variable dependiente y 
una o más variables independientes. 
✓ La variable independiente del 
cambio está asociada con el cambio 
en las variables independientes. 
✓ El modelo está dado por la ecuación: 
y꞊a +bx. Con a= Intercepto y 
b=pendiente
¡Importante!
Un modelo de regresión lineal no es 
simplemente una recta de ajuste por 
mínimos cuadrados, sino que deben 
cumplirse una serie de condiciones 
rigurosas que deben probarse 
matemáticamente. 
Condiciones para el 
uso de Regresión
Condiciones de uso
✓ Normalidad: Los residuales del modelo 
tienen una distribución normal.
✓ Relación lineal: Existe una relación 
lineal entre las variables independientes 
y la dependiente
✓ Independencia: no hay correlación 
entre los residuales como lo que ocurre 
en series de tiempo
✓ Homocedasticidad: Los residuales 
tienen una varianza constante. 
Si estos supuestos no se cumplen la 
regresión pierde potencia (poder predictivo)
¿Cómo funciona el 
método de mínimos 
✓ Se toma cada punto individual y se 
cuadros?
calcula su distancia vertical a la 
recta (denominada error y simbolizada 
con la letra e). 
✓ Se realiza entonces la suma de todas 
las distancias verticales elevadas al 
cuadrado. 
✓ El objetivo es minimizar los errores: 
Ejemplo
✓ Existe una fórmula (¡que no veremos                           REEMPLAZAR 
   aquí!) para encontrar precisamente la                        POR IMAGEN
   recta que cumple con la condición de que 
   la fórmula de mínimos cuadrados.
✓ El método de mínimos cuadrados es 
   el método por defecto que utiliza el 
   modelo de regresión lineal.
Regresión dentro de 
Data Science
Machine Learning está asociado a tres 
tipos de problemas
1. Aprendizaje Supervisado
2. Aprendizaje No supervisado
3. Aprendizaje por refuerzo
La regresión hace parte de lo que se 
conoce como Aprendizaje 
Supervisado
La regresión se puede llevar a cabo 
por un amplio número de algoritmos 
(e.g Modelo lineal, XGBOOST, Random 
Forest, Regresión Ridge, Lasso y 
ElasticNet)
        Tipos de problemas que 
         resuelve el Aprendizaje 
                        Supervisado
     Problemas de                             Problemas de 
     clasificación                            regresión
     Necesitan predecir la clase más          En vez de predecir categorías, 
     probable de un elemento, en              predicen valores numéricos. Es 
     función de un conjunto de variables      decir, la variable target en un 
     de entrada. Para este tipo de            problema de regresión es de tipo 
     algoritmos, la variable target o         cuantitativa.
     respuesta, es una variable de 
     tipo categórica.
Aprendizaje 
Supervisado
Entonces, ¿cómo sé si tengo que 
utilizar un algoritmo de 
clasificación o de regresión? 
Depende del tipo de problema que 
plantea mi variable a predecir ������
    Regresión dentro de DS
Ruta de trabajo elemental para trabajo con algoritmos de Scikit-Learn
Regresión lineal 
simple y múltiple, 
nociones y
aplicaciones 
prácticas
El concepto modelo
Para poder aplicar un 
modelo de regresión lineal
                1                              2                              3
        Aplicar un ajuste            Hacer verificaciones             Además de tener 
        por el método de              para chequear que              una recta de ajuste, 
       mínimos cuadrados.            el modelo sea válido                     la 
                                           y bueno.                   denominaremos 
                                                                          recta de 
                                                                     regresión, porque 
                                                                       cumple con las 
                                                                       condiciones ������.
Regresión 
lineal simple
✓ Si tenemos dos variables X 
    (independiente) y Y (dependiente) y                             REEMPLAZAR 
    tomamos una muestra de tamaño n                                  POR IMAGEN
    tenemos: (x1,y1), (x2,y2),...... (xn,yn)
✓ Para cada valor xi tenemos una variable 
    aleatoria: Yi= Y|X 
    De tal forma que las observaciones 
    (y1,y2,...,yn) son una realización de las 
    variables (Y1,Y2,....,Yn)
✓ En este modelo se tiene solo una 
    variable dependiente (Y) y una 
    independiente (X), el modelo está dado 
    por: y= a+bx
Regresión 
lineal múltiple
✓ Es una generalización del modelo de 
    regresión lineal simple                                     REEMPLAZAR 
✓ Para este caso tenemos una variable                           POR IMAGEN
    dependiente (Y) con varias independientes 
    (X1,X2,X3,...,Xn) llamadas regresores
✓ Si fijamos n niveles tenemos:
✓ Por ende para cada observación tenemos:
          
Para pensar
Imaginemos que trabajamos para una compañía que 
vender productos de primera necesidad. Si nos piden 
que desarrollemos un algoritmos que obtenga la 
propensión (similar a probabilidad) para clientes a la 
marca, ¿tendría sentido usar un modelo de 
regresión?
Nota: la propensión es una medida de adherencia
Contesta en el chat de Zoom 
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Interpretación de 
regresión
El test de beta
Primera condición 
a cumplir
✓ Verificación de la pendiente, 
asociado al valor b de la función:
✓ La pregunta a responder es si 
efectivamente la recta es válida 
como representante del 
conjunto de datos.
Ejemplo  1
✓ Hay una clara relación positiva, se 
ve que ante mayores valores de x, 
llegamos a mayores valores de y.
Ejemplo  2
✓ Es clara la situación pero aquí, dada 
la relación negativa, la variable y 
disminuye conforme x aumenta y 
viceversa
Ejemplo  3
✓ Aquí la recta parece tener una 
pendiente prácticamente nula y 
la recta parece representar bastante 
apropiadamente los datos.
Pero tenemos un 
problema…
Desde un punto de vista puramente 
matemático:
✓ Supongamos, llevando este ejemplo       ✓ El término que acompaña a la x, 
   al extremo, que la pendiente de b es       multiplicado por cero, se anula.
   efectivamente igual a cero. Entonces   ✓ Esto significa que la variable y deja 
   la función:                                de ser dependiente de x y como no 
                          se transforma       tenemos dependencia, ya no 
                                              podemos aplicar el modelo de 
                                              regresión ������.
          Checklist
✓ ¿Tienen las puntos una relación entre      ✓ La recta de mínimos cuadrados, 
    sí? Sí.                                      ¿representa adecuadamente a los 
✓ ¿Tiene la relación una forma lineal? Sí.       datos? No
✓ ¿Existe una recta que pueda ser            ✓ ¿Por qué? Porque al ser la pendiente 
    calculada con el método de mínimos           de la recta igual a cero, no hay 
    cuadrados, y que en ese sentido pase         dependencia de la variable y hacia la 
    por la parte “más central” de los            variable x. Y como no hay 
    datos? Sí.                                   dependencia, no existe un modelo 
✓ ¿Puede armarse un modelo de                    de regresión lineal que 
    regresión lineal a partir de este            represente adecuadamente a 
    ajuste? No                                   estos datos.
Significancia de 
coeficientes
✓ Este es un test estadístico para 
verificar que la pendiente no sea 
cero. Puede ser positiva o negativa, 
pero no debería ser cero.
✓ No entraremos aquí en los detalles 
del test, pero sí aprenderemos a ver 
sus salidas. 
✓ En Python podemos obtener esta 
información con el paquete extra 
pingouin
    Las siguientes salidas fueron obtenidas con este paquete, y 
    corresponden al primero y tercer caso planteados anteriormente:
           Caso 1      names        coef         se           T       pval         r2     adj_r2   CI[2.5%] CI[97.5%]
             0        Intercept      7.94       4.65       1.71       0.09       0.94       0.94      -1.22      17.10
             1             x1        0.68       0.01      58.94       0.00       0.94       0.94       0.66       0.71
           Caso 3      names        coef         se           T       pval         r2     adj_r2   CI[2.5%] CI[97.5%]
             0        Intercept   251.70        4.53      55.59       0.00       0.01        0.0     242.77     260.63
             1             x1       -0.01       0.01      -1.02       0.31       0.01        0.0      -0.03       0.01
    Para el caso 1, la pendiente estará entre los valores 242.77 y 
    260.63. Para el caso 2, la pendiente estará entre los valores -0.03 y 
    0.01. Esto quiere decir que, con un alto nivel de confianza, la 
    pendiente estará en un intervalo que incluye al valor cero, 
    esto es, bien podría ser cero con una probabilidad del 95%.
El intervalo de confianza
✓ Proporciona el mismo resultado que        ✓ Si tiene un valor positivo y uno 
    el p-value y además agrega la              negativo, quiere decir que cubre al 
    información acerca de dónde                cero, y por lo tanto se mantiene la 
    podría encontrarse el valor de la          conclusión de que la pendiente 
    pendiente (efectos de cada                 (efecto) no es significativa.
    variable). 
✓ Si el intervalo de confianza solo 
    tiene valores positivos = la 
    pendiente es positiva; si tiene 
    valores negativos = la pendiente es 
    negativa. 
El coeficiente de 
determinación
               Segunda 
               condición a 
 REEMPLAZAR    ✓ Este es un número que puede tomar valores entre 
 POR IMAGEN    cumplir
                 0 y 1, y puede interpretarse también de manera 
                 porcentual (de 0% a 100%). 
               ✓ Se simboliza como R2 y representa el 
                 porcentaje de variabilidad de los datos 
                 explicada por el modelo de regresión lineal.
¿Variabilidad de los 
datos?
✓ Es la cualidad que le da la forma al       ✓ Por lo tanto, un modelo que 
    conjunto de datos.                           funcione bien será el que mejor 
✓ La consecuencia de esta noción de              explique la variabilidad de los 
    variabilidad es que si podemos               datos. ������
    explicar o representar la variabilidad 
    de los datos, podremos explicar su 
    forma y así entenderlos mejor. 
Ejemplos
 Conjunto de puntos que tiene una recta         Conjunto de datos no parece ser lo 
 de ajuste que parece funcionar muy             más conveniente utilizar una 
 bien, por cuanto pasa bastante bien por        recta para representar los puntos.
 el “centro” de los datos.
    Veamos los resultados con el valor de R2 resaltado:
         Caso 1      names         coef         se           T       pval          r2     adj_r2   CI[2.5%] CI[97.5%]
           0        Intercept      7.94        4.65       1.71        0.09       0.94       0.94       -1.22      17.10
           1              x1       0.68        0.01      58.94        0.00       0.94       0.94        0.66       0.71
         Caso 2      names         coef         se           T       pval          r2     adj_r2   CI[2.5%] CI[97.5%]
           0        Intercept    251.70        4.53      55.59        0.00       0.01        0.0      242.77     260.63
           1              x1       -0.01       0.01       -1.02       0.31       0.01        0.0       -0.03       0.01
    Pasa satisfactoriamente el test de beta, con lo cual es un modelo válido, pero 
    ciertamente poco útil. Convendría aplicar otro tipo de modelo que se ajuste a 
    la forma curva de los datos. Tengamos entonces siempre en cuenta que el 
    valor de R2 solamente tiene sentido una vez que se verificó y se pasó 
    satisfactoriamente el test de beta. Buscamos un modelo con un valor de 
    R2 que sea lo más pequeño posible.
Usar el
modelo
Aplicar el modelo para 
predecir valores 
desconocidos
✓ Realizaremos predicciones para        ✓ El nuevo punto a predecir se 
   valores de y a partir de valores         colocará sobre la recta, y se podrá 
   de x que no habían sido utilizados       afirmar que el valor predicho tendrá 
   antes.                                   una “fidelidad” igual al valor de 
✓ Utilizando la recta de regresión          R.
   para hacer la predicción. 
Ejemplo
Si hay algún valor de x faltante en el                            REEMPLAZAR 
conjunto de datos, podemos suponer que su                          POR IMAGEN
valor de y correspondiente corresponderá al 
señalado por la recta para dicho valor de 
x, como se muestra con el punto cuadrado de 
color verde.
������ En Python el modelo se utiliza simplemente a 
través de la función predict().
Aplicaciones 
prácticas
Ejemplo en vivo
Estudiaremos un caso real aplicado donde 
podremos ver el uso de la regresión. 
EJEMPLO EN VIVO
¿Qué factores están impulsando la discriminación 
salarial entre hombres y mujeres en su 
organización?
 ✓ Contexto empresarial
                                                                     REEMPLAZAR 
Su empresa está pasando por una revisión                              POR IMAGEN
interna de sus prácticas de contratación y 
compensación a los empleados. En los últimos 
años, su empresa ha tenido poco éxito en la 
conversión de candidatas de alta calidad que 
deseaba contratar. La gerencia plantea la 
hipótesis de que esto se debe a una posible 
discriminación salarial y quiere averiguar qué la 
está causando.
EJEMPLO EN VIVO
✓ Problema empresarial: 
Como parte de la revisión interna, el 
departamento de recursos humanos se ha 
acercado a usted para investigar específicamente 
la siguiente pregunta: "En general, ¿se les paga 
más a los hombres que a las mujeres en su 
organización? Si es así, ¿qué conduciendo esta 
brecha?"
✓ Contexto analítico: 
Cuenta con una base de datos de empleados que 
contiene información sobre varios atributos como 
rendimiento, educación, ingresos, antigüedad, 
etc. 
EJEMPLO EN VIVO
Algunos 
gráficos 
La mediana de hombres es ligeramente mayor 
interesantes
que la mediana para mujeres, la variabilidad es 
similar en ambos casos
EJEMPLO EN VIVO
Algunos 
gráficos 
Se logra ver que existe alguna relación entre la 
interesantes
edad y el salario. Sin embargo la relación lineal 
no es tan fuerte, lo cual se puede evidenciar 
con algunos valores atípicos 
EJEMPLO EN VIVO
Algunos 
gráficos 
A incrementar el nivel de educación parece 
interesantes
incrementar, lo cual se ve con la mayor 
mediana para el PhD y la menor para High 
School.
EJEMPLO EN VIVO
Algunos 
gráficos 
A mayor nivel de experiencia incrementa el 
interesantes
salario. Esta es una de las relaciones más 
fuertes de las variables que hemos analizado 
hasta ahora.
EJEMPLO EN VIVO
Algunos 
gráficos 
En este caso se puede ver que los hombres 
interesantes
tienden a ganar un poco más que las 
mujeres en todos los niveles de educación.
EJEMPLO EN VIVO
Algunos 
gráficos 
Cuando hacemos la relación teniendo en 
interesantes
cuenta el tipo de trabajo pareciera que no 
siempre los hombres tienden a ganar más 
que las mujeres (e.g Manager, Data 
Scientist, Graphic Designer, IT entre otros), 
con alta variabilidad.
EJEMPLO EN VIVO
Creamos un modelo lineal simple por medio de la librería statsmodels de la 
siguiente forma:
model1 = 'pay~age'
lm1   = sm.ols(formula = model1, data = Data).fit()
print(lm1.summary())
Miremos el resultado de el ajuste del modelo lineal simple
                                  OLS Regression Results                            
         ==============================================================================
         Dep. Variable:                    pay   R-squared:                       0.285
         Model:                            OLS   Adj. R-squared:                  0.284
         Method:                 Least Squares   F-statistic:                     397.5
         Date:                Fri, 31 Dec 2021   Prob (F-statistic):           1.04e-74
         Time:                        20:54:35   Log-Likelihood:                -11384.
         No. Observations:                1000   AIC:                         2.277e+04
         Df Residuals:                     998   BIC:                         2.278e+04
         Df Model:                           1                                         
         Covariance Type:            nonrobust                                         
         ==============================================================================
                              REEMPLAZAR 
                          coef    std err          t      P>|t|      [0.025      0.975]
                               POR VIDEO
         ------------------------------------------------------------------------------
         Intercept   6.206e+04   2062.885     30.085      0.000     5.8e+04    6.61e+04
         age          939.2501     47.109     19.938      0.000     846.806    1031.694
         ==============================================================================
         Omnibus:                        6.360   Durbin-Watson:                   1.905
         Prob(Omnibus):                  0.042   Jarque-Bera (JB):                6.421
         Skew:                           0.182   Prob(JB):                       0.0403
         Kurtosis:                       2.853   Cond. No.                         134.
         ==============================================================================
EJEMPLO EN VIVO
Creamos un modelo lineal de regresión múltiple por medio de la librería 
statsmodels de la siguiente forma:
model2 = 'pay~age + gender'
lm2    = sm.ols(formula = model2, data = Data).fit()
print(lm2.summary())
Miremos el resultado de el ajuste del modelo de regresión múltiple
                                        OLS Regression Results                           
       
      ==============================================================================
      Dep. Variable:                    pay   R-squared:                       0.319
      Model:                            OLS   Adj. R-squared:                  0.317
      Method:                 Least Squares   F-statistic:                     233.2
      Date:                Fri, 31 Dec 2021   Prob (F-statistic):           8.10e-84
      Time:                        21:02:16   Log-Likelihood:                -11359.
      No. Observations:                1000   AIC:                         2.272e+04
      Df Residuals:                     997   BIC:                         2.274e+04
      Df Model:                           2                                         
      Covariance Type:            nonrobust                                         
      ==================================================================================
                              REEMPLAZAR 
                           coef    std err          t      P>|t|      [0.025      0.975]
                               POR VIDEO
      ----------------------------------------------------------------------------------
      Intercept       5.674e+04   2151.480     26.373      0.000    5.25e+04     6.1e+04
      gender[T.Male]  9279.3180   1317.787      7.042      0.000    6693.364    1.19e+04
      age              948.5266     46.022     20.610      0.000     858.216    1038.837
      ==============================================================================
      Omnibus:                        9.898   Durbin-Watson:                   1.871
      Prob(Omnibus):                  0.007   Jarque-Bera (JB):                9.345
      Skew:                           0.197   Prob(JB):                      0.00935
      Kurtosis:                       2.737   Cond. No.                         148.
      ==============================================================================
EJEMPLO EN VIVO
Interpretación del 
modelo
✓ Coeficiente género: Solo muestra         ✓ Coeficiente edad: si la edad 
   masculino (T.male), porque la categoría     aumenta en un año, se espera que 
   femenina se toma como la categoría          el salario aumente en 948,5 USD. 
   predeterminada. (Tenga en cuenta que    ✓ Intercepto: Representa el efecto 
   la elección de la categoría                 común a todos los individuos (salario 
   predeterminada no importa; fácilmente       base) en este caso es de 56740 USD 
   podríamos haber elegido hacer               al año
   masculino como categoría                ✓ Coeficiente R²: 0.319 es bastante 
   predeterminada y, por lo tanto, el          bajo solo 31.9% de la variabilidad es 
   coeficiente de género sería T.female).      explicada por el modelo lineal
   El coeficiente 9279.3180 se interpreta 
   de la siguiente manera: para 
   empleados de la misma edad, en 
   promedio, los hombres ganan 
   9279,3180 USD más que las mujeres.
                     3
     Estructurando un 
       Proyecto de DS 
Deberás entregar el tercer avance de tu proyecto final. Crearás un notebook que deberá 
            (parte I)
tener en primera instancia un abstract (250/500 palabras) de acuerdo al dataset elegido 
del desafío “Visualizaciones en Python”. Además se deben establecer las preguntas 
e hipótesis de interés sobre el dataset elegido. Finalmente, deberás generar 
visualizaciones (univariadas, bivariadas o multivariadas) junto con resúmenes 
numéricos acompañado de la interpretaciones respectivas que permitan responder la 
              pregunta problema.
Recordemos
                                     Elegimos un dataset de interés
    Desafío anterior:                Realizamos gráficos con 
 Visualización en Python             Matplotlib
                                    Realizamos gráficos con Seaborn
                                   Obtención de Insights preliminares
                                                  ������
     DESAFÍO 
     ENTREGABLE
Estructurando un 
Proyecto de DS (parte I)
                                                 Formato
Consigna                                            ✓ Entregar un archivo con formato .ipynb. 
Continuarás trabajando con base en lo                   Debe tener el nombre 
realizado en el Desafío entregable:                     “ProyectoDS_ParteI_+Apellido.ipynb”
Visualización en Python, en esta 
oportunidad deberás complementar con lo           Sugerencias
siguiente:                                          ✓ Preparar el código y probar los 
 1.  Generar preguntas de interés o                    resultados con subconjuntos del 
     hipótesis de interés sobre el dataset             conjunto original.
     elegido para el proyecto final.               ✓ Link video explicativo
 2.  Crear visualizaciones (univariados, 
     bivariados o trivariados) junto con         Aspectos a incluir:
     resúmenes numéricos básicos acordes           ✓ El código debe estar hecho en un 
     con los tipos de variables disponibles.           notebook y debe estar probado.
 3.  Interpretar los resultados obtenidos
CLASE N°12 
Glosario
Función lineal: cualquier forma lineal         Variable dependiente: se le conoce 
que satisfaga la ecuación y= mx +b             también como variable objetivo o a 
Regresión: es un método para                   modelar, es la que intentamos predecir
determinar la influencia de variables          Condiciones para modelo regresión 
independientes en una variable                 lineal: necesitamos varias condiciones 1) 
dependiente                                    linealidad 2) homocedasticidad (varianza 
Variable independiente: son todas              constante) 3) normalidad de la variable 
aquellas variables que permiten explicar a     respuesta e 4) independencia de 
una variable dependiente                       residuales
CLASE N°12 
Glosario
Interpretación de intercepto en               Coeficiente de determinación: es una 
regresión: coeficiente que representa el      métrica para cuantificar la bondad de 
efecto común a todos los individuos           ajuste del modelo de regresión, indica el 
Interpretación de pendiente en                porcentaje de varianza explicado por la 
regresión: representa el efecto aleatorio     regresión, mientras más cercano a uno es 
de una variable independiente en la           más deseable siempre y cuando se elija el 
variable respuesta.                           menor número de variables posibles
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Análisis predictivo con regresión
      ✓ Interpretación de regresión
      ✓ Aplicaciones prácticas & ejemplo en vivo de un 
        caso real.
Opina y valora 
esta clase
Muchas 
gracias.
               Encuesta
               sobre esta clase
               Por encuestas de Zoom
               ¡Terminamos la clase! 
               Cuéntanos qué temas te resultaron más complejos de 
               entender. Puedes elegir más de uno. Vamos a 
               retomar aquellos temas que resultaron de mayor 
               dificultad en el próximo AfterClass.
Esta clase va a ser
grabad
  a
      Clase 36. DATA SCIENCE
 Análisis Bivariado
Temario
               35                      36                      37
            Análisis                Análisis                Análisis 
          univariado y              bivariado             multivariado
            gráficos
         ✓ Tipos de análisis    ✓ Análisis 
             estadśiticos           bivariado           ✓ Objetivos
         ✓ Gráficos             ✓                       ✓ Ventajas y 
             estadísticos           Pasos a seguir         desventajas
                                ✓ Tipos                 ✓ Integración con R y 
                                                           Power BI
Objetivos de la clase
         Identificar las particularidades del Análisis 
         bivariado de datos
         Reconocer ejemplos y aplicaciones de Python
MAPA DE CONCEPTOS    Análisis            Tipos
                    univariado y 
                     gráficos           Gráficos 
                                       estadísticos
                                      Pasos a seguir
Análisis             Análisis 
estadístico           bivariado          Tres tipos
                                       Objetivos
                     Análisis          Ventajas y 
                    Multivariado       desventajas
                                     Integración con R y 
                                        PowerBI
PARA RECORDAR
Análisis de variables
Como hemos visto en clases anteriores cuando 
analizamos una variable, empleamos la estadística 
descriptiva para calcular medidas de tendencia central 
como un promedio, mediana o la moda. De igual forma, 
cuando estimamos dispersión o variabilidad por 
medio de varianza o desviación estándar hacemos uso 
del análisis univariado.
Cuestionario de tarea
¿Te gustaría comprobar tus 
conocimientos de la clase anterior?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
Análisis Bivariado
Definición
¿Qué es el Análisis 
Bivariado?
Es una de las formas más simples de análisis estadístico, 
que se utiliza para averiguar si existe una relación entre 
dos  conjuntos  de  valores.  Por  lo  general  involucra  las 
variables X e Y.
El  análisis  bivariado no es lo mismo que el análisis de 
datos de dos muestras. Con el análisis de datos de dos 
muestras  (como  una  prueba  Z),  X  e  Y  no  están 
directamente relacionados.
¿Qué es el Análisis 
Bivariado?
Ahora bien, cuando empleamos dos o más variables en 
nuestros  estudios  o  investigaciones,  buscando  conocer 
causalidad, efectos o correlaciones podemos hablar de 
análisis bivariados o multivariados. 
A lo largo de esta clase, nos centramos exclusivamente 
en el análisis bivariado.
Ejemplos
Ejemplos
Un  análisis  bivariado  típico  puede  ser  por  ejemplo, 
determinar la relación entre la ingesta de carbohidratos 
en el peso de las adolescentes. Decimos que se trata de 
un  análisis  bivariado  justamente  porque  estamos 
analizamos dos dimensiones:
✔ Una variable dependiente que viene a ser el peso 
de la población analizada.
✔ La variable independiente que será la cantidad o 
ingesta de carbohidratos en su dieta. 
Ejemplos
Por lo tanto en términos generales el análisis bivariado es la 
investigación  de  la  relación  entre  dos  conjuntos  de 
datos,  como  pares  de  observaciones  tomadas  de  una 
misma muestra o individuo.
Las  correlaciones  bivariadas  son  herramientas  de  amplia 
aplicación  y  se  utilizan  para  estudiar  cómo  una  variable 
influye en la otra.
Pasos a seguir
Pasos a seguir
       1                                2                             3                           4
                                                                                                 4
  Observar la                  Identificar valores              Significancia               Determinar si 
naturaleza de la                   nominales,                    estadística                    existe 
posible relación                 ordinales o de                                             relación o no 
de las variables                      ratios                                                  entre las 
                                                                                              variables
1 Pasos a seguir
El primer paso es observar la naturaleza de la posible 
relación de las variables es decir, cómo estas podrían 
vincularse:  podríamos tener correlación directa, de 
manera  que  cuando  una  variable  aumenta  la  otra 
también  lo  hará  o  correlación  inversa,  cuando  un 
cambio en una variable mueve a la otra en sentido 
contrario.
2 Pasos a seguir
Un segundo paso tiene que ver con identificar los 
niveles de medición de los datos y con ello 
definimos  si  se  tratan  de  valores  nominales, 
ordinales o de ratios.
3 Pasos a seguir
El análisis bivariado y multivariado va de la mano 
con el rigor de la “significancia estadística” y 
con ello tomar por sentado que los resultados que 
obtenemos  serán  los  mismos  que  con  otra 
muestra o estudio similar y no se deben a la mera 
casualidad. 
Para  la  mayoría  de  los  casos,  una  correlación 
bivariada asume una significancia de 0.05. Esto 
quiere decir que de 100 estudios, 95 concluirán 
en los mismos resultados y solo 5 se atribuyen al 
azar ������.
4 Pasos a seguir
Si  conocemos  las  variables  y  sus  niveles  de 
medición,  sólo  nos  resta  determinar  si  existe 
relación  o  no  entre  las  variables.  Para  conocer  si 
existe o no una correlación significativa, la medida 
más  comúnmente  usada  es  el  coeficiente  de 
correlación  de  Pearson,  tema  que  ya  hemos 
estudiado a lo largo del curso. 
   4 Pasos a seguir
 Existen diferentes tipos de correlación (todo depende de la naturaleza en la escala de 
 medida de las variables). Dependiendo del tipo de variable podemos elegir cualquiera de 
 los coeficientes de correlación que se muestran a continuación:
         Variable 1    Variable 2    Tipo de correlación            Mayor información
         Nominal       Nominal       Coeficiente Phi                Coeficiente de Phi
         Nominal       Ordinal       Coeficiente Rango biserial     Coeficiente Biserial
         Nominal       Intervalo     Coeficiente biserial puntual   Coeficiente biserial puntual
         Ordinal       Ordinal       Coeficiente Spearman           Coeficiente de Spearman
         Intervalo     Intervalo     Coeficiente Pearson            Coeficiente de Pearson
4 Pasos a seguir
                          La correlación puede ser 
                          positiva, negativa o 0. El rango 
                          está dado por -1 < rho < 1.
                          1 >> Indica perfecta asociación 
                          lineal
                          0 >> indica ausencia de 
                          correlación
                          -1 >> indica perfecta asociación 
                          inversa
4 Pasos a seguir
                        Una forma rápida de poder 
                        analizar correlación entre 
                        variables es mediante el uso de 
                        la matriz de correlación que 
                        algunas veces se denomina 
                        Heatmap.
Tipos de Análisis 
Bivariados
Tipos
Tipos de Análisis Bivariado
 Variable        Variable    Variable Numérica 
Numérica &     Categórica &       & 
 Variable        Variable    Variable Categórica.
 Numérica.      Categórica.
          Tipos de Análisis Bivariado
 Numérico vs Numérico
 En este tipo de variable tanto las variables de los 
 datos    bivariados   que    incluyen   la   variable 
 dependiente  como  la  independiente  tienen  valor 
 numérico (reales).
 Tipos de Análisis Bivariado
Categórica vs Categórica
Se puede utilizar el coeficiente de correlación de Phi 
y además se pueden generar tablas de contingencia 
para comparar dependencia o independencia de las 
variables 
 Tipos de Análisis Bivariado
Numérica vs Categórica
Se  puede  utilizar  el  coeficiente  de  correlación  de 
Rank-biserial  o  Point-biserial.  Además  se  pueden 
crear  agrupaciones  para  la  variable  numérica  de 
acuerdo a las clases de la variable categórica. 
Tipos de gráficos
               Bar Charts        Scatterplots.
FacetGrid.      (Gráficos de 
                barras)
Herramientas para 
análisis bivariado
Scatterplots
 Por    medio     de   esta    visualización    podemos 
 comprender  si  existe  algún  tipo  de  relación  lineal 
 entre dos variables numéricas. A primera vista es de 
 gran  ayuda  para  entender  la  influencia  de  una 
 variable en otra.
Regresión
 Permite determinar cómo se pueden relacionar los 
 datos.  Busca  crear  una  ecuación  de  mejor  ajuste 
 para los datos con ciertas suposiciones. La regresión 
 puede ser lineal  o  múltiple  y  siempre  se  tiene  un 
 variable   dependiente  (Y)  y  una  o  varias   
 independientes (X)
Coeficientes de correlación
Este  coeficiente  dice  si  las  variables  están 
relacionadas entre sí.  Cero significa que no están 
correlacionadas de manera lineal, mientras que un 1 
(ya  sea  positivo  o  negativo)  significa  que  las 
variables  están  perfectamente  correlacionadas  de 
forma perfecta o inversa.
         ☕
       Break
     ¡10 minutos y 
     volvemos!
          ¡Lanzamos la
          Bolsa de 
          Empleos!
         Un espacio para seguir potenciando tu carrera y 
         que tengas más oportunidades de inserción 
         laboral.
         Podrás encontrar la Bolsa de Empleos en el menú 
         izquierdo de la plataforma.
         Te invitamos a conocerla y ¡postularte a tu futuro 
         trabajo!
           Conócela
Ejemplo en vivo
Utilizaremos el notebook y los datos dentro de la 
carpeta de clase. 
Repararemos las diferentes formas de  realizar un 
análisis bivariado con varios ejemplos.
Además analizaremos la librería pandas_profiling, 
una gran alternativa para generar resúmenes rápidos. 
 Análisis bivariado
Aplicaremos los conocimientos aprendidos hasta el 
   momento de Análisis bivariado
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Análisis bivariado
Se les propone realizar el análisis bivariado del dataset 
de Properati (Datos Properati) 
✔ Elegir algunas variables de cada dataset y luego 
seleccionar gráficos y medidas de resumen 
apropiadas según el caso.
✔ Generar interpretaciones de los resultados 
obtenidos
                      9
   Desafío entregable: 
         Obtención de 
  Deberás  entregar  el  noveno  avance  de  tu  proyecto  final.  Continuaremos 
             Insights
  hablando sobre lo trabajado en el desafío “Data Storytelling”. Crearás un 
  notebook donde se pueda observar las fases de análisis univariado y bivariado 
  que  junto  con  el  trabajo  previo  realizado  permitan  obtener  insights  que 
  ayuden a dar respuesta a la(s) pregunta(s) problema del proyecto final.
Recordemos…
                  Generamos visualizaciones
                  Respondimos preguntas de 
                  interés
                  Soluciones a la pregunta 
                  problema
Clase 33
Desafío entregable:
Data Storytelling
                  Mejora de Insights obtenidos
    DESAFÍO 
    ENTREGABLE
Obtención de insights
Consigna
  ✓ Generar insights que permitan dar      Formato
     respuesta a las preguntas por           ✓ Se espera un notebook en 
     responder                                  formato .ipynb. Dicho notebook debe 
                                                tener el siguiente nombre: 
                                                “Data_StoryTelling+Apellido.ipynb”
Aspectos a incluir                              .
  ✓ Notebook con código y estructura         ✓ Presentación en formato pptx o pdf
     eficiente                             Sugerencias
  ✓ Presentación ejecutiva                   ✓ Se recomienda que la historia cuente 
Ejemplo                                         con una estructura similar a la 
  ✓                                             presentación de referencia
     Ejemplo de presentación
                                           Explicación del desafío
                                             ✓ ¡Click aquí!
CLASE N°36
Glosario
Análisis bivariado: se utiliza para            Diagrama de dispersión: consiste en la 
averiguar si existe una relación entre dos     representación gráfica de dos variables 
conjuntos de valores.                          para un conjunto de datos. 
Correlación: La correlación estadística        Regresión: En estadística, el análisis de la 
determina la relación o dependencia que        regresión es un proceso estadístico para 
existe entre las dos variables que             estimar las relaciones entre variables. 
intervienen en una distribución                Usualmente se tiene una variable 
bidimensional.                                 dependiente (respuesta) y una o varias 
                                              variables independientes
Significancia estadística: se define 
como la probabilidad de tomar la decisión 
de rechazar la verdadera hipótesis nula 
cuando ésta es verdadera
¿Preguntas?
Opina y valora 
esta clase
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Análisis bivariado: Pasos a seguir
      ✓ Tipos
Esta clase va a ser
grabad
  a
      Clase 15. DATA SCIENCE
 Modelos Analíticos 
      para DS III
Temario
               14                      15                     16
            Modelos                Modelos               Estudios de 
        analíticos para         analíticos para           casos de  
              DS II                  DS III                modelos 
                                                         analíticos I
          ✓ Recapitulación                               ✓ Casos de 
                                ✓ Modelo analítico          éxitos con 
          ✓ Aprendizaje                                     ciencias de 
             supervisado        ✓ Reglas de 
                                    asociación              datos
          ✓ Clasificación
                                ✓ Reducción de           ✓ Armado de 
          ✓ KNN                                             presentación 
                                    dimensionalidad         ejecutiva
          ✓ Regresión
Objetivos de la clase
        Profundizar en el Tipo de Aprendizaje No 
        Supervisado.
        Entender los algoritmos de Clustering y 
        Reglas de Asociación.
        Conocer la Reducción de la Dimensionalidad y 
        PCA.
MAPA DE CONCEPTOS
                            Característica
                               s
Modelos      Aprendizaje 
Analíticos para   No            Diferencias       Clustering
Ciencia de    Supervisado
Datos III                                       Reglas de 
                                              Asociación
                            Algoritmos      Reducción de la 
                                             Dimensionalida
                                                 d
                                                PCA
Repaso clase anterior
 En la clase de Modelos Analíticos para DS II,         En esta sesión, nos centraremos 
 hemos estudiado el Tipo de Aprendizaje                exclusivamente en el Aprendizaje de Tipo 
 Supervisado tanto para problemas de                   No Supervisado, características, 
 Clasificación como así también de                     aplicaciones y particularidades. 
 Regresión.                                            ¡Empecemos!������
Modelo analítico
Aprendizaje no supervisado
✓ Es una subcategoría del aprendizaje 
automático y la IA. 
✓ Se define por el uso de conjuntos de datos 
no etiquetados para entrenar algoritmos 
que encuentren patrones ocultos. 
✓ El aprendizaje no supervisado ayuda a las 
organizaciones a resolver una variedad de 
problemas del mundo real a gran escala, 
por ejemplo en sistemas de recomendación 
como Amazon con base en Clustering.
 Diferencias
 ✓ Sus  métodos  no  se  pueden  aplicar       ✓ Permiten      realizar   tareas    de 
    directamente  a  un  problema  de              procesamiento  más  complejas. 
    regresión  o  clasificación,  porque  no       Puede  ser  más  impredecible  en 
    tenemos idea de cuáles deberían ser            comparación  con  otros  métodos  de 
    los valores de los datos de salida. ������         aprendizaje naturales.
    Recordemos  que  la  variable  es          ✓ Se utilizan para agrupar los datos 
    desconocida.                                   no   estructurados     según  sus 
 ✓ Puede  utilizarse  para  descubrir  la          similitudes y patrones distintos en 
    estructura  subyacente  de  los                el conjunto de datos. 
    datos.
        En 
resumen… ������ 
✓ No hay una variable objetivo (variable de salida). 
✓ No hay variables que ayudan a predecir a la variable 
de salida. 
✓ Todas las variables tienen la misma importancia. 
✓ Se busca la interdependencia de las variables.
¿Cómo funciona?
¿Cómo funciona? 
✓ Funcionan con datos no etiquetados. 
  Su   propósito   es   naturalmente    la 
  exploración. 
✓ Si  el  Aprendizaje  Supervisado  funciona 
  bajo  reglas  claramente  definidas,  el 
  Aprendizaje       no      Supervisado 
  funciona bajo condiciones en las que 
  los  resultados  son  desconocidos  y 
  por lo tanto, es necesario definirlos en el 
  proceso.
Están acostumbrados a
 ✓ Explorar la estructura de la información y     En   otras   palabras,  describe    la 
     detectar patrones distintos.                 información  o  nuestro  dataset 
                                                  identificando  las  relaciones  entre 
 ✓ Extraer ideas valiosas.                        los features
 ✓ Aumentar la eficacia del proceso de toma 
     de  decisiones  en  base  a  los  patrones 
     detectados.
                               APRENDIZAJE                        APRENDIZAJE NO 
                              SUPERVISADO                           SUPERVISADO
Entrenamiento             Utiliza datos etiquetados                  Utiliza datos no etiquetados
Retroalimentaci           Retroalimentación directa               No tiene retroalimentación directa
   ón
 Datos           Se proporcionan de entrada y salida           Se proporcionan sólo de entrada
Objetivo             Predecir con nuevos datos             Encontrar patrones ocultos de los datos
Supervisión             Necesita ser supervisado                     No necesita supervisión
Clasificación     Problemas de Clasificación y Regresión      Problemas de Clustering y Asociaciones
Usabilidad Cuando conocemos la entrada y las salidas        Cuando solo tenemos datos de entrada
Resultado                  Resultado preciso                             Menos preciso
Relación con         No está cerca de la verdadera IA          Está más cerca de la verdadera IA, ya 
   AI                                                      que aprende de manera similar a como 
                                                                           un niño
Algoritmos
Algoritmos
Los problemas de aprendizaje no 
supervisado se clasifican principalmente 
en dos categorías: 
✓ Cluster (donde tenemos algoritmos 
como k-means. clustering jerárquico, 
modelos de mixturas gaussianas, o 
algoritmo basados en densidad como 
DBSCAN) 
✓ Reducción de dimensionalidad (como 
PCA, ICA, Análisis Factorial)
 ¿Qué tipo de problemas 
 resuelve?
Problemas de Clustering:                      Problemas de Reducción 
Asignación de individuos/objetos a            Dimensionalidad: 
grupos homogéneos asegurando mínima           Cuyo propósito es reducir el número de 
varianza intra-cluster y máxima varianza      features (variables) por medio de 
inter-cluster intentando descubrir la         feature selection (selección existente) o 
estructura oculta de los objetos              feature extraction (combinación de 
                                              datos originales).
Clustering vs. Reducción 
de dimensionalidad
Usamos clustering cuando 
queremos agrupar observaciones 
con base en ciertos features 
(reducción de número de 
observaciones a K grupos), 
mientras que reducción de 
dimensionalidad se usa cuando 
se tienen muchas dimensiones y 
queremos comprender las 
relaciones existentes en menos 
dimensiones.
Múltiples algoritmos del 
Aprendizaje No 
Supervisado.
Algunos de los más populares son:
✓ Clustering o Agrupamiento
✓ Reglas de Asociación
✓ Algoritmos de Reducción de la 
Dimensionalidad
Clustering
También conocidas como agrupamiento o segmentación tienen como principal función 
encontrar una estructura o un patrón en una colección de datos no clasificados. 
������
Es decir, Intentan encontrar grupos en los datos que compartan atributos en 
común ������ 
Técnicas para 
codificación de 
categorías
One Hot Encoding
Toman variables numéricas para medir la 
distancia. Sin embargo, existe la manera 
de trabajar con variables categóricas 
haciéndolas variables dummy, a través 
del uso de la técnica de transformación de 
datos One Hot Encoding (OHE).
Label Encoder
Sin embargo One Hot Encoding es la única 
técnica para transformar variables 
categóricas, existe una alternativa para 
reducir el problema de 
multidimensionalidad cuando tenemos 
muchas categorías en una variable a 
través del uso de la técnica de 
transformación de datos Label Encoder 
(LE).
One Hot Encoding vs Label 
1) La variable categórica no es ordinal               1) La variable categórica es ordinal 
Encoder
   (como los países anteriores)                         (como Jr. kg, Sr. kg)
2) La  cantidad  de  categorías  es                   2) El número de categorías es bastante 
   pequeña  para  evitar  problemas  de                 grande ya que la codificación one-hot 
   multicolinealidad y overfit.                         puede llevar a un alto consumo de 
                                                        memoria.
   Manzan     Poll  Bróco   Caloría                      Nombre       Categorí   Caloría
   a          o     li      s                            de la        a          s
                                                         comida
   1          0     0       95
                                                         Manzana      1          95
   0          1     0       231
                                                         Pollo        2          231
   0          0     1       50
                                                         Brócoli      3          50
Tipos de algoritmos 
Clustering
Clustering
Particiones      Hierarchical           Density             Grid              Model
K-means          Aglomerativo/        DBSCAN            Wavecluster     GMM (Gaussian 
                  Divisivo                                              Mixture Model)
PAM (Partition   BIRCH (Balanced        OPTICS              STING 
around medoids)    Iterative Reduced)                                          COBWEB
CLARA           ROCK (Robust         DBCLASD            CLIQUE            CLASSIT
(Clustering Large     Clustering 
Applications)       Algorithm)
FCM (Fuzzy C-    CURE (Clustering       DENCLUE            OptiGrid           SOMs
Means)              using 
               representatives)
Fuente: Adaptado de Mehta V et al. (2020).
      Algoritmos
              Algoritmos                    Tiempo                 Accuracy              Manejo 
                                                                                        outliers
          Affinity Propagation               Medio                   Medio                  si
       Agglomerative Clustering               Alto                    Alto                  Si
                 BIRCH                       Medio                    Alto                  Si
                DBSCAN                        Alto                    Bajo                  No
                K-means                      Medio                    Bajo                  No
           Mini-Batch K-Means                 Alto                   Medio                  No
               Mean Shift                    Medio                    Bajo                  Si
                 OPTICS                       Alto                   Medio                  No
           Spectral Clustering               Medio                   Medio                  No
Fuente: Adaptado de Rujasari P et al. (2010). En negrita, los algoritmos más populares 
Clustering 
Jerárquico
Clustering Jerárquico
                En estos algoritmos se generan sucesiones 
                ordenadas (jerarquías) de 
                conglomerados. Puede ser agrupando 
                clústers pequeños en uno más grande o 
                dividiendo grandes clusters en otros más 
                pequeños. 
                La estructura jerárquica es representada en 
                forma de un árbol llamado Dendograma.  
������ Podemos encontrar 2 clasificaciones adicionales:
1. Jerárquicos 
aglomerativos (bottom-
Inicialmente cada instancia es un clúster. 
Las  estrategias  aglomerativas  parten  de 
up)
un  conjunto  de  elementos  individuales  y 
van  “juntando”  los  elementos  que 
más se parezcan hasta quedarse con 
un  número  de  clusters  que  se 
considere óptimo.
2.    Jerárquicos 
divisivos (top-down)
Inicialmente todas las instancias están en un 
solo clúster y luego se van dividiendo, tal cual 
su nombre lo indica. Las estrategias divisivas, 
parten del conjunto de elementos completos y 
se van separando en grupos diferentes 
entre sí, hasta quedarse con un número 
de clusters que se considere óptimo.
Criterios importantes
Debemos tomar en cuenta dos factores 
importantes para la formación de grupos:
1. Medida de distancia 
2. Criterio de enlace (usualmente criterio 
de Ward)
3. Existen otros criterios de enlace (Simple, 
Completo,Promedio y Centroide)
                        from sklearn.cluster import AgglomerativeClustering
                        import scipy.cluster.hierarchy as sch
                        dataset = pd.read_csv('Mall_Customers.csv')
                        X = dataset.iloc[:, [3, 4]].values
Ejemplo                    plt.figure(figsize=(10,6))
                        dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
                        model = AgglomerativeClustering(n_clusters=5, affinity='euclidean', 
                        linkage='ward')
                        model.fit(X)
                        labels = model.labels_
                        plt.scatter(X[labels==0, 0], X[labels==0, 1], s=50, marker='o', 
                        color='red')
                        plt.scatter(X[labels==1, 0], X[labels==1, 1], s=50, marker='o', 
                        color='blue')
                        plt.scatter(X[labels==2, 0], X[labels==2, 1], s=50, marker='o', 
                        color='green')
                        plt.scatter(X[labels==3, 0], X[labels==3, 1], s=50, marker='o', 
                        color='purple')
                        plt.scatter(X[labels==4, 0], X[labels==4, 1], s=50, marker='o', 
                        color='orange')
                        plt.show()
Clustering No 
Jerárquico
   Cluster No Jerárquico
    Volviendo  a  este  tipo  de  algoritmo,  la 
    cantidad  de  clústeres  óptima  se  define  de 
    antemano, y los registros  se  asignan  a  los 
    clústeres   según     su   cercanía.    Existen 
    múltiples  algoritmos  de  Tipo  No  Jerárquico, 
    como  ser  por  ejemplo:  K  –  Means  o 
    DBSCAN. 
K-Means
                         ¿Qué hace este método?
                         Se necesita dar los centroides 
                         iniciales para que el método 
                         comience las iteraciones
Tipos de distancia
Hay diferentes tipos de distancias con las que 
se puede medir la similitud/diferencia entre los 
diferentes registros. 
Cada distancia puede representar un tipo de 
problema diferente y también puede cambiar 
sustancialmente el resultado de mi 
clustering.
Medidas de similitud y 
disimilitud
Medidas de similitud y 
disimilitud
                        import numpy as np;import pandas as pd
                        from matplotlib import pyplot as plt
Ejemplo                     from sklearn.datasets import make_blobs
                        from sklearn.cluster import KMeans
                        X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, 
                        random_state=0);wcss = []
                        for i in range(1, 11):
                           kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, 
                        n_init=10, random_state=0)
                           kmeans.fit(X)
                           wcss.append(kmeans.inertia_)
                        plt.plot(range(1, 11), wcss)plt.title('Metodo del 
                        codo');plt.xlabel('Numero de clusters')
                        plt.ylabel('Inercia');plt.show()
                        kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, 
                        n_init=10, random_state=0)
                        pred_y = kmeans.fit_predict(X)
                        plt.scatter(X[:,0], X[:,1])
                        plt.scatter(kmeans.cluster_centers_[:, 0], 
                        kmeans.cluster_centers_[:, 1], s=300, c='red')
                        plt.show()
Clustering basados 
en Densidad
Clustering basados en 
Son métodos de aprendizaje no supervisado que identifican grupos/clústeres, basados 
en la idea de que un clúster es un espacio de datos es una región contigua de alta 
Densidad
densidad de puntos, separada de otros clústeres similares por regiones contiguas de 
baja densidad de puntos
Clustering basados en 
Se tienen en cuenta dos parámetros importantes:
✓ eps:  Define  la  vecindad  alrededor  de  un 

Densidad
punto,  si  la  distancia  entre  dos  puntos  es 
menor o igual a "eps", entonces se consideran 
vecinos. Si el valor de eps se elige demasiado 
pequeño,  gran  parte  de  los  datos  se 
considerarán valores atípicos.
✓ MinPts: Número mínimo de vecinos (puntos) 
dentro  del  radio  eps.  MinPts  se  pueden 
calcular del número de dimensiones D como 
MinPts >= D + 1. El valor mínimo de MinPts 
debe ser de al menos 3.
                       from sklearn.datasets import make_blobs
                       from sklearn.cluster import DBSCAN
                       # Configuracion de datos y parametros
                       num_samples_total = 1000;cluster_centers = [(3,3), (7,7)]
Ejemplo                   num_classes = len(cluster_centers);epsilon = 1.0;min_samples = 13
                       # Generacion de datos
                       X, y = make_blobs(n_samples = num_samples_total, centers = 
                       cluster_centers, n_features = num_classes, center_box=(0, 1), 
                       cluster_std = 0.5)
                       # DBSCAN
                       db = DBSCAN(eps=epsilon, min_samples=min_samples).fit(X)
                       labels = db.labels_;no_clusters = len(np.unique(labels) )
                       no_noise = np.sum(np.array(labels) == -1, axis=0) # Ruido (Outliers)
                       print('#. clusters estimado: %d' % no_clusters)
                       print('# puntos ruidosos: %d' % no_noise)
                       # Generar figura de datos
                       colors = list(map(lambda x: '#3b4cc0' if x == 1 else '#b40426', 
                       labels))
                       plt.scatter(X[:,0], X[:,1], c=colors, marker="o", picker=True)
                       plt.title('Clasificacion DBSCAN');
                       plt.xlabel('Eje X[0]');plt.ylabel('Eje X[1]');plt.show()
Actividad colaborativa
Clustering aplicado a acciones
Utilizaremos información de precios de acciones 
para identificar cuáles son las más parecidas
Discutiremos en el final de la actividad los 
resultados obtenidos
Duración: 20 minutos
Grupos de 3-4 personas
     ACTIVIDAD COLABORATIVA
Acuerdos
Presencia                                       Apertura al aprendizaje
✓ Participar y “estar” en la clase, que          ✓ Siempre, pero siempre puedes 
    tu alrededor no te distraiga                    seguir aprendiendo. Compartir el 
                                                    conocimiento es válido, la 
Escucha activa                                       construcción colaborativa es la 
                                                    propuesta.
✓ Escuchar más allá de lo que la 
    persona está expresando 
    directamente                               Todas las voces
                                                 ✓ Escuchar a todos, todos podemos 
                                                    reflexionar. Dejar el espacio para 
                                                    que todos podamos participar.
    ACTIVIDAD COLABORATIVA
Clustering aplicado a acciones
Consigna: 
1. Importar datos de Acciones Globales       3.    Aplicar la alternativa elegida y 
    (que están hosteados en GITHUB en        establecer conclusiones
    el siguiente enlace Monitoreo de 
    Acciones
2. Identificar qué algoritmo de 
    clustering aplicaría en este caso 
    para identificar qué acciones serían 
    similares: No Jerárquico (K-means), 
    Jerárquico (Aglomerativo), Densidad 
    (DBSCAN)
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Reglas de asociación
¿De qué se trata?
✓ Se refiere a la identificación de los 
 objetos que se encuentran juntos 
 en un evento o registro dado. Ej. 
 Una transacción. 
✓ También se conoce como análisis de 
 canasta de mercado, por su aplicación 
 en  el  análisis  de  patrones  en  las 
 compras de los supermercados. 
Para recordar
Esos patrones pueden ayudar a tomar decisiones tales como qué 
cupones distribuir, cuándo poner un producto a la venta, o cómo 
colocar los artículos en las góndolas, etc. 
Las interpretaciones de las reglas se expresan como “Si el artículo A 
es parte de una transacción, entonces el artículo B es también parte 
de la transacción X”.
Las reglas no deben ser interpretadas como una relación causal, sino 
como una asociación entre dos o más elementos.
Veamos algunos 
✓ Si un hombre compra zapatos, el 
ejemplos...
10% de las veces también compra 
medias.
✓ Clientes que adquieren un producto 
lácteo  tienden  a  comprar  un 
producto de panificados.
✓ El 75% de los clientes que compra 
fideos  y  algún  tipo  de  salsa, 
también compra queso rallado.
 ¿Cómo funcionan?
 ✓ Una  regla  de  asociación  tiene  un 
    antecedente  (lado  izquierdo)  y  un 
                                         Ejemplo: {fideos, salsa} → {queso rallado} 
    consecuente  (lado  derecho).  Ambos 
    lados  de  la  regla  son  un  conjunto  de 
    elementos.                                   Antecedente          Consecuente
 ✓ Si  el  conjunto  de  elementos  X  es  el 
    antecedente y conjunto de elementos Y     Itemset: fideos, salsa, queso rallado
    es el consecuente, entonces la regla de 
    asociación se escribe como:
                     X → Y
                 Ventajas vs. Desventajas
✔ El concepto del algoritmo de Reglas de           ������ Se generan muchas reglas con un 
Asociación es realmente muy sencillo.              pequeño número de elementos.
✔Su implementación no requiere gran                ������  Las reglas pueden ser cíclicas, es decir, 
complejidad y suele funcionar bien, es 
decir, presenta una buena performance.             (A, B) → C, (A, C) → B y (B, C) → A. 
                                                 ������ Se necesita filtrar las reglas si 
                                                 determinados artículos son buscados como 
                                                 consecuente.
Casos de estudio
 Caso I: E-commerce
✓ Cuando      las   personas    compran       ✓ En  general  los  clientes  tienen  a 
   productos  a  través  de  nuestro  sitio       comprar productos similares a los que 
   tenemos  la  información  de  cada             adquieren, a esto se le llama sistemas 
   transacción. ������                                de recomendación. ������
✓ Podemos      usar   esta   información      ✓ Estos sistema generan mayor eficiencia 
   eficientemente    para    incrementar          en las ventas. ������ 
   nuestras ventas. ������
                                              ✓ Esta  metodología  se  conoce  como 
✓ Podríamos investigar que productos son          ‘Association  rule  Mining’  o  reglas  de 
   los más vendidos. ������                           asociación. ������
✓ Se  pueden  lanzar  anuncios        de 
   recomendación  cuando  un  usuario         ✓ En Marketing se conoce como Market 
   compre algún producto. ������                      Basket Analysis. ������
  Caso II: Tienda de libros
   ✓ Si  quisiéramos  saber  qué  tipos  de  géneros 
      deben colocarse uno al lado de los de negocios 
      para    lograr   vender    más?     (Economía, 
      Gastronomía, Viajes, etc)
   ✓ Si tenemos información histórica de compras 
      podemos ver en las transacciones que libros 
      compran  junto  a  los  de  negocios  y  así 
      establecer la estrategia cross-selling
   ✓ Antes de cualquier conclusión se debe analizar 
      cuidadosamente      antes   de    implementar 
      medidas
Reglas de asociación 
vs Minería para 
reglas de asociación
Comparación
● Una  regla  de  asociación  simplemente  es  una 
regla  que  describe  qué  productos  de  nuestra 
tienda serán comprados al tiempo
● En  cambio,  la  minería  para  reglas  de 
asociación  es  una  metodología  que  trata  de 
descubrir las reglas en los datos.
Reducción de la 
dimensionalidad
Reducción de la 
Buscamos reducir la cantidad de features de un dataset, pero reteniendo la mayor 
cantidad de “información” posible. 
dimensionalidad
Tenemos dos aplicaciones principales con esta técnica
              1- Eliminar variables                  2- Encontrar grupos
Caso 1: Eliminar 
variables
             Original:                          Reducción de 1 dimensión: 
3 variables para predecir “Ingreso”                     2 variables
Caso 2: Transformación 
matemática
                Ejemplo: Proyectar la tierra (esfera en 3D) en 
                un plano (2D).
                Si bien por un lado ganamos una mejor 
                visualización y entendimiento, inevitablemente 
                vamos a perder información (por deformación 
                del mapa)
                         En los métodos de Reducción de la 
                         Dimensionalidad, siempre vamos a 
                            perder información 
                         ¡El objetivo es perder lo menos 
                               posible! 
¿Para qué lo aplicaríamos?
  ✓ Para enfrentar “La Maldición de la        ✓ Compresión de archivos.
     Dimensionalidad” es decir, tenemos 
     tantos features que termina siendo algo  ✓ Detectar features relevantes en 
     negativo para nuestro modelo de ML.         datasets o variables altamente 
                                                 correlacionadas.
  ✓ Reducir el input en un modelo de 
     regresión o clasificación.
  ✓ Visualizar mucho mejor nuestros datos. 
Algoritmos de aplicación
Algunos de los más populares son:
                ✓ PCA: Principal Component Analysis.
                ✓ Auto-Encoders con Redes Neuronales. 
                ✓ MDS: Multidimensional scaling.
                ✓ UMAP, entre otros.
Principal Component 
Analysis
¿Qué hace el PCA? 
 ✓ El método gira los datos de forma que, 
     desde un punto de vista estadístico, no 
     exista   una  correlación  entre  las 
     características   rotadas     pero    que 
     conserven  la  mayor  cantidad 
     posible de la varianza de los datos 
     originales. 
 ✓ Es  decir,       el   PCA  reduce  la 
     dimensionalidad de  un  conjunto  de 
     datos        proyectándose          sobre 
     un subespacio           de         menor 
     dimensionalidad.
¿Qué hace el                                                              Reducción
  Por   ejemplo,  datos  con dos  características 
PCA? 
  (dispuestos en un plano) pueden ser proyectados 
  sobre una única línea. 
  Por  otro  lado  un  conjunto  de  datos  de  tres 
  características  (dispuestos  en  un  espacio  de  tres 
  dimensiones)  pueden  ser  proyectados  en  un  plano 
  (de    dos    dimensiones).     Incluso     los   datos       3 features         2 features
  resultantes en el plano podrían ser reducidos a 
  una única línea es decir pasar de 3 dimensiones 
  a 1. ������
USArrests = sm.datasets.get_rdataset("USArrests", "datasets")
datos = USArrests.data
# Entrenamiento modelo PCA con escalado de los datos
pca_pipe = make_pipeline(StandardScaler(), PCA())                               Ejemplo
pca_pipe.fit(datos)
# Se extrae el modelo entrenado del pipeline
modelo_pca = pca_pipe.named_steps['pca']
import seaborn as sns;sns.set_style("whitegrid")
fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))
ax.bar(x= np.arange(modelo_pca.n_components_) + 1,height = 
modelo_pca.explained_variance_ratio_)
for x, y in zip(np.arange(len(datos.columns)) + 1, 
modelo_pca.explained_variance_ratio_):
label = round(y, 2);ax.annotate(label,(x,y),textcoords="offset 
points",xytext=(0,10),ha='center')
ax.set_xticks(np.arange(modelo_pca.n_components_) + 1);ax.set_ylim(0, 
1.1)
ax.set_title('Porcentaje de varianza explicada por cada componente')
ax.set_xlabel('Componente principal')
ax.set_ylabel('Por. varianza explicada');
Ejemplo
# Proyección de las observaciones de entrenamiento
proyecciones = pca_pipe.transform(X=datos)
proyecciones = pd.DataFrame(proyecciones,columns = ['PC1', 'PC2', 'PC3', 'PC4'],index= datos.index)
proyecciones = np.dot(modelo_pca.components_, scale(datos).T)
proyecciones = pd.DataFrame(proyecciones, index = ['PC1', 'PC2', 'PC3', 'PC4'])
proyecciones = proyecciones.transpose().set_index(datos.index)
plt.figure(figsize=(15,6))
proyecciones['val']=proyecciones.index
ax = proyecciones.set_index('PC1')['PC2'].plot(style='o')
def label_point(x, y, val, ax):
a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)
for i, point in a.iterrows():
ax.text(point['x'], point['y'], str(point['val']))
label_point(proyecciones.PC1, proyecciones.PC2, proyecciones.val, ax)
plt.axvline(x=0,color='black');plt.axhline(y=0,color='black')
plt.title('PC1 vs PC2 estados EU');plt.xlabel('PC1',color='k')
plt.ylabel('PC2',color='black')
Ejemplo
CLASE N°15
Glosario
Aprendizaje No Supervisado:                      One Hot Encoding: técnica de discretización 
subcategoría del aprendizaje automático y la  de variables categóricas, bastante útil cuando 
inteligencia artificial que cuenta con NO        se tienen pocas categoŕias, se le conoce 
datos etiquetados para encontrar patrones        también como crear variables dummy
ocultos en los datos                             Label Encoder: técnica de discretización de 
Problemas de clustering: son aquellos            variables categóricas, bastante útil cuando se 
donde se busca encontrar grupos similares        tienen muchas categorías. 
minimizando la varianza inter cluster y          Tipos de algoritmos de clustering: existen 
maximizando la varianza entre cluster            varias opciones (utilizando particiones, 
Problema de reducción de                         jerarquías, densidad, mapas o modelos) pero las 
dimensionalidad: son aquellos donde se           técnicas más comunes con K-means 
busca encontrar proyecciones de las              (particiones), clustering aglomerativo 
variables originales para entender mejor las     (jerárquico) y DBSCAN (densidad).
asociaciones entre individuos y variables        Reglas de asociación: entendimiento de 
                                                 antecedentes y consecuentes analizados por 
                                                 medio de relaciones causales
¿Aún quieres conocer 
  más?
Te recomendamos el 
siguiente material
 MATERIAL AMPLIADO
Recursos multimedia
Título
✓ Reducción de la dimensionalidad. Aprendé IA  | aprendeia.com 
✓ Reducción de la dimensionalidad | interactivechaos.com 
✓ La maldición de la dimensión en Machine Learning | iartificial.net 
Disponible en nuestro repositorio.
¿Preguntas?
                 Resumen 
           de la clase hoy
          ✓ Aprendizaje No Supervisado
          ✓ Clustering
          ✓ Reglas de Asociación
          ✓ Reducción de la Dimensionalidad
          ✓ PCA
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 29. DATA SCIENCE
 Data Wrangling II
Temario
               28                      29                     30
      Data Wrangling I         Data Wrangling            Workshop: 
                                       II                Limpieza de 
                                                            datos
         ✓ Data Wrangling en    ✓ Etapas de Data         ✓ Repaso Data 
            proyectos DS           Wrangling               Wrangling+ tips 
                                                           buenas prácticas
         ✓ Etapas Data          ✓ Data Transformation
            Wrangling                                    ✓ Recomendaciones 
                                ✓ Opciones de remoción     para Data Wrangling
         ✓ Combinar y fusionar     de duplicados
            objetos con Pandas.                          ✓ ¿Cómo hacer una 
                                ✓ Índices Jerárquicos      revisión de pares y 
            Merge y Concat                                 dar feedback?
                                ✓ Print, GroupBy, Apply
Objetivos de la clase
         Identificar las funciones de manipulación de 
         datos en Pandas
         Hacer un repaso de algunas de las más 
         usadas
MAPA DE CONCEPTOS            Data Wrangling 
                           en proyecto de DS
                            Etapas de Data 
                Parte I      Wrangling
                            Merge y Concat
Data Wrangling                   Data 
                            Transformation
                            Remoción de 
                             duplicados
                Parte II      Índices 
                             jerárquicos
                            Print, GroupBy, 
                              Apply
Repaso: Etapas Data 
Wrangling
PARA RECORDAR
Repaso
¿Qué es el Data Wrangling?
¿Qué tan importante se puede considerar 
este proceso en el desarrollo de un 
proyecto de DS?
  Repaso Data Wrangling
La manipulación de datos es el proceso de 
limpieza y unificación de conjuntos de datos 
complejos y desordenados para facilitar el 
acceso,  análisis y modelado. 
Gran parte del tiempo de un científico de datos 
se ocupa en la manipulación de datos.
Fases iniciales de un 
proyecto DS
   1                2               3                 4                5
Definición de     Contexto         Problema            Data       Exploratory Data 
objetivo       Comercial       Comercial         Acquisition    Analysis (EDA)
Fases finales de un 
proyecto DS
  6               7               8                 9               10
Data        Selección del  Desarrollo del     Validación y    Conclusiones
Wrangling        algoritmo       algoritmo        despliegue
(Munging)        apropiado
                     ~60% del tiempo de un Data Scientist consiste 
                     en limpiar y manipular datos
Etapas del Data Wrangling
Descubrimiento   Limpieza       Validación
1       2       3       4       5      6
  Estructuración Enriquecimiento   Publicación
Data Transformation
Definición
Data Transformation
La transformación de datos es el proceso de cambiar 
el formato, la estructura o los valores de los datos. 
Muchas organizaciones utilizan almacenes de datos 
basados en la nube, que pueden escalar los recursos 
informáticos y de almacenamiento con una latencia 
medida en segundos o minutos. 
La escalabilidad de la plataforma en la nube permite a 
las organizaciones omitir las transformaciones de 
carga previa para luego transformarlos en el momento 
de la consulta, un modelo llamado ELT (extraer, 
cargar, transformar).
Procesos 
involucrados
Procesos involucrados
Los procesos como la integración de datos, la 
migración de datos, el almacenamiento de datos 
y el Data Munging pueden implicar la 
transformación de datos.
La transformación de datos puede ser 
constructiva (agregar, copiar y replicar datos), 
destructiva (eliminar campos y registros), estética 
(estandarizar valores o nombres) o estructural 
(renombrar, mover y combinar columnas en una 
base de datos).
Beneficios y 
Desafíos de Data 
Transformation
Beneficios
1. Los datos se transforman para que estén mejor 
organizados. Los datos transformados pueden 
ser más fáciles de usar tanto para humanos 
como para computadoras.
2. Los datos correctamente formateados y 
validados mejoran la calidad de los datos y 
protegen de problemas, como valores nulos, 
duplicados inesperados, indexación incorrecta y 
formatos incompatibles.
3. La transformación de datos facilita la 
compatibilidad entre aplicaciones, sistemas y 
tipos de datos. Los datos utilizados para 
múltiples propósitos pueden necesitar ser 
transformados de diferentes maneras.
Desafíos
1. La transformación de datos puede ser costosa. El 
costo depende de la infraestructura, el software 
y las herramientas específicas utilizadas para 
procesar los datos. 
2. Los procesos de transformación de datos pueden 
consumir muchos recursos. Realizar 
transformaciones en un almacén de datos local 
después de la carga, o transformar los datos 
antes de introducirlos en las aplicaciones, puede 
crear una carga informática que ralentiza otras 
operaciones.
Desafíos
3. La falta de experiencia y el descuido pueden 
presentar problemas durante la transformación. 
Los analistas de datos sin la experiencia 
adecuada en la materia tienen menos 
probabilidades de notar errores tipográficos o 
datos incorrectos porque están menos 
familiarizados con el rango de valores exactos y 
permisibles.
4. Las empresas pueden realizar 
transformaciones que no se ajustan a sus 
necesidades. 
Cómo aplicar Data 
Transformation
Extracción y Parsing
En el proceso ELT moderno, la adquisición de datos 
comienza con la extracción de información de una 
fuente de datos, seguida de la copia de los datos en 
su destino. Las transformaciones iniciales se centran 
en dar forma al formato y la estructura de los datos 
para garantizar su compatibilidad tanto con el sistema 
de destino como con los datos que ya existen. 
Translación y Mapping
Algunas de las transformaciones de datos más 
básicas involucran el mapeo y la traducción de 
datos. 
La traducción convierte datos de formatos usados 
en un sistema a formatos apropiados para un 
sistema diferente. Incluso después del análisis, los 
datos web pueden llegar en forma de archivos 
JSON o XML jerárquicos, pero deben traducirse en 
datos de filas y columnas para incluirlos en una 
base de datos relacional.
Filtros, agregación y 
Los datos se pueden consolidar filtrando campos, 
summary
columnas y registros innecesarios. Los datos 
omitidos pueden incluir índices numéricos en datos 
destinados a gráficos y tableros o registros de 
regiones comerciales que no son de interés en un 
estudio en particular.
Los datos también se pueden agregar o resumir. 
por ejemplo, transformando una serie temporal de 
transacciones de clientes en recuentos de ventas 
por hora o por día.
Enriquecimiento e 
Los datos de diferentes fuentes se pueden fusionar 
imputación
para crear información enriquecida y desnormalizada. 
Las transacciones de un cliente se pueden acumular en 
un total general y agregarse a una tabla de información 
del cliente para una referencia más rápida o para uso 
de los sistemas de análisis de clientes. 
Los valores faltantes se pueden imputar o los datos 
dañados se pueden reemplazar como resultado de este 
tipo de transformaciones.
Indexamiento y 
ordenamiento
Los datos se pueden transformar para que se ordenen 
lógicamente o para adaptarse a un esquema de 
almacenamiento de datos. 
En los sistemas de gestión de bases de datos 
relacionales, por ejemplo, la creación de índices puede 
mejorar el rendimiento o mejorar la gestión de las 
relaciones entre diferentes tablas.
Anonimización y 
encriptado
Los datos que contengan información de identificación 
personal u otra información que pueda comprometer la 
privacidad o la seguridad deben anonimizarse antes de 
su propagación. 
El cifrado de datos privados es un requisito en muchas 
industrias, y los sistemas pueden realizar el cifrado en 
múltiples niveles, desde celdas de bases de datos 
individuales hasta registros o campos completos.
Opciones de 
remoción de 
duplicados
Valores Duplicados
Valores duplicados
Un valor duplicado es aquel en el que todos los 
valores de al menos una fila son idénticos a todos 
los valores de otra fila. Una comparación de valores 
duplicados depende de lo que aparece en la celda, 
no del valor subyacente almacenado en la celda.
Pueden ser de dos tipos:
a. Valores duplicados: cuando dos 
características tienen el mismo conjunto de 
valores
b. Índice duplicado: cuando el valor de dos 
características es diferente, pero ocurren en el 
mismo índice
¿Cómo lidiar con 
ellos?
¿Cómo lidiar con ellos?
Lo primero que debemos hacer es identificarlos
                        ������
¿Cómo lidiar con ellos?
                  También se pueden borrar por columnas
Podemos borrar todos los duplicados
                 ������
Índices Jerárquicos
Índices Jerárquicos 
La indexación jerárquica es una característica 
importante de pandas que le permite tener múltiples 
(dos o más) niveles de índice en un eje. 
De forma algo abstracta, proporciona una forma de 
que trabaje con datos de dimensiones superiores en 
una forma de dimensiones inferiores.
Índices Jerárquicos 
   Ejemplo
   Serie de pandas       Vista de los dos niveles de         Acceso al primer nivel
                         índices
Índices Jerárquicos 
   Ejemplo
   Serie de pandas          Acceso al segundo nivel          Acceso al primer nivel
Índices Jerárquicos 
    Ejemplo
    Serie de pandas     Haciendo un reshape con unstack
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Ejemplo en vivo
Utilizaremos el notebook llamado Clase 7 - Data 
Wrangling II - Ejemplo en vivo.ipynb y el archivo 
train.csv dentro de la carpeta de clase para 
repasar conceptos asociados de Data Wrangling. 
Aprenderemos a cómo lidiar con nulos, duplicados, 
estandarizar valores, rellenar con la mediana, valor 
por defecto y tomar decisiones respecto a qué 
columnas retener para el análisis.
Print, GroupBy, Apply 
y Pivot
 Print
Print
La función Python print() toma cualquier 
número de parámetros y los imprime en una 
línea de texto. Cada uno de los elementos se 
convierte a formato de texto, separados por 
espacios, y hay un único '\n' al final (el 
carácter de "nueva línea").
Por defecto, print() pone un solo '\n' después 
de todos los elementos. El parámetro 
opcional end= establece una cadena 
personalizada para que aparezca después de 
todos los elementos.
GroupBy
GroupBy
               ✔ La sentencia GROUP BY identifica una columna 
                 seleccionada para utilizarla para agrupar 
                 resultados. 
               ✔ Divide los datos en grupos por los valores de la 
                 columna especificada, y devuelve una fila de 
                 resultados para cada grupo. 
               ✔ Se puede utilizar GROUP BY con más de un 
                 nombre de columna.
GroupBy
Ejemplo de 
agrupamiento por la 
columna key, con la 
agregación suma de la 
columna data.
GroupBy
Dataset        Creamos grouped que un objeto de tipo Groupby
 Ponemos la agregación
GroupBy
Dataset
              Hacemos todo en una línea y usamos dos 
              columnas
               Devuelve una serie con dos índices y la 
               media para cada combinación de índices
Apply
Apply
Permite a los usuarios pasar una función y 
aplicarla en cada valor de la serie Pandas. 
Se trata de una gran mejora para la biblioteca 
de pandas, ya que esta función ayuda a 
segregar los datos de acuerdo con las 
condiciones requeridas, por lo que se utiliza 
de manera eficiente en la ciencia de datos y el 
aprendizaje automático.
Apply
Puede ser una función de una librería específica como en este ejemplo y 
       aplica a todo el dataframe esta operación.
Apply
Puede ser una función definida manualmente y luego aplicarla al dataframe
Pivot
Pivot
PIVOT rota una tabla convirtiendo los valores únicos 
de una columna en múltiples columnas. 
A su vez, ejecuta agregaciones donde se requieren 
en cualquier valor de columna restante que se 
desee en el resultado final.
Pivot
En este caso, vemos como utiliza los valores únicos de la columna B y los 
transforma en múltiples columnas. 
Pivot
Ahora, vemos como utiliza los valores únicos de la columna B y los 
transforma en múltiples columnas 
                         Se pueden usar varios 
                         valores
Pivot
Podemos rotar una tabla convirtiendo los valores únicos de una columna en múltiples 
columnas.
En el caso de abajo, utilizamos los valores únicos de la columna B y los transformamos 
en múltiples columnas                  Index no puede tener valores repetidos. 
                                      Esto da un error porque ‘A’ tiene valores 
                                      repetidos
Otras funciones
.astype
Permite cambiar el tipo de datos de las columnas de las columnas de un 
DataFrame
.value_counts()
Cuenta cuántos valores únicos hay en cada columna. Es muy útil para datos categóricos
     DataFrame con datos categóricos          Contando cuantos datos 
                                              repetidos hay de cada valor 
                                              único 
.sort_index() y sort_values()
      Permite ordenar por índice o por columna seleccionada
     DataFrame original      Ordenando por índice       Ordenando por 
                                                        columna
Melt
Es una función que permite cambiar el formato del dataframe a long
Se usa para crear un formato específico donde se usan columnas identificadoras
Todas las demás columnas se usan como valores.

Pueden usar la función pivot para unmelt the output.
Investigando empresas 
    productivas
Investigaremos los consumidores más importantes del 
último corte de una empresa y la importancia de los 
        duplicados
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Investigando 
empresas productivas
Utilizaremos el archivo ‘datos_empresas.csv’ y 
responderemos las siguientes preguntas:
1. Investiguemos cuáles son los mejores clientes en 
ventas
2. Qué tipos de errores pueden observan en este 
dataset
3. Remover las filas duplicadas con base en la 
columna Nombre
4. Qué problemas observa en este dataset respecto 
a la estructura en una mayor escala
                                7
       Desafío entregable: 
           Data Wrangling
     Deberás entregar el séptimo avance de tu proyecto final. Continuaremos hablando 
     sobre lo trabajado en el desafío “Descarga de datos desde APIs públicas”. 
     Crearás un notebook donde se desarrollará la limpieza de los datos elegidos para tu 
     proyecto final, deberás tener en cuenta técnicas vistas en clase para el tratamiento 
     de valores duplicados, nulos y outliers con su respectiva justificación.  
                  Recordemos…
                                      Exploramos diversas APIs
                                     Extrajimos los datos en un 
                                             dataframe
                                     Realizamos una exploración 
                                               simple
        Clase 27
  Desafío entregable: 
 Descarga de datos desde              Obtención de Insights 
      APIs públicas                       preliminares
    DESAFÍO 
    ENTREGABLE
Data Wrangling
Consigna                                        Formato
 ✓ Iniciar el proceso de limpieza y             ✓ Se espera un notebook en 
    exploración de datos según el                   formato .ipynb. Dicho notebook debe 
    dataset elegido para el proyecto                tener el siguiente nombre: 
    final                                           “Data_Wrangling+Apellido.ipynb”.
Aspectos a incluir                              Sugerencias
 ✓ Notebook con código y estructura             ✓ Utilizar las herramientas vistas en el 
    eficiente                                       curso
                                                ✓ Manejo de duplicados nulos y análisis 
Ejemplo                                              exploratorio
 ✓ Data Wrangling                              Explicación del desafío
                                                ✓ ¡Click aquí!
CLASE N°29
Glosario
Data Transformation: La                       Group By: operación que identifica una 
transformación de datos es el proceso de      columna seleccionada para utilizarla para 
cambiar el formato, la estructura o los       agrupar resultados. 
valores de los datos. 
                                             Apply: Permite a los usuarios pasar una 
Valores duplicados: un valor duplicado        función y aplicarla en cada valor de la serie 
es aquel en el que todos los valores de al    Pandas. 
menos una fila son idénticos a todos los 
valores de otra fila                          Pivot: rota una tabla convirtiendo los 
                                             valores únicos de una columna en múltiples 
Indexación jerárquica: La indexación          columnas.
jerárquica es una característica 
importante de pandas que le permite           Melt: Es una función que permite cambiar 
tener múltiples (dos o más) niveles de        el formato del dataframe a long
índice en un eje. 
    ¡Atención!
La clase que viene realizaremos una revisión de pares. 
Se recomienda tener adelantadas las fases de Data 
Acquisition, Data Wrangling y Exploratory Data Analysis 
para obtener feedback de compañeros y tutores con el 
 fin de mejorar nuestro proyecto.
¿Preguntas?
¿Aún quieres conocer 
  más?
Te recomendamos el 
siguiente material
 MATERIAL AMPLIADO
Recursos multimedia
  Porque Data Cleaning es necesario?
   ✓ Why is “Data cleaning” necessary | Medium | 
     Why DATA CLEANING
  Missingno library
   ✓ Missing data visualization module for Python | Github | 
     Missingno
Disponible en nuestro repositorio.
Opina y valora 
esta clase
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Etapas de Data Wrangling
      ✓ Data Transformation
      ✓ Opciones de remoción de duplicados
      ✓ Índices jerárquicos
      ✓ Print, GroupBy, Apply
Esta clase va a ser
grabad
  a
      Clase 50. DATA SCIENCE
Modelos de Ensamble y 
   Boosting Models
Temario
                49                      50                      51
       Mejora de modelos           Modelos de             Despliegue de 
           de Machine          Ensamble y Boosting        Modelos MLOps
           Learning II                Models
                                                          ✓ Fundamentos de Cloud 
                                 ✓ Métodos de                Computing
         ✓ Repaso                   ensamble              ✓ DevOps vs. DevSecOps
            validación de 
            modelos                                       ✓ Continuous Deployment
                                 ✓ Metodologías de 
         ✓ Hypertuning              ensamble              ✓ Data team
            parameter                                     ✓ MLOps
Objetivos de la clase
         Identificar algoritmos de Ensamble y Boosting
MAPA DE CONCEPTOS
                                         Definición
                                        Características
                                        Metodologías
      Métodos de                         Bagging
      Ensamble
                                         Boosting
                                        Algoritmos
Métodos de Ensamble
Métodos de 
Ensamble
También llamados “métodos combinados”, 
intentan ayudar a mejorar el rendimiento de 
los modelos de Machine Learning al mejorar 
su precisión.
Este es un proceso mediante el cual se construyen 
estratégicamente varios modelos de Machine 
Learning para resolver un problema particular.
Ejemplo aplicado
Métodos de 
Ensamble
Supongamos que queremos invertir en una 
empresa, pero no estamos seguros de su 
posible rendimiento, por lo tanto buscamos 
varios expertos financieros para que nos 
aconsejen si el precio de la acción aumentará 
en más de un 5% anual. 
Métodos de Ensamble
   Empleada        Asesor Financiero       Operadora            Empleado 
                                           Mercado de          Competidor
                                             Valores
    Conoce la        Conoce cómo la     Conoce el precio de      Conoce la 
  funcionalidad        estrategia de     las acciones de la    funcionalidad 
  interna de la       empresas será       empresa en los       interna de las 
    empresa.           desarrollada.       últimos años.          firmas 
                                                               competidoras.
 Ha tenido razón    Ha tenido razón 75  Ha tenido razón 65 
70% de las veces.    % de las veces.      % de las veces.     Ha tenido razón 
                                                             60% de las veces.
Métodos de Ensamble
Empleada de la empresa: Esta persona 
conoce la funcionalidad interna de la 
empresa y tiene información sobre la 
misma, pero carece de una perspectiva 
más amplia sobre cómo están 
innovando los competidores y cómo 
está evolucionando la tecnología. En el 
pasado, ha tenido razón 70% de veces
Métodos de Ensamble
Asesor  financiero  de  la  empresa:  Esta 
persona tiene una perspectiva más amplia 
sobre  cómo  la  estrategia  de  las 
empresas será ajustada en este entorno 
competitivo, sin embargo, carece de una 
visión sobre cómo las políticas internas 
de   la  empresa  impactan  en  su 
funcionalidad. En el paso ha tenido razón 
un 75% de las veces.
Métodos de Ensamble
Operadora del mercado de valores: Esta 
persona  ha  observado  el  precio  de  las 
acciones de la empresa en los últimos años y 
conoce las tendencias de estacionalidad 
y  el  rendimiento  del  mercado  en 
general,  pero  también  conoce  que  las 
acciones  pueden  variar  con  el  tiempo. 
En el pasado ha tenido razón 65% de veces.
Métodos de Ensamble
Empleado de un competidor: Esta persona 
conoce la funcionalidad interna de las 
firmas competidoras y está consciente de 
ciertos cambios que aún no se han 
implementado, pero por otra parte carece de 
conocimiento sobre la empresa enfocada y 
de los factores externos que pueden 
relacionar el crecimiento del competidor. 
En el pasado, ha tenido razón el 60% de las 
veces.
Métodos de Ensamble
Como podemos observar del ejemplo anterior, cada experto tiene su propia 
opinión respecto del tema y nos ofrece una respuesta a la pregunta 
solicitada. La idea justamente es encontrar la decisión más adecuada en 
   base a las diversas miradas de los expertos. 
Para pensar
Ahora bien, ¿Cuál es la relación entre el ejemplo 
propuesto y los modelos de Ensamble? ¿Han 
escuchado de alguno de estos modelos 
anteriormente?
Contesta en el chat de Zoom 
Para pensar
Cuando hablamos de “Model Ensamble” se 
entiende como el proceso de combinar diversos 
modelos para improvisar la estabilidad y poder 
predictivo del modelo, es decir tener en cuenta la 
diferentes opiniones para crear predicciones más 
robustas
Características de 
modelos de 
Ensamble
Características 
1. Permiten reducir el error de generalización 
en las predicciones 
2. Las predicciones de diferentes modelos son 
diversas e independientes lo que hace
3. Busca la sabiduría de las multitudes al hacer 
una predicción. 
4. Aunque el modelo de conjunto tiene 
múltiples modelos base dentro del modelo, 
actúa y funciona como un solo modelo.
Características 
De manera sencilla podría decirse que 
son algoritmos formados por 
algoritmos más simples. Estos 
algoritmos simples, se unen para 
formar un algoritmo más potente, 
siguiendo la premisa: “La unión hace la 
fuerza”.
Características 
No son todos iguales por diversas razones:
1. Puede haber diferencia en la población 
de datos
2. Puede haber una técnica de modelado 
diferente utilizada
3. Puede haber una hipótesis diferente 
En nuestro ejemplo “las diferentes 
opiniones de nuestros expertos 
financieros”
Características 
Aunque hay diversas formas de ensamblar 
o unir algoritmos débiles para formar otros, 
las  más  usadas  y  populares  son  el 
bagging y el boosting.
Cada tipo de algoritmo tiene unas ventajas 
y   desventajas,  pudiendo  ser  usados 
convenientemente  según  sea  nuestra 
problemática a resolver.
Para pensar
Al ajustar un modelo de regresión observan que a medida que 
aumenta la cantidad de datos de entrenamiento, el error de prueba 
disminuye y el error de entrenamiento aumenta. El error de train es 
bastante bajo, mientras que el error de prueba es mucho mayor. 
¿Cuál crees que es la razón principal detrás de este 
comportamiento? 
A. Alta varianza
B. Alto sesgo en el modelo
C. Alta estimación de sesgo
D. Ninguna de las anteriores
Contesta en el chat de Zoom 
Actividad colaborativa
Checkpoint proyectos finales III
Formaremos grupos de 4-5 personas en el Break-out 
Rooms y analizaremos el siguiente proyecto disponible 
Ensemble Models, donde entenderemos como crear un 
modelo de ensamble, cálculo de métricas y como métodos 
de validación cruzada y técnicas de hypertuning de 
parámetros. Posteriormente resolveremos dudas 
conceptuales y técnicas acerca del proyecto final
Duración: 30 minutos
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Metodologías de 
Ensamble
Métodos de Ensamble
Existen diferentes técnicas para poder 
aplicar estos métodos:
1. Bagging
2. Boosting
3. Stacking (No lo veremos en clase)
A continuación explicaremos cada una 
de  estas  técnicas  junto  con  sus 
algoritmos más importantes
Bagging
¿De dónde proviene la idea?
Bagging proviene del concepto de agregación 
de bootstrap. Un forma de reducir la varianza 
de las estimaciones es promediando 
estimaciones de distintos modelos o algoritmos.
Para obtener la agregación de las salidas de 
cada modelo simple e independiente, bagging 
puede usar la votación para los métodos de 
clasificación y el promedio para los 
métodos de regresión.
¿De qué trata?
Los métodos de bagging son métodos 
donde los algoritmos simples son 
usados en paralelo.
El principal objetivo de los métodos en 
paralelo es el de aprovecharse de la 
independencia que hay entre los 
algoritmos simples, ya que el error se 
puede reducir bastante al promediar 
las salidas de los modelos simples. 
¿De qué trata?
Podríamos pensarlo como como si 
quisiéramos resolver un problema entre 
varias personas independientes unas de 
otras, y finalmente damos por bueno lo 
que eligiese la mayoría de las 
personas.
Importante: El principal objetivo intrínseco 
de  los  algoritmos  de  bagging  es  el  de  la 
reducción de la varianza.
¿De qué trata?
                       A la izquierda se ilustra el 
                       funcionamiento del Bagging
Algoritmos
Estos  algoritmos  aplican  tres 
procesos fundamentales que son: 
Bootstrapping, Entrenamiento en 
paralelo y Agregación
Existen diversos algoritmos pero 
el  modelo  más  usado  es  el 
Random Forest
Boosting
¿De dónde proviene la idea?
Boosting es un método de aprendizaje conjunto 
que combina un conjunto de modelos simples 
para minimizar los errores de entrenamiento. Se 
selecciona una muestra aleatoria de datos y se 
ajusta con un modelo y luego se entrena 
secuencialmente. Con cada iteración, las reglas 
débiles de cada clasificador individual se 
combinan para formar una regla de predicción 
fuerte.
¿De qué trata?
En los algoritmos de boosting, los 
modelos simples son utilizados 
secuencialmente, es decir, cada modelo 
simple va delante o detrás de otro 
modelo simple. 
El principal objetivo de los métodos 
secuenciales es el de aprovecharse de 
la dependencia entre los modelos 
simples.
¿De qué trata?
El rendimiento general puede ser mejorado 
haciendo que un modelo simple posterior, le dé 
más importancia a los errores cometidos por 
un modelo simple previo.
Sería como si nosotros al resolver un 
problema, aprovechásemos nuestro 
conocimiento de los errores de otros para 
mejorar en algo intentando no cometerlos 
nosotros.
¿De qué trata?
                    A la izquierda se ilustra el 
                    funcionamiento del Boosting
Para pensar
¿Qué diferencias pueden encontrar 
respecto a la metodología Bagging? ¿Se 
podrán aplicar los mismos algoritmos 
para ambas técnicas?
Contesta en el chat de Zoom 
 Para pensar
La diferencia con el Bagging es que en el 
Boosting los algoritmos no se 
entrenan independientemente, sino 
que se ponderan según los errores de los 
anteriores
Contesta en el chat de Zoom 
Algoritmos
1          2          3          4
ADABOOST   GRADIENT    XG BOOST  LIGHT GBM
       BOOSTING
AdaBoost
Este algoritmo entrena de forma 
secuencial un conjunto de 
aprendices débiles a partir de un 
algoritmo base común. Todos los 
aprendices son entrenados con el mismo 
conjunto de datos pero éstos van 
recibiendo pesos que dependen de los 
errores cometidos por cada aprendiz. 
AdaBoost
Así, inicialmente todos las muestras 
reciben un peso inicial wi de 1/n 
(suponiendo que haya n muestras). El 
primer aprendiz es entrenado y se 
estima su tasa de error. 
Si trabajamos en un problema de 
clasificación, la tasa de error está dada 
por:
AdaBoost
A la derecha se 
ilustra el 
funcionamiento del 
modelo AdaBoost
Gradient Boosting
Es un tipo de algoritmo de aumento. 
Se basa en la intuición de que el 
mejor modelo siguiente posible, 
cuando se combina con modelos 
anteriores, minimiza el error de 
predicción general. 
Miremos a detalle cómo funciona!
Gradient Boosting
La idea clave es establecer 
los resultados objetivo para 
este próximo modelo con el 
fin de minimizar el error. El 
aumento de gradiente se 
puede utilizar tanto para 
clasificación como para 
regresión.
XGBoosting
Extreme Gradient Boosting, es uno de los 
algoritmos de machine learning de tipo 
supervisado más usados en la actualidad. 
Este algoritmo se caracteriza por obtener 
buenos resultados de predicción con 
relativamente poco esfuerzo, en muchos 
casos equiparables o mejores que los 
devueltos por modelos más complejos 
computacionalmente, en particular para 
problemas con datos heterogéneos.
XGBoosting
Se basa en el principio de “boosting”, 
que como bien sabemos, consiste en 
generar múltiples modelos de 
predicción “débiles” 
secuencialmente y que cada uno de 
estos tomé los resultados del 
modelo anterior, para generar un 
modelo más “fuerte”, con mejor poder 
predictivo y mayor estabilidad en sus 
resultados.
XGBoosting
Para conseguir un modelo más fuerte, se 
emplea un algoritmo de optimización, este 
caso Gradient Descent (descenso de 
gradiente). Durante el entrenamiento, los 
parámetros de cada modelo débil son 
ajustados iterativamente tratando de 
encontrar el mínimo de una función 
objetivo, que puede ser la proporción de error 
en la clasificación, el área bajo la curva (AUC), 
la raíz del error cuadrático medio (RMSE), etc.
XGBoosting
Cada modelo es comparado con el 
anterior. Si un nuevo modelo tiene 
mejores resultados, entonces se toma 
este como base para realizar nuevas 
modificaciones. Si, por el contrario, 
tiene peores resultados, se 
regresa al mejor modelo anterior y 
se modifica ese de una manera 
diferente.
XGBoosting
A derecha se ilustra el 
funcionamiento del 
XGBoosting
XGBoosting
Este proceso se repite hasta llegar a un 
punto en el que la diferencia entre 
modelos consecutivos es insignificante, 
lo cual nos indica que hemos 
encontrado el mejor modelo posible, o 
cuando se llega al número de 
iteraciones máximas definido por el 
usuario.
LightGBM
Utiliza la técnica Gradient Boosting. Con 
este método los árboles se construyen de 
manera secuencial y cada uno que se 
agrega aporta su granito de arena para 
refinar la predicción anterior. Es decir, se 
comienza con un valor constante y 
cada árbol nuevo se entrena para 
predecir el error en la suma de todas 
las predicciones de los árboles 
anteriores.
LightGBM
Una vez terminado el proceso, las 
predicciones se calculan sumando 
los resultados de todos los árboles 
que se construyeron. El efecto que 
tiene esto es que cada vez que se 
agrega un árbol nuevo se le presta 
atención a las muestras en las que el 
modelo está funcionando peor y se 
trabaja para mejorar ese aspecto.
LightGBM
A derecha se ilustra el 
funcionamiento del 
LightGBM
Ventajas y desventajas
   Ventajas y desventajas
         Método                                  Ventajas                                                  Desventajas
    Bagging                  -    Fácil implementación con librerías                  -   Pérdida de interpretabilidad debido al 
                                  y buena documentación                                   proceso de promediado 
                             -    Reducción de la varianza                            -   Consume bastantes recursos 
                                  especialmente en datasets de alta                       computacionales a medida que la 
                                  dimensionalidad                                         cantidad de datos incrementa
                                                                                      -   Menos flexible y puede llegar a ser 
                                                                                          menos estable
    Boosting                 -    Fácil implementación con muchos                     -   Sensible al overfitting aunque depende 
                                  hiper parámetros para configurar                        de los parámetros
                                  el performance                                      -   Computación intensiva por ende no es 
                             -    Reducción del sesgo con base el                         tan fácil de escalar, ya que cada 
                                  aprendizaje de los weak learners                        estimador se crea con base en sus 
                             -    Eficiencia computacional y menor                        predecesores
                                  tiempo de ejecución                                 -   Son mucho más lentos que los 
                                                                                          algoritmos de Bagging
Ejemplo en vivo
A continuación analizaremos un ejemplo 
donde pondremos en práctica el proceso de 
Optimización de Hiperparametros con las 
técnicas aprendidas y revisaremos la 
metodología para aplicar Optimización 
Bayesiana.
Aplicación de métodos de 
     ensamble
Utilizaremos lo aprendido en clase para aplicar modelos 
       de Ensamble
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Aplicación de métodos 
de ensemble
Trabajaremos con base en lo desarrollado en clases 
previas con los datos de fuga en el enlace: 
Telco Customer Churn
1. Separar los datos en train (70%)/test(30%)
2. Elegir algún algoritmo de Ensamble (e.g AdaBoost, 
Gradient Boosting, XGBoost, RandomForest o Light 
GBM)
3. Ajustar dicho algoritmo a los datos 
4. Evaluar el performance del algoritmo y obtener 
conclusiones
¿Preguntas?
CLASE N°50
Glosario
Métodos de Ensamble: También                   Boosting: es un método de aprendizaje 
llamados “métodos combinados”,                 conjunto que combina un conjunto de 
intentan ayudar a mejorar el rendimiento       modelos simples para minimizar los errores 
                                              de entrenamiento. Se selecciona una 
de los modelos de Machine Learning al          muestra aleatoria de datos y se ajusta con 
mejorar su precisión. Existen tres             un modelo y luego se entrena 
metodologías: Bagging, Boosting y              secuencialmente. Con cada iteración, las 
Stacking                                       reglas débiles de cada clasificador 
                                              individual se combinan para formar una 
Bagging: son métodos donde los                 regla de predicción fuerte.
algoritmos simples son usados en paralelo      Sus algoritmos más importantes son: 
con el fin de aprender con base en una         AdaBoost, GradientBoosting, XGBoost y 
estructura en paralelo de cada modelo          LightGBM
individual. El algoritmo más importante es 
el Random Forest
Opina y valora 
esta clase
           Resumen 
       de la clase hoy
      ✓ Modelos de Ensamble
      ✓ Características
      ✓ Bagging
      ✓ Boosting
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 53. DATA SCIENCE
    Introducción al 
  procesamiento de 
 Lenguaje Natural I
Temario
                 52                       53                       54
          Introducción a           Introducción al           Introducción al 
          Deep Learning           Procesamiento de         Procesamiento de 
                                  Lenguaje Natural I       Lenguaje Natural II
          ✓                        ✓  Procesamiento del 
             Introducción a Deep      lenguaje natural
             Learning                                       ✓ Introducción a spaCY
                                   ✓  Expresiones regulares
          ✓ Perceptrón y perceptrón                         ✓ Análisis de 
             multi-capa            ✓  Bag of words
                                                               sentimiento 
          ✓ CNN                    ✓  NLTK
          ✓ RNN                    ✓  Corpus, Document team, 
                                      Matrix y Term Document 
                                      Matrix
Objetivos de la clase
         Introducir a los estudiantes al mundo del 
         procesamiento de Lenguaje Natural
         Explicar las técnicas básicas del dominio
   MAPA DE CONCEPTOS
                                                     NLP y características
                                                                         Clase 
                                                       Bag of Words      53
                                                      Intro NLTK, DTM y 
                                                        TF-IDF score
         Introducción al 
       procesamiento de                              Otras técnicas NLP
        Lenguaje Natural
                                                       Intro a spacy
                                                        Análisis de 
                                                       sentimiento
    Repaso
             Les proponemos tomarse unos minutos 
             para realizar un repaso de los conceptos 
              aprendidos en Kahoot, ¿están listos?
                  Profe, puedes compartir el 
                   PIN o link de acceso al 
                        juego
Procesamiento de 
lenguaje natural
Definición
Procesamiento de lenguaje 
Natural (NLP)
El procesamiento del lenguaje natural (NLP, por sus 
siglas en inglés) se refiere a la rama de la inteligencia 
artificial o IA, que se ocupa de brindar a las 
computadoras la capacidad de comprender textos y 
palabras habladas de la misma manera que lo hacen los 
seres humanos.
Procesamiento de lenguaje 
Natural (NLP)
NLP combina la lingüística computacional con modelos 
estadísticos, ML y de Deep Learning. Juntas, estas 
tecnologías permiten a las computadoras procesar el 
lenguaje humano en forma de texto o datos de voz y 
"comprender" su significado completo, teniendo en 
cuenta la intención y el sentimiento del hablante o 
escritor.
Origen
NLP Eras:
Antes y post Deep Learning
             Te invitamos a recorrer las eras NLP en este 
                     Genially interactivo
Podrás hacer click sobre cada uno de los elementos interactivos para obtener información sobre el origen del NLP
Principales tareas 
de NLP
Principales tareas de NLP
Dentro de las tareas más importantes 
podemos resaltar:
1. Análisis de sentimiento
2. Name Entity Recognition (NER)
3. Stemming y Lemmantization
4. Bag of Words
5. TF-IDF
6. Wordclouds
1) Análisis de sentimiento
El análisis de sentimientos es una de las 
técnicas de NLP más populares que 
implica tomar un fragmento de texto (por 
ejemplo, un comentario, una reseña o un 
documento) y determina si los datos 
tienen un sentido positivos, negativos o 
neutrales. Tiene muchas aplicaciones en 
salud, atención al cliente, banca, etc.
2) Reconocimiento de entidades 
(NER)
El NER es una técnica utilizada para 
ubicar y clasificar entidades nombradas 
en texto en categorías como personas, 
organizaciones, ubicaciones, expresiones 
de tiempo, cantidades, valores 
monetarios, porcentajes, etc. Se utiliza 
para optimizar algoritmos de motores de 
búsqueda, sistemas de recomendación , 
atención al cliente, etc.
3) Stemming y Lematización
Stemming: truncar una palabra a su 
raíz. Por ejemplo, las palabras "amigos", 
"amistad", "amistad" se reducirán a 
"amigo". 
Lematización: A diferencia del 
stemming, la lematización extrae el lema 
correcto de cada palabra, por lo que 
muchas veces requieren de un diccionario 
del idioma para poder categorizar cada 
palabra correctamente.
4) Bag of words
El modelo Bag of Words (BoW) es una 
representación que convierte el texto en 
vectores de longitud fija. Esto nos ayuda 
a representar texto en números para que 
podamos usarlo para modelos de 
aprendizaje automático. Al modelo no le 
importa el orden de las palabras, solo le 
preocupa la frecuencia de las palabras en 
el texto.
5) TF-IDF
Calcula "pesos" que representan la 
relevancia de una palabra para un 
documento en una colección de 
documentos (también conocido como 
corpus). El valor de TF-IDF aumenta 
proporcionalmente al número de veces 
que aparece una palabra en el 
documento y se compensa con el número 
de documentos del corpus que contienen 
la palabra. 
6) Wordcloud
Wordcloud es una técnica popular que 
nos ayuda a identificar las palabras clave 
en un texto. En una nube de palabras, las 
palabras más frecuentes tienen una 
fuente más grande, mientras que las 
palabras menos frecuentes tienen una 
fuente más pequeña o más delgada.
Características de 
 NLP
Características de Deep Learning
Algunas de las características más 
importantes de NLP son:
1. Conversión text-to-speech y viceversa
2. Traducción de lenguaje
3. Categorización e indexación
4. Organización de documentos
5. Identificación de sentimientos
6. Aprendizaje supervisado en su mayoría
Fases típicas de NLP
En general se tienen 5 fases 
preponderantes que son:
1. Análisis léxico y morfológico
2. Análisis sintáctico
3. Análisis semántico
4. Integración de discurso
5. Análisis pragmático
¿Cómo se ejecuta el análisis de NLP?
El procesamiento del lenguaje natural implica 
la lectura y comprensión del lenguaje hablado 
o escrito a través de una computadora. Esto 
incluye, por ejemplo, la traducción automática 
de un idioma a otro, pero también el 
reconocimiento de palabras habladas o la 
respuesta automática a preguntas.
Ventajas de NLP
Ventajas de NLP
1. Ofrece respuestas exactas a las preguntas 
sin información no deseada
2. La precisión de la respuesta incrementa con 
la cantidad de información 
3. Se pueden obtener respuestas sobre 
cualquier tema
4. Fácil de implementar hoy dia
5. Es menos costoso que contratar personal
6. Permite realizar comparaciones 
7. Tiempos de respuesta al cliente optimizados
Desventajas de NLP
Desventajas de NLP
1. Si deseamos desarrollar modelos nuevos sin 
modelos pre entrenados puede tomar 
bastante tiempo entrenar
2. Diseñado para tareas únicas y específicas 
3. Puede que no puede dar respuesta a las 
preguntas deseadas si están mal redactadas
4. No es 100% de confiable, existe la 
posibilidad de error en su predicción y 
resultados
5. En algunos casos su implementación puede 
ser complejo
Para pensar
Ahora bien, ¿Han escuchado sobre alguna técnica 
de NLP?, ¿El algoritmo GPT-3 de AI lo han 
escuchado anteriormente?
Contesta en el chat de Zoom 
Expresiones 
regulares
Definición
Expresiones regulares
Las expresiones regulares (regex) son 
cadenas de texto codificadas que se 
utilizan como patrones. Comenzaron a 
surgir en la década de 1940 como una 
forma de describir lenguajes, pero 
realmente comenzaron a aparecer en el 
mundo de la programación durante la 
década de 1970. El primer lugar donde se 
encontraron fue en el editor de texto QED 
escrito por Ken Thompson.
Aplicaciones
Validación
El uso más común de expresiones 
regulares es la validación de formularios, 
es decir, validación de correo electrónico, 
validación de contraseña, validación de 
número de teléfono y muchos otros 
campos del formulario.
Esto nos permite identificar inconsistencias 
que puedan existir en dichos formularios
Detalles de cuentas bancarias
Seguro han notado que cada banco tiene 
un código IFSC para sus diferentes 
sucursales que comienza con el nombre 
del banco. El número de la tarjeta de 
crédito consta de 16 dígitos y los primeros 
dígitos representan si la tarjeta es Master, 
Visa o Rupay. En todos estos casos, se usa 
regex con propósitos de validación
Minería de datos
Cuando los datos están presentes en forma 
no estructurada, es decir, en forma de 
texto, es necesario convertirlos en 
números para entrenar el modelo. Por lo 
tanto, regex juega un papel importante en 
el análisis de los datos, encuentra patrones 
en los datos y, finalmente, realiza 
operaciones en el conjunto de datos.
NLP
En el NLP una computadora entiende y 
genera el lenguaje humano. Las 
expresiones regulares se utilizan para 
eliminar las palabras innecesarias, es decir, 
detener las palabras del texto, lo que 
ayuda en la limpieza de datos. Regex 
también se usa para analizar los textos y, 
por lo tanto, ayuda en la predicción del 
algoritmo para procesar los datos.
Redes sociales
Las plataformas de redes sociales como 
Google, Facebook, Twitter brindan varias 
técnicas de búsqueda, que son diferentes y 
eficientes de una búsqueda normal. Si 
conocemos estas técnicas, podemos 
utilizar expresiones regulares en el 
backend para procesar estas búsquedas.
Ejemplos
Ejemplos
● Si queremos extraer todos los números 
de un documento: “[0-9]+”
● Si se quieren remover todos los 
caracteres que no son números: “[^0-
9]+”
● Si queremos extraer palabras que 
empiezen por “A” y terminan en “h”: 
“^A[a-zA-Z]+h$”
● Extraer correos electrónicos: “^[a-zA-Z]
[a-zA-Z0-9._+-]+@[A-Za-z]+.[A-Za-z]”
Ventajas y 
Desventajas
Ventajas y desventajas
                    Ventajas                                 Desventajas
                Bastante flexibles                 Difíciles de leer ya que el contexto 
                                                              puede variar
              Procesamiento rápido               Dificultad en el debug si no hay match
           Independiente del lenguaje             Depende mucho de la calidad de los 
                                                                 datos
        Mucho trabajo en pocas líneas de        Se pueden cometer typos que dificultan 
                      código                                   el análisis
Para pensar
¿La regular expression [a-z] extrae solo 
las letras a y z de los documentos de 
análisis?
Verdadero/Falso 
Contesta en el chat de Zoom 
Para pensar
Falso
Recuerden que la expresión [a-z] permite 
extraer caracteres entre la a y la z de 
cualquier texto y no solo las letras a y z. 
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Bag of Words
Definición
Bag of Words
Un modelo de bolsa de palabras, o BoW, 
es una forma de extraer características 
del texto para usar en el modelado, 
como con algoritmos de aprendizaje 
automático.
El enfoque es muy simple y flexible, y se 
puede usar de muchas maneras para 
extraer características de los 
documentos.
Bag of words
Una bolsa de palabras es una 
representación de texto que describe la 
ocurrencia de palabras dentro de un 
documento, lo cual implica dos cosas:
1. Un vocabulario de palabras 
conocidas.
2. Una medida de la presencia de 
palabras conocidas.
Bag of words
Un supuesto importante es que solo del 
contenido podemos aprender algo sobre 
el significado del documento.
La bolsa de palabras puede ser tan 
simple o compleja como queramos. La 
complejidad viene tanto al decidir cómo 
diseñar el vocabulario de palabras 
conocidas o cómo calificar la presencia 
de palabras conocidas.
Aplicaciones
NLP
El modelo de bolsa de palabras se usa 
ampliamente en el procesamiento del 
lenguaje natural y la recuperación de 
información (IR). También se usa con 
bastante frecuencia en los métodos de 
clasificación de documentos donde la 
frecuencia de aparición de cada palabra se 
usa como una característica para entrenar 
un clasificador.
Computer vision
El modelo de bolsa de palabras se puede 
utilizar en problemas de visión por 
computadora con el fin de detectar 
patrones, anomalías o secuencias 
específicas que pueden ser utilizadas para 
generar modelos de Machine Learning o 
Deep Learning 
Feature Generation
La aplicación práctica más común del 
modelo de bolsa de palabras es como 
herramienta para la generación de 
características (Feature Generation). 
Después de transformar el texto en una 
"bolsa de palabras", le será posible calcular 
varias medidas diferentes que se pueden 
usar para caracterizar el texto (e.g 
Frecuencia bruta o TF-IDF)
Ejemplo
Ejemplo
Paso 1: Recolección de datos
Supongamos que tenemos las siguientes 
respuestas:
1. “Fue el mejor de los tiempos”,
2. “fue el peor de los tiempos”,
3. “era la edad de la sabiduría”,
4. “era la edad de la locura”,
Ejemplo
Paso 2: Diseño del vocabulario
Generamos una lista de las palabras únicas 
(ignorando los signos de puntuación y si se 
quiere removiendo stopwords):
[Fue, el, mejor, de, los, tiempos, peor, 
era, la, edad, sabiduría, locura]
Este es un vocabulario de 12 palabras 
según el corpus y 24 palabras en total
Ejemplo
Paso 3: Crear vectores para el 
documento
Creamos un vector de score para cada 
documento:
[Fue, el, mejor, de, los, tiempos, peor, 
era, la, edad, sabiduría, locura]
“Fue el mejor de los tiempos” 
=[1,1,1,1,1,1,0,0,0,0,0,0]
“fue el peor de los tiempos” 
=[1,1,01,1,1,1,0,0,0,0,0]
Ejemplo
Se descarta todo orden de las palabras y 
tenemos una forma consistente de extraer 
características de cualquier documento en 
nuestro corpus, listo para usar en el 
modelado.
Los nuevos documentos que se superponen 
con el vocabulario de palabras conocidas, 
con palabras fuera del vocabulario, aún 
pueden codificarse, donde solo se puntúa la 
aparición de palabras conocidas
Ventajas y 
Desventajas
                    Ventajas                                Desventajas
      Bastante flexibles en diversos casos        El vocabulario requiere un diseño 
                     de uso                                  cuidadoso.
     Permite generar modelos elementales        Representaciones dispersas son más 
        de las respuestas o documentos                  difíciles de modelar.
         Fácil de entender y programar               Cuando se cuenta con poca 
                                                 información no es sencillo generar 
                                                               insights
         Puede ser utilizado en muchos          Al descartar el orden de las palabras 
               contextos aplicados                    ignoramos su significado
Introducción a NLTK
Definición
NLTK
NLTK es un set de herramientas creado 
para trabajar con NLP en Python. Nos 
proporciona varias bibliotecas de 
procesamiento de texto con muchos 
conjuntos de datos de prueba. Se puede 
realizar una variedad de tareas, como 
tokenización, visualización de árboles 
de análisis, etc. 
Funcionalidades
Funcionalidades
Nos permite realizar diferentes tareas 
como:
1. Tokenización
2. Conversión a Lower/Upper case
3. Remoción de Stopwords
4. Stemming
5. Lematización
6. Parse Tree o generación de sintaxis
7. POS Tagging
1) Tokenización
Es un proceso de desglose del texto en 
unidades más pequeñas se llama tokens. 
los tokens son una pequeña parte de ese 
texto. Si tenemos una oración, la idea es 
separar cada palabra y construir un 
vocabulario tal que podamos representar 
todas las palabras de manera única en 
una lista. Números, palabras, etc., todos 
caen bajo fichas.
2) Conversión Lower/Upper case
Queremos que nuestro modelo no se 
confunda al ver la misma palabra con 
diferentes casos, como uno que comienza 
con mayúscula y otro sin ella, e interpretar 
ambos de manera diferente. Así que 
convertimos usualmente todas las 
palabras a minúsculas o mayúsculas  para 
evitar la redundancia en la lista de tokens.
3) Remoción de Stopwords
Cuando usamos las características de un 
texto para modelar, encontraremos mucho 
ruido. Estas son las palabras vacías como 
yo, él, ella, etc., que no nos ayudan y 
simplemente se eliminan antes del 
procesamiento para un procesamiento 
más limpio dentro del modelo. Con NLTK 
podemos ver todas las palabras vacías 
disponibles en diversos idiomas.
4) Stemming
En nuestro texto podemos encontrar 
muchas palabras como jugando, jugaron, 
juego, etc… que tienen una raíz, jugar, 
todas transmiten el mismo significado. Así 
que podemos extraer la raíz de la palabra y 
eliminar el resto. Aquí, la raíz de la palabra 
formada se llama 'tallo (stem)' y no es 
necesariamente que el stem deba existir y 
tener un significado. Con solo cometer el 
sufijo y el prefijo, generamos las raíces.
5) Lemantización
Queremos extraer la forma base de la 
palabras con la Lemantización. La palabra 
extraída aquí se llama Lemma y está 
disponible en el diccionario. Tenemos el 
corpus de WordNet y el lema generado 
estará disponible en este corpus. NLTK nos 
proporciona WordNet Lemmatizer que 
utiliza la base de datos de WordNet para 
buscar lemas de palabras.
6) Parse Tree
Podemos definir la gramática de acuerdo 
con el idioma de interés y luego usar la 
librería NLTK con su metódo RegexpParser 
para extraer todas las partes del discurso 
(Parts of Speech) de la oración y dibujar por 
ejemplo árboles de decisión para visualizar 
las asociaciones.
7) POS Tagging
POS Tagging se utiliza en el procesamiento 
de texto para evitar la confusión entre dos 
mismas palabras que tienen significados 
diferentes. Con respecto a la definición y el 
contexto, le damos a cada palabra una 
etiqueta particular y las procesamos. Aquí 
se utilizan dos pasos:
1. Tokenizar texto (word_tokenize).
2. Aplicar pos_tag de NLTK.
Corpus, Document 
Term Matrix y Term 
Document Matrix
Corpus
Corpus
Un corpus es una colección de texto o 
audio auténtico organizado en conjuntos 
de datos. auténtico (texto escrito o audio 
hablado por un nativo del idioma o 
dialecto). Un corpus puede estar 
compuesto desde periódicos, novelas, 
recetas, transmisiones de radio hasta 
programas de televisión, películas y 
tweets.
Corpus
En NLP, un corpus contiene datos de 
texto y voz que se pueden usar para 
entrenar sistemas de inteligencia 
artificial y ML. Si un usuario tiene un 
problema u objetivo específico que desea 
abordar, necesitará una recopilación de 
datos que respalde, o al menos sea una 
representación de, lo que busca lograr 
con el aprendizaje automático y la PNL.
Características de un buen Corpus
1. Gran tamaño: mientras más grande 
mejores resultados se pueden obtener
2. Buena calidad de datos: errores 
pequeños de sintaxis pueden generar 
errores de gran escala en algoritmos
3. Datos limpios: se necesitan procesos 
de limpieza avanzados para evitar 
errores
4. Balance: mantener el balance es 
importante cuando se desean realizar 
predicciones
Problemas típicos al elaborar Corpus
A pesar de que es un concepto muy simple y 
genérico pueden ocurrir inconvenientes 
como:
1. Decidir el tipo de data a ser analizada 
puede ser complejo
2. Es necesario disponibilidad de datos
3. Se debe garantizar calidad en los datos
4. Hay que estar dispuestos a poder 
adecuar los datos en términos de la 
cantidad disponible (alto dinamismo)
Document Term 
Matrix
Document Term Matrix
Es una matriz matemática que describe la 
frecuencia de los términos que aparecen en 
una colección de documentos y donde:
1. Cada fila representa un documento
2. Cada columna representa un término 
(palabra)
3. Cada valor (típicamente) contiene el 
número de apariciones de ese término en 
ese documento
Document Term Matrix
Las matrices de términos de documento 
(Document Term Matrix)  a menudo se 
almacenan como un objeto de matriz 
dispersa o sparse por sus siglas en inglés. 
Estos objetos se pueden tratar como si 
fueran matrices (por ejemplo, acceder a filas 
y columnas), pero se almacenan en un 
formato más eficiente.
Term Document 
Matrix
Document Term Matrix
Term Document Matrix (TDM)  representa 
vectores de documentos en forma de matriz 
en la que las filas corresponden a los 
términos del documento, las columnas 
corresponden a los documentos del corpus y 
las celdas corresponden a los pesos de los 
términos. Es simplemente la transpuesta de 
la Document Term Matrix
TF-IDF score
TF-IDF score
Es una forma de valorar aquellos términos que no son tan comunes en el 
corpus (IDF relativamente alto), pero que aún tienen un nivel razonable de 
frecuencia (TF relativamente alto). Es la métrica utilizada con mayor 
frecuencia para calcular los pesos de los términos en un modelo de espacio 
vectorial.
Ejemplo en vivo
Analizaremos un ejemplo donde utilizaremos el 
NLP para encontrar insights y patrones 
interesantes para revisiones de películas de 
Amazon, crearemos n-grams, identificaremos 
palabras más relevantes en buenas y malas 
revisiones entre otras tareas.
Análisis descriptivo NLP
En esta oportunidad utilizaremos lo aprendido en clase 
para poner en práctica los conceptos de NLP
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Análisis Descriptivo 
NLP
Nos reuniremos en breakout rooms y 
formaremos grupos. Se les propone:
✔ Escoger 2 periódicos digitales con 
distintas temáticas (por ejemplo, uno 
que hable sobre economía y otro sobre 
deportes)
✔ Copiar 5 a 10 titulares de la página 
principal y guardarlos en un csv
✔ Utilizando la técnica bag of words deben 
identificar las 10 palabras más 
frecuentes para cada periódico
¿Preguntas?
CLASE N°53
Glosario
NLP:se refiere a la rama de la inteligencia      Bag of Words:  es una forma de extraer 
artificial o IA, que se ocupa de brindar a       características del texto para usar en el 
las computadoras la capacidad de                 modelado, como con algoritmos de 
comprender textos y palabras habladas de         aprendizaje automático
la misma manera que lo hacen los seres 
humanos.                                         TF-IDF score: Es una forma de valorar 
.                                                aquellos términos que no son tan comunes 
Expresiones regulares: son cadenas de            en el corpus (IDF relativamente alto), pero 
texto codificadas que se utilizan como           que aún tienen un nivel razonable de 
patrones. Comenzaron a surgir en la              frecuencia (TF relativamente alto). Es la 
década de 1940 como una forma de                 métrica utilizada con mayor frecuencia 
                                                para calcular los pesos de los términos en 
describir lenguajes, pero realmente              un modelo de espacio vectorial.
comenzaron a aparecer en el mundo de la 
programación durante la década de 1970
Opina y valora 
esta clase
           Resumen 
       de la clase hoy
      ✓ Introducción a NLP
      ✓ Expresiones regulares
      ✓ Bag of Words (BoW)
      ✓ Intro NLTK
      ✓ Corpus, Document Term Matrix, Term Document 
        Matrix
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 43. DATA SCIENCE
     Algoritmos de 
     Agrupación I
 Temario
                        42                               43                               44
                Algoritmos de                    Algoritmos de                     Algoritmos de 
                clasificación y                   Agrupación I                      Agrupación II
                   Regresión
              ✓ SVM
                                                                                  ✓ PCA
              ✓ Ejemplos clasificación            ✓ K means
                  errónea                                                         ✓ Reducción de 
              ✓ Regresión Lineal y                ✓ DBSCAN                             dimensionalida
                  Múltiple                                                             d
              ✓ Optimización Hiper                                                ✓ Detección de 
                  parámetros                                                           Outliers
Objetivos de la clase
         Profundizar en el Aprendizaje No Supervisado 
         Identificar conceptos de aplicación de los 
         modelos de Clustering
  MAPA DE CONCEPTOS
                                                   K Means
                                                   DBSCAN        Clase 
                                                                 43
                                                  HDBSCAN
     Algoritmos de                                Reducción de 
      Agrupación                                 dimensionalidad
                                                     PCA
                                                  Detección de 
                                                   outliers
    Repaso
             Les proponemos tomarse unos minutos 
             para realizar un repaso de los conceptos 
              aprendidos en Kahoot, ¿están listos?
                  Profe, puedes compartir el 
                   PIN o link de acceso al 
                        juego
K means
Para pensar
Recuerdan ¿Cuál era la utilidad del algoritmo K 
means?. ¿En qué contextos se podía aplicar y con 
qué fin? ¿Hacía parte del aprendizaje 
Supervisado o No supervisado?
Contesta en el chat de Zoom 
Definición
K means
K-means es un algoritmo de aprendizaje 
no supervisado que se utiliza para 
resolver problemas de agrupamiento. 
Sigue un procedimiento simple de 
clasificar un conjunto de datos dado en 
una serie de grupos, definidos por la 
letra "k", que se fija de antemano por 
medio de un proceso iterativo
¿Para qué sirve?
El objetivo de este algoritmo es 
encontrar grupos en los datos, con 
el número de grupos representados 
por la variable K. 
El algoritmo funciona de manera 
iterativa para asignar cada punto de 
datos a uno de los grupos K en función 
de las características que se 
proporcionan. 
¿Cómo se agrupa?
Los puntos de datos se agrupan en 
función de la similitud de las 
características. Los resultados de K 
Means son:
● Los  centroides  de  los  clústeres  K,  que 
pueden  ser  usados  para  etiquetar 
nuevos datos.
● Etiquetas para los datos de  formación, 
cada  punto  de  datos  se  asigna  a  un 
único clúster.
Ejemplo aplicado
K means
Los datos reales pueden venir en 
diferentes formatos y formas y 
muchas veces interesa generar 
grupos automáticamente 
K means
                         Paso 1: Seleccionar el 
                         número de clusters que se 
                         quieren obtener (k=3)
                         Paso 2: Seleccionar 
                         aleatoriamente 3 diferentes 
                         puntos 
                          Paso 3: Medir la distancia 
                          entre el primer punto y los 3 
                          centroides
K means
                       Paso 4: Asignar el cluster al 
                       grupo más cercano
                       Paso 5: Repetimos el mismo 
                       proceso para los demás valores
                       Paso 6: Calcular la media de 
                       cada cluster
K means
                         Paso 7: Repetir el mismo 
                         proceso de cálculo de distancias 
                         y asignación
                         Paso 8: Se puede revisar que 
                         tan bien quedo el algoritmo con 
                         lo que se conoce como la 
                         Variación total de clusters 
                         De lo anterior se puede 
                         deducir que la elección inicial 
                         de los centroides es 
                         importante
K means
                          Paso 9: La selección de 
                          puntos se puede cambiar 
                          Paso 10: El proceso se 
                          repite iterativamente varias 
                          veces hasta lograr 
                          convergencia 
K means
   Si repetimos el proceso podemos identificar cuál 
   metodología funciona mejor (Esto puede ser mucho 
   mayor que 3 veces)
K means - Selección valor óptimo 
K                                k=1
                              k=2   El objetivo es ir iterando el valor 
                                    de K de tal manera que se 
                                    identifique el valor de K que 
                                    minimice la variación total
                              k=3
K means - Selección valor óptimo 
K
                       Cada vez que se agrega un cluster 
                       la variación total se hace menor 
                       hasta el punto que tendremos un 
                       grupo para cada individuo y no 
                       habrá variation total
   Elbow Method (Método del 
   Codo)
Ejemplo en vivo
Dentro de la carpeta de clase exploraremos 
el notebooks “K - Means - CoderHouse 
(Ejemplo 1).ipynb” con el fin de 
comprender el funcionamiento del 
algoritmo K means
 DBSCAN
Para pensar
¿Alguna vez han escuchado de algoritmos de 
agrupación basados en Densidad de puntos? 
¿Conocen el algoritmo DBSCAN y HDBSCAN?
Contesta en el chat de Zoom 
Problemas del K 
means
¿Para qué sirve?
        Puede pasar a veces que si no se 
        escalan/normalizan los datos, no anda nada 
        bien porque asume que los datos son 
        “esféricos”.
        Como solución se suele utilizar algoritmos de 
        clustering basado en densidades, como el 
        HDBSCAN o el DBSCAN
Definición
DBSCAN
DBSCAN  significa  Density  Based 
Spatial Clustering of Application with 
Noise
Fue propuesta por Martín Ester et al. 
en 1996. DBSCAN es un algoritmo de 
agrupamiento basado en la densidad 
que  funciona  asumiendo  que  los 
agrupamientos  son  regiones  densas 
en el espacio separadas por regiones 
de menor densidad.
Funcionamiento
Cómo funciona el DBSCAN
Separa los clusters de alta densidad de 
los de baja densidad
Input: Puntos, min_pts, eps
Output: Clusters con densidad 
Cada punto es:
Core point:  tiene al menos min_pts en 
su vecindario
Border point: no es core pero tiene 
al menos 1 core point en su vecin-
dario
Noise point: No es core o border
Ventajas/Desventajas DBSCAN
Ventajas
1. No necesita número de clusters 
2. Clustering en diferentes formas 
3. Elimina puntos de ruido
Desventajas
4. Sensible a los parámetros (si el
min_pts es pequeño problemas)
5. Produce malos resultados 
cuando hay mucho ruido.
eps (valor crítico para considerar 
dos puntos como vecinos)
HDBSCAN
HDBSCAN
Es un algoritmo de clustering o 
agrupamiento basado en la densidad que 
puede ser utilizado para identificar 
clústeres de cualquier forma en un 
conjunto de datos que contiene ruido y 
valores atípicos. La idea básica detrás del 
enfoque de agrupamiento basado en la 
densidad se deriva de un método intuitivo de 
 agrupamiento humano. 
HDBSCAN
HDBSCAN= High DBSCAN
 -    Se enfoca mucho más en las altas 
      densidades
 -    Reduce la velocidad de clustering 
      en comparación con otros métodos
 -    Tiene un parámetro adicional 
      min_cluster_size que define que tan 
      grande debe ser un cluster para ser 
      formado.
HDBSCAN en acción
Por ejemplo, al mirar la figura de la 
derecha, uno puede identificar 
fácilmente tres grupos junto con 
varios puntos de ruido, debido a las 
diferencias en la densidad de puntos.
        Importante
                  Por lo tanto, los clústeres son 
                  regiones densas en el espacio 
                  de datos, separadas por 
                  regiones de menor densidad 
                  de puntos. El algoritmo 
                  HDBSCAN se basa en esta 
                  noción intuitiva de 
                  clústeres y ruido. 
K Means vs 
DBSCAN/HDBCAN
K means vs DBSCAN/HDBSCAN
                      K-Means                                    DBSCAN/HDBSCAN
     La agrupación del K means es sensible a la       No es necesario especificar el número de 
         cantidad de agrupaciones definidas                        conglomerados
    Su agrupación en Clústeres no funciona bien        Su agrupación en Clústeres maneja de 
    con valores atípicos y conjuntos de datos con      manera eficiente los outliers y los datos 
                        ruido                                         ruidosos
     Las densidades variables de los puntos de          La agrupación basada en clústeres no 
       datos no afectan el algoritmo K means        funciona muy bien para conjuntos dispersos o 
                                                         para puntos con densidad variable
     La agrupación en clústeres de K-means es          No puede manejar de manera eficiente 
      más eficiente para grandes conjuntos de          conjuntos de datos con alta dimensión
                       datps
Ejemplo en vivo
Dentro de la carpeta de clase exploraremos 
el notebooks “HDBSCAN - CoderHouse 
(Ejemplo 2).ipynb” con el fin de 
comprender el funcionamiento del 
algoritmo HDBSCAN
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Elaborando un algoritmo de 
    agrupación
Utilizaremos lo aprendido en clase para crear un 
     modelo de agrupación
     Duración: 20-30 mins
      ACTIVIDAD EN CLASE
Elaborando un 
algoritmo de 
 En esta oportunidad nos reuniremos en grupos de máximo 4 
agrupación
 personas.
   1.  Elegir 4 variables independientes que consideren útiles 
       para predecir los “costos” de nuevos clientes
   2.  Realizar el Encoding de las variables independientes (una 
       persona hace el código y comparte, los demás ayudan 
       dando instrucciones, etc) para generar matriz para el 
       modelo
   3.  Generar una predicción de clusters sobre el dataset y 
       agregar esta predicción como la variable cluster
   4.  Agregar la variable costos al dataset de entrenamiento y 
       describir los clusters (pensar en cómo). 
       Ayuda: Pueden agrupar por cluster y utilizar funciones de 
       agregación como la media
¿Preguntas?
CLASE N°43
Glosario
K means: algoritmo de aprendizaje no             HDBSCAN: algoritmo de clustering o 
supervisado que se utiliza para resolver         agrupamiento basado en la densidad 
problemas de agrupamiento por medio de           que puede ser utilizado para identificar 
una serie de grupos, definidos por la letra      clústeres de cualquier forma en un 
"k", que se fija de antemano por medio de        conjunto de datos que contiene ruido 
un proceso iterativo                             y valores atípicos. La idea básica detrás 
                                                del enfoque de agrupamiento basado en la 
DBSCAN: algoritmo de agrupamiento                densidad se deriva de un método intuitivo 
basado en la densidad que funciona               de agrupamiento humano. 
asumiendo que los agrupamientos son 
regiones densas en el espacio separadas 
por regiones de menor densidad.
Opina y valora 
esta clase
                 Resumen 
           de la clase hoy
           ✓ K means
           ✓ DBSCAN
           ✓ HDBSCAN
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 07.  DATA SCIENCE
  Visualizaciones y 
primeros pasos con 
Data Science (parte I)
Temario
            06                            07                            08
    Introducción a la            Visualizaciones y            Visualizaciones y 
    manipulación de               primeros pasos               primeros pasos 
   datos con Pandas                    con DS I                    con DS II
        (parte II)
    ✓ Fuentes de datos             ✓Matplotlib                 ✓ Seaborn
    ✓ Series y data frame                                      ✓ Tipos de gráficos
                                   ✓Tipos de gráficos
    ✓ Selección de datos                                       ✓ Subplot y Facetgrip
                                   ✓Customizaciones            ✓ Customizaciones sobre 
    ✓ Operaciones                    sobre gráficos
                                                                gráficos
    ✓ Agregaciones                                             ✓ Nutshell
    ✓ Strings                                                  ✓ Scikit - Learn
Objetivos de la clase
         Conocer las librerías más utilizadas para 
         visualización Python
         Aprender a graficar datos en Python
         Entender el uso básico de las librerías más 
         utilizadas: Matplotlib y Seaborn
MAPA DE CONCEPTOS
                                            Gráfico de líneas
                      Interfaces            Gráfico de puntos
Introducción a        Gráficos comunes        Gráfico de barras
Matplotlib
                      Subgráficos            Histogramas
                                              Boxxplot
Matplotlib:
Comandos básicos
¿Por qué Matplotlib?
✓ Es una biblioteca multiplataforma           ✓ Proporciona una API orientada a 
    para hacer gráficos 2D a partir de           objetos que ayuda a incrustar 
    datos en matrices.                           gráficos en aplicaciones que utilizan 
✓ Está escrito en Python y hace uso              kits de herramientas de la GUI de 
    de NumPy.                                    Python como PyQt, 
                                                 WxPythonotTkinter
                                              ✓ Matplotlib fue escrito originalmente 
                                                 por John D. Hunter en 2003.
¿Por qué Matplotlib?
✓ Es la librería de visualización más 
utilizada en el entorno de Python.
✓ Es sencilla y fácil de usar.
✓ Permite un alto nivel de 
personalización de los gráficos.
✓ Es open source.
✓ Es la base sobre la que se 
construyen otras librerías como 
Seaborn.
Interfaces Matplotlib
Interfaces
Las interfaces en Matplotlib definen la            ✓ Interfaz orientada a estados: 
forma en la que interactuamos con el                   orientada a usuarios de MATLAB 
gráfico. Además, proveen compatibilidad                para mantener compatibilidad.
con el lenguaje que inspiró la librería:           ✓ Interfaz orientada a objetos: 
MATLAB. Existen dos interfaces                         Permite mayor grado de control 
disponibles:                                           sobre los gráficos porque los 
                                                      tratamos como objetos. Más 
                                                      Pythonista, y la más utilizada.
Interfaz orientada a 
objetos
✓ Nos permite reutilizar objetos.
✓ Es mejor en momentos en los que es 
necesario dibujar varios gráficos al 
tiempo.
✓ La idea detrás de esta interfaz es 
crear objetos tipo figura (darles un 
nombre) para ser usados después.
✓ El usuario crea explícitamente la 
figura y realiza un seguimiento de 
la figura y los objetos de los ejes.
Interfaz orientada a 
estados
✓ La interfaz pyplot es una interfaz 
basada en estados.
✓ La principal cualidad de la interfaz 
basada en estados es que nos 
permite agregar elementos y o 
modificar el gráfico, siempre que lo 
necesitemos.
✓ La interfaz de Pyplot comparte 
muchas similitudes en sintaxis y 
metodología con MATLAB.
Setup para la clase
Antes que nada,               import matplotlib as mpl
importemos las librerías      import matplotlib.pyplot as plt
                          import seaborn as sns
que usaremos en la clase.     import pandas as pd
Los estilos por defecto de 
Matplotlib no son muy 
estéticos, podemos            mpl.style.use('bmh')
cambiarlos fácilmente.
Tipos de estilos
Primeros pasos
Primeros pasos
✓ Grafiquemos una línea que una los puntos con 
    coordenadas (x,y) = (1, 2) y (x,y) = (3, 4).  Necesitamos:
1. Un arreglo con las dos coordenadas del eje x = [1, 3]
2. Un arreglo con las dos coordenadas del eje y =  [2, 4]
  Interfaz orientada a objetos                          Interfaz orientada a 
  estados
  fig, ax = plt.subplots()                  plt.plot([1, 3], [2, 4])
  ax.plot([1, 3], [2, 4])
Primeros pasos
✓ Ambas formas retornan el mismo resultado.                   REEMPLAZAR 
✓ La interfaz orientada a estados parece más                   POR IMAGEN
   simple, pero al hacer gráficos más complejos 
   y profesionales es más difícil de 
   implementar.
Para evitar confusión, usaremos y recomendamos 
la interfaz orientada a objetos.
Comandos básicos
                 Contextualizar los 
                               gráficos
                             Etiquetar los ejes        ax.set_xlabel y  
                                                       ax.set_ylabel
   Cuando generemos 
   visualizaciones, una      Añadir un título          ax.set_title
   buena práctica es                            REEMPLAZAR 
                                                 POR IMAGEN
   procurar siempre          Añadir una leyenda        ax.legend
   incluir información 
   acerca de lo que se       Definir límites           ax.set_xlim y ax.set_ylim
   muestra.
                             Líneas ‘h’ o ‘v’          ax.axhline y ax.axvline
Ejemplo
✓ Importemos el Data Frame de precipitaciones de la clase pasada. 
  Los datos están disponibles en este enlace.
df_lluvias = pd.read_csv('<ruta>/pune_1965_to_2002.csv')
df_lluvias.head()
✓ Por conveniencia, ponemos a Year como índice del Data Frame y lo 
  eliminamos de las columnas
df_lluvias.index = df_lluvias['Year']
df_lluvias = df_lluvias.drop('Year', axis='columns')
Tras el cambio de índice, los datos quedan de la siguiente 
          manera:
  Grafiquemos las precipitaciones acumuladas para los distintos años:
   ✓ Para el eje x, seleccionamos los años
   x = df_lluvias.index
   ✓ Para el eje y, acumulamos las precipitaciones por año
   y = df_lluvias.sum(axis='columns')
   y
   ✓ Definimos los objetos fig y ax, los cuales contendrán la figura:
     fig, ax = plt.subplots(figsize=(12, 4))
     ax.plot(x, y, label='Precipitaciones acumuladas')
   ✓ Etiquetamos los ejes, añadimos un título e insertamos la leyenda:
     ax.set_xlabel('Año')  
     ax.set_ylabel('Precipitación acumulada (mm.)')
     ax.set_title('Precipitaciones acumuladas según el año')
     ax.legend() 
   El resultado…
Algunas observaciones
✓ El método ax.plot recibe el parámetro figsize, que define 
el tamaño del gráfico. Para una figura rectangular de 12x4
fig, ax = plt.subplots(figsize=(12, 4))
✓ El método ax.legend inserta la leyenda en alguna esquina 
que no tape el gráfico. Esto se puede modificar con el 
parámetro loc.
fig, ax = plt.subplots(figsize=(12, 4))
Forzará a que la leyenda aparezca en la esquina superior 
derecha.
CoderTip
Si no recordamos que parámetros acepta un método o 
función, podemos escribir el nombre del método 
seguido de un "?". Esto abre la documentación 
directamente en el entorno de Jupyter.
Por ejemplo, ax.legend?
muestra información relacionada al método legend
Algunas observaciones
✓ Si se observa la serie de tiempo anterior, pueden notarse 
los bordes vacíos en los laterales. Pueden recortarse con  
ax.set_xlim
ax.set_xlim(df_lluvias.index[0], df_lluvias.index[-1])
Exportando los gráficos
✓ Matplotlib permite guardar las visualizaciones en la 
computadora.
✓ Algunos de los formatos soportados son jpeg, jpg, png, 
pdf y svg
✓ El gráfico se guardará en la ruta actual, pero igualmente 
puede especificar cualquier otra ruta.
fig.savefig("precipitaciones_año.pdf")
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Para pensar
¿Qué tipos de gráficos descriptivos usarían para 
la presentación de información?
Utilizaremos en notebook Clase_07.ipynb (Ejemplo 
1) para entender el funcionamiento básico de 
Matplotlib  
Contesta en el chat de Zoom 
Tipos de
gráficos
Gráfico de líneas
Gráficos de 
líneas
✓ Son adecuados para visualizar datos con                       REEMPLAZAR 
   secuencialidad temporal, como las series de                   POR IMAGEN
   tiempo.
✓ Se grafican con el método  ax.plot(x, y)
✓ En caso de no especificarse x, matplotlib toma 
   como coordenadas en x al arreglo de números 
   enteros [0, 1, 2, …, n]
Gráficos de 
líneas
✓ Tipo de gráfico que muestra información 
    como una serie de puntos de datos                             REEMPLAZAR 
    conectados por segmentos de línea recta.                      POR IMAGEN
✓ Se utilizan generalmente para visualizar el 
    movimiento direccional de uno o más datos a 
    lo largo del tiempo. 
✓ En este caso, el eje X (fecha) eje y contiene 
    la cantidad medida (precio de las acciones, el 
    clima, las ventas mensuales, etc). Es ideal 
    para visualizar series de tiempo.
Ejemplo
fig, ax = plt.subplots()
ax.plot([0, 1, 2, 3, 4, 5, 6], [1, 5, 2, 4, 8, 9, 2])
equivale a…
fig, ax = plt.subplots()
ax.plot([1, 5, 2, 4, 8, 9, 2])
Gráfico de puntos
Gráficos de 
puntos
 ✓ Se utiliza para visualizar la relación entre las                      REEMPLAZAR 
     dos variables.                                                       POR IMAGEN
 ✓ Si el valor a lo largo del eje Y parece 
     aumentar a medida que aumenta (o 
     disminuye) el eje X, podría indicar una 
     relación lineal positiva (o negativa).
 ✓ Si los puntos se distribuyen al azar sin un 
     patrón obvio, (posiblemente falta de relación 
     de dependencia).
Gráficos de puntos
✓ Útiles cuando se tienen una gran cantidad de 
datos numéricos emparejados
✓ Permiten visualizar la relación entre las variables 
por medio de la nube de puntos
✓ Nube de puntos “alineada” = relación 
 fuerte
✓ Nube de puntos “dispersa” = relación débil 
 o nula
✓ Se grafican con ax.scatter
                                            Ejemplo
           Consideremos las mediciones del peso y altura de 50 alumnos.
           pesos = [42.8, 43.3, 42. , 44. , 44.1, 43.5, 48.1, 48.9, 47.7,46.9,50.4,
                  52.7, 51.8, 54.5, 54.2, 56.9, 55.4, 55.5, 57.1, 58.3, 63.7, 58.8,
                  64.6, 60.2, 64. , 63.8, 61.4, 66.3, 64.7, 63.9, 69.3, 67.9, 65.2,
                  70.8, 70.5, 69.3, 75.3, 75.5, 78.2, 78. , 73.2, 78. , 80.1, 78.2,
                  76. , 81.5, 79.4, 81.8, 81.8, 84.1]               REEMPLAZAR 
                                                                     POR IMAGEN
           alturas = [149. , 149. , 149.9, 156.8, 150.6, 155.4, 151. , 162. , 165.,
                  157.8, 164.4, 160.1, 160.8, 163.8, 175.2, 162. , 159.5, 159.2,
                  169.8, 166.7, 179.4, 180.6, 163.3, 178.8, 176.3, 184.8, 181. ,
                  170.5, 184.1, 187.1, 187.1, 177.7, 184.5, 190.3, 196. , 192.1,
                  200.4, 201.8, 187.5, 202.1, 200.3, 208.8, 204.6, 193.5, 200.9,
                  196.8, 213.1, 204.8, 215.5, 210.2] 
              Ejemplo
Como era de esperar, se observa una fuerte relación positiva entre 
el peso y la altura
                       REEMPLAZAR 
                       POR IMAGEN
Algunas observaciones
✓ Para poder visualizar la relación, los valores de los arreglos 
que se emparejan deben guardar correspondencia 
entre sí. El peso de la primera persona debe estar junto 
con la altura de la misma persona.
✓ El parámetro alpha permite cambiar la transparencia de los 
puntos. Muy útil cuando graficamos muchos puntos.
1. alpha = 1                   puntos sólidos
2. alpha = 0.01             puntos casi transparentes
Probemos el Data Frame de 
precipitaciones
✓ ¿Guardarán algún tipo de relación las precipitaciones de 
Agosto respecto de las de septiembre? ������
fig, ax = plt.subplots()  
mapeo_colores = ax.scatter(df_lluvias['Aug'], df_lluvias['Sep'], c=df_lluvias.index)
fig.colorbar(mapeo_colores)
ax.set_title('Precipitaciones Agosto-Septiembre')
ax.set_xlabel('Precipitaciones en Agosto (mm.)')
ax.set_ylabel('Precipitaciones en Septiembre (mm.)')
Probemos el 
Data Frame de 
precipitaciones                    REEMPLAZAR 
                                POR IMAGEN
La relación en este caso es débil
✓ Es posible asignar un rango de colores a los 
puntos con el parámetro c y fig.colorbar
✓ También se puede asignar un rango de 
tamaño con el parámetro s
Gráfico de barras
Gráficos de 
barras
✓ Un diagrama de barras muestra datos                       REEMPLAZAR 
   categóricos como barras rectangulares                     POR IMAGEN
   con alturas proporcionales al valor que 
   representan. 
✓ Una variable categórica no es más que 
   una agrupación de datos en diferentes 
   grupos lógicos, por ejemplo, datos sobre la 
   altura de las personas que se agrupan 
   como "Alto", "Medio", "Bajo", etc.
Gráficos de barras
✓ Permiten comparar y poner en perspectiva los valores de 
distintas variables categóricas. Por ejemplo, las 
precipitaciones según el mes del año. 
✓ Para el ejemplo, acumulemos las precipitaciones para los 
distintos meses a lo largo de los años.
precipitaciones_acumuladas = df_lluvias.sum()
precipitaciones_acumuladas
Ejemplo
fig, ax = plt.subplots(figsize=(8,4))
precipitaciones_acumuladas = df_lluvias.sum()
ax.bar(df_lluvias.columns, precipitaciones_acumuladas)
ax.set_title('Precipitaciones acumuladas desde 1965 a 2002, según el mes')
ax.set_ylabel('Precipitación total (mm.)')
ax.set_xlabel('Mes')
CoderTip
El eje x representa categorías. La altura de cada barra 
en el eje y representa la cantidad de elementos para 
la categoría correspondiente.
Se grafican con ax.bar,que recibe como 
parámetros:
✓ Las etiquetas para el eje x
✓ La altura (frecuencia) de la barra para cada 
etiqueta
Histograma
Histograma
✓ Un histograma es una gráfica de la 
    distribución de frecuencia de una 
    matriz numérica dividiéndola en                                REEMPLAZAR 
    pequeños contenedores de igual tamaño                           POR IMAGEN
    (bins).
✓ Los bins usualmente se calculan por 
    medio de la fórmula de sturges:
Histograma
✓ La altura de cada barra representa la proporción o 
cantidad de los distintos valores de una variable 
numérica.
✓ Requiere clasificar a los datos en intervalos de clase.
✓ Permiten comparar la frecuencia relativa o absoluta de 
cada intervalo.
✓ Se construyen con ax.hist, que recibe como parámetro:
✓ El arreglo de valores. 
✓ bins, que representa la cantidad de intervalos a 
 construir.
Ejemplo
Aplanemos los valores del Data Frame con el método flatten
df_lluvias.values.flatten()
fig, ax = plt.subplots(figsize=(8, 4))
ax.hist(df_lluvias.values.flatten(), bins=10)
ax.set_title('Histograma de precipitaciones')
ax.set_xlabel('Intervalos de precipitaciones (mm.)')
ax.set_ylabel('Frecuencia absoluta')
Algunas observaciones
✓ La forma del histograma depende del número de intervalos 
de clase que pasemos al parámetro bins.
✓ En el ejemplo se representó la frecuencia absoluta de los 
intervalos.
✓ También se puede representar la frecuencia relativa de 
cada intervalo o el porcentaje respecto del total.
Boxplot
Boxplot
 ✓ El diagrama de caja es un método para 
    mostrar gráficamente la distribución de 
    una variable numérica a través de los                            REEMPLAZAR 
    cuartiles.                                                       POR IMAGEN
 ✓ Permite identificar la presencia de 
    atípicos y outliers
 ✓ También permite comparar la distribución 
    de una variable númerica entre 
    categorías
Ejemplo
✓ Creemos un boxplot de precipitaciones mensuales
fig, ax = plt.subplots(figsize=(8, 4))
ax.boxplot(df_lluvias.T)
ax.set_title('Boxplot de precipitaciones')
ax.set_xlabel('Meses')
ax.set_ylabel('Precipitacione (mm)')
Piechart
Piechart
✓ Un Piechart solo puede mostrar una 
    serie de datos. 
✓ Los gráficos circulares muestran el                              REEMPLAZAR 
    tamaño de los elementos denominados                             POR IMAGEN
    cuñas (wedges) en una serie de datos, 
    proporcional a la suma de los elementos.
✓  Los puntos de datos en un gráfico 
    circular se muestran como un porcentaje 
    del pastel completo.
Ejemplo
✓ Creemos un piechart de precipitaciones mensuales
cars = ['AUDI', 'BMW', 'FORD',
     'TESLA', 'JAGUAR', 'MERCEDES']
data = [23, 17, 35, 29, 12, 41]
fig,ax = plt.subplots(figsize =(10, 7))
ax.pie(data, labels = cars)
Enriqueciendo las 
visualizaciones
Múltiples elementos
✓ En ocasiones necesitamos resaltar ciertas características 
de los datos. 
✓ Por ejemplo, ¿Qué pasa si quisiéramos resaltar el punto 
máximo en una serie de tiempo?
✓ ¿Podemos cargar al objeto ax con múltiples elementos 
para que los muestre todos juntos?
Ejemplo en vivo
Utilizaremos en notebook Clase_07.ipynb 
(Ejemplo 2) con base en el archivo 
pune_1965_to_2002.csv  de precipitaciones 
que hemos usado antes, para comprender 
el proceso de enriquecimiento de 
visualizaciones con Matplotlib.
✓ Primero, comparemos las precipitaciones de Enero y Febrero en el 
  mismo objeto ax
fig, ax = plt.subplots(figsize=(12, 3))  
ax.plot(df_lluvias.index, df_lluvias['Jan'], label='Precipitaciones de enero')
ax.plot(df_lluvias.index, df_lluvias['Feb'], label='Precipitaciones de febrero', color='C1')
✓ Luego, Con una agregación, calculamos el máximo de cada uno
maximo_enero = df_lluvias['Jan'].max()
maximo_febrero =  df_lluvias['Feb'].max()
✓ El método axhline permite graficar líneas horizontales. Usemos 
  esto para resaltar los máximos de cada serie de tiempo
ax.axhline(maximo_enero, color='red', linestyle='--', alpha=0.5,    linewidth=3, 
label='Máxima de enero')
ax.axhline(maximo_febrero, color='red', linestyle=':', alpha=0.5, linewidth=3, 
label='Máxima de febrero')
✓ También se puede graficar líneas verticales con el método 
  axvline ������
✓ Por último, las etiquetas
ax.set_xlabel('Año')  
ax.set_ylabel('Precipitación (mm.)') 
ax.set_title('Precipitaciones de enero y febrero') 
ax.set_xlim(df_lluvias.index[0], df_lluvias.index[-1])
ax.legend() 
A tener en cuenta
Incluir muchos elementos puede 
entorpecer la lectura y comprensión 
de la figura. Procure no sobrecargarla.
Customizaciones 
sobre
gráficos
Personalizando 
Matplotlib
Personalizando Matplotlib
✓ Matplotlib permite modificar cada aspecto de sus gráficos 
por medio de parámetros.
✓ Al importar la librería, Matplotlib establece establece sus 
parámetros por defecto.
✓ Los parámetros se guardan en una estructura de datos de 
tipo dict. Se puede obtener una lista de los parámetros 
consultando sus claves.
mpl.rcParams.keys()
              Algunos parámetros 
                              comunes
          Parámetro               Descripción             Valor por defecto
    axes.grid                 Mostrar grilla           True
    axes.titleweight          Grosor tipografía        "normal"
                              título
    axes.titlelocation        Posición del título      "center"
    axes.grid.axis            Ejes de la grilla        "both"
    axes.labelcolor           Color de etiquetas       "black"
    axes.labelsize            Tamaño de fuente de      "large"
                              las etiquetas
              Algunos parámetros 
                             comunes
          Parámetro               Descripción            Valor por defecto
   axes.labelweight          Grosor de fuente de      "normal"
                             las etiquetas
   grid.alpha                Transparencia de grilla  1.0
   grid.color                Color de la grilla       "#b2b2b2"
   grid.linestyle            Estilo de grilla         "--"
   grid.linewidth            Grosor de la grilla      0.5
   legend.fontsize           Tamaño de fuente de la  "medium"
                             leyenda
Ejemplo
 mpl.rcParams['axes.titleweight'] = 'bold' 
 mpl.rcParams['axes.titlelocation'] = 'left' 
 mpl.rcParams['axes.titlecolor'] = 'firebrick' 
 mpl.rcParams['axes.labelcolor'] = 'blue' 
 mpl.rcParams['axes.labelsize'] = '10' 
 mpl.rcParams['axes.labelweight'] = 'light' 
 mpl.rcParams['axes.linewidth'] = '1' 
 mpl.rcParams['grid.color'] = 'black' 
 mpl.rcParams['grid.linestyle'] = '-.' 
 mpl.rcParams['grid.linewidth'] = '2' 
Ejemplo
 fig, ax = plt.subplots(figsize=(7, 4))  
 ax.scatter(df_lluvias['Aug'], df_lluvias['Sep'], c=df_lluvias.index)
 ax.set_title('(Título rojo en negrita)')
 ax.set_xlabel('(Etiqueta eje horiz.)')
 ax.set_ylabel('(Etiqueta eje vert.)')
A tener en cuenta
Matplotlib no es la única librería que 
nos permite generar visualizaciones 
en Python. Tenemos otras librerías para 
generar gráficos estáticos (e.g Seaborn) o 
dinámicos (Bokeh, Plotly, entre otras)
La librería Seaborn
Seaborn
Extendiendo las capacidades de Matplotlib
✓ Funciona por encima de matplotlib. 
✓ Se integra muy bien con las estructuras de datos de 
Pandas.
✓ Provee métodos que facilitan la generación de gráficos 
para la comparación de variables categóricas.
✓ Provee sus propios estilos y colores (muy estéticos, por 
cierto ������).
Cuestiones a considerar
✓ Seaborn tiene una sintaxis diferente a matplotlib, 
por lo que sólo lo aprovecharemos por su punto fuerte: 
las visualizaciones de variables categóricas.
✓ Como Seaborn se construye sobre matplotlib, puede 
actualizar los parámetros de matplotlib con los estilos 
de Seaborn y seguir graficando normalmente.
sns.set()
Hands on lab
Creación de gráficos con Matplotlib
¿De qué manera?
El profesor demostrará cómo hacerlo y tú lo puedes 
ir replicando en tu computadora. Si surgen dudas las 
puedes compartir para resolverlas en conjunto de la 
mano de los tutores.
Tiempo estimado: 25 - 30 minutos
HANDS ON LAB
Creación de gráficos con 
Matplotlib
1. Escoger un dataset de los elegidos para la Clase 5
2. Cargar el archivo usando la función pd.read_csv() o pd.read_excel()
3. Elegir dos gráficos apropiados para el análisis (lineplot, scatterplot, barras, 
histograma, boxplot)
4. Realizar  los  gráficos  seleccionados  utilizando  la  interfaz  orientada  a 
objetos y a estados.
             Trabajaremos individualmente. 
            Tiempo estimado: 15 minutos.
CLASE N°7
Glosario
 Matplotlib: librería multiplataforma de          Gráfico de barras: ideal para representar 
 Python que permite generar gráficos              frecuencias absolutas o relativas de datos 
 interactuando con otras librerías como           categóricos 
 Pandas y Numpy. Bastante sencilla, de            Histograma: ideal para representar 
 licencia libre y con calidad relativamente 
 alta.                                            distribución de variables numéricas 
 Interfaces de Matplotlib: definen la             continuas (sesgo, asimetría, curtosis)
 forma en cómo interactuamos con los              Boxplot: ideal para mostrar distribución de 
 paneles gráficos. Existen dos tipos              variables numéricas, comparar 
 orientado a estados y orientado a objetos        comportamientos entre categorías e 
 (más recomendable por mejor operatividad)        identificar valores atípicos.
 Gráfico de líneas: ideal para representar        Piechart: ideal para mostrar proporciones 
 variaciones en el tiempo (e.g series de 
 tiempo)                                          de variables categóricas similar a gráfico de 
 Gráfico de puntos:ideal para la                  barras
 representación de relaciones bivariadas por 
 medio de diagramas de dispersión
¿Preguntas?
           Resumen 
      de la clase hoy
      ✓ Visualizaciones en Python: Matplotlib 
      ✓ Gráficos comunes (Lineplots, Barplots, 
        Histogramas, Gráficos de Barras, Piecharts).
      ✓ Personalización de gráficos Matplotlib
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 20. DATA SCIENCE
 Stack Tecnológico I
Temario
             19                    20                    21
        Algoritmos y             Stack                 Stack 
        validación de        tecnológico I         tecnológico II
       modelos de ML
                                                  ✓ Sistema In - 
        ✓ Conceptos           ✓ Bases de             House
           básicos               datos            ✓
                                                     Cloud 
        ✓ Aprendizaje         ✓ Lenguajes            Computing
           y Validación          DS               ✓
                                                     Fundamentos 
        ✓ Métricas  y         ✓ Visualización        del Big Data 
           evaluación            de datos         ✓
                                                     ETL
Objetivos de la clase
         Clasificar las principales herramientas para un 
         Científico de Datos y sus características.
MAPA DE CONCEPTOS
Stack             Bases de            Relacionales
Tecnológico I       datos               No 
                                     relacionales
                                     R
                 Lenguajes           Python
                 Data Science
                                     Otras 
                                     herramienta
                                     s
                                     Power BI
                 Visualización
                                     Tableau
Cuestionario de tarea
¿Te gustaría comprobar tus 
conocimientos de la clase anterior?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
¡Empecemos!
Un científico de datos es un profesional 
dedicado exclusivamente en analizar e 
interpretar grandes bases de datos. Para 
ello, debe aprender a utilizar 
múltiples herramientas que 
estaremos clasificando a lo largo de 
esta sesión. 
Bases de Datos
PARA RECORDAR
Bases de Datos
Una Base de Datos es un conjunto de 
datos pertenecientes a un mismo 
contexto y almacenados 
sistemáticamente para su posterior 
uso.
En términos generales, podemos dividirlas 
en:
✓Bases de Datos Relacionales
✓Base de Datos no Relacionales – No 
SQL
Tecnologías de 
bases de datos 
relacionales
BD relacionales
Responden al Modelo de Datos relacional 
propuesto por Edward Frank Codd en 
1970, tal cual como podemos observar en la 
siguiente imagen:
Microsoft SQL Server
Es un sistema de gestión de bases de 
datos relacionales (RDBMS) que admite 
una amplia variedad de aplicaciones de 
procesamiento de transacciones, 
inteligencia empresarial y análisis en 
entornos informáticos corporativos. 
MySQL
Es un sistema de gestión de bases de 
datos relacional, desarrollado bajo una 
licencia dual: Licencia pública general / 
Licencia Comercial por Oracle 
Corporation. A su vez también, es una de 
las bases de datos más populares en 
general junto a Oracle y SQL Server. 
PostgreSQL
Es un sistema gestor de bases de datos 
relacionales, orientado a objetos, 
multiplataforma y open source. Está 
desarrollado desde 1996 por la 
comunidad de SGBD POSGRES.
Otras: Oracle Database, IMB DB2, Access, 
SQL Cloud.
Tecnologías de 
bases de datos no 
relacionales
BD no 
relacionales
Modelo propuesto por Carlo Strozzi en 
1998, como una base de datos "relacional" de 
código abierto y liviana que no usa SQL, 
desarrollado en principio para datos web (no 
estructurados) y por la necesidad de un 
procesamiento más rápido. 
NoSQL
Los datos masivos, reciben el nombre de 
Big Data, y el tipo de tecnología que ha 
surgido para tratar de poner solución a 
muchos de estos problemas se conoce 
como NoSQL.
Los sistemas NoSQL no solo pueden 
manejar datos estructurados y no 
estructurados, sino que también pueden 
procesar Big Data no estructurado 
rápidamente
  Internet en 1 minuto - Big 
                     Data
Fuente: statista.com
¿Por qué usar bases de 
datos NoSQL?
������ Evitar la complejidad innecesaria
������ Conseguir un alto rendimiento
������Escalabilidad horizontal y hardware de 
bajo costo
������ Transformar la famosa frase: “One size 
fit’s it all”
  Ejemplo de consulta SQL
   SELECT p.FirstName, p.LastName, a.City, cd.Detail
   FROM Person p
   JOIN ContactDetail cd ON cd.PersonId = p.Id
   JOIN ContactDetailType cdt ON cdt.Id = cd.TypeId
   JOIN Address a ON a.PersonId = p.Id
     Ejemplo de consulta no 
       {                 SQL
          “Id”: “1”,
          “firstName”: “Thomas”,
          “lastName”: “Andersen”,
          “addresses”:  [
              {
                 “line1”: “100 Some Street”,
                 “line2”: “Unit 1”,
                 “city”: “Seattle”,
                 “state”: “WA”,
                 “zip”: “98012”
              }
          ],
          “contactDetails”: [
              {“email”: “thomas@anderson.com”},
              {“phone”: “*1 555 555-5555”, “extension”: 5555},
          ]
       }
Algunas bases NoSQL
MongoDB es una base de datos                    Apache Cassandra se trata de un software 
orientada a documentos. Esto quiere             NoSQL distribuido y basado en un modelo de 
decir que en lugar de guardar los datos en      almacenamiento de «clave-valor», de 
registros, guarda los datos en                  código abierto que está escrita en Java. 
documentos. Estos documentos son                Permite grandes volúmenes de datos en 
almacenados en BSON, que es una                 forma distribuida. 
representación binaria de JSON.
Algunas bases NoSQL
Redis es un motor de base de datos en        Neo4j es una base de datos open-source 
memoria, basado en el almacenamiento         orientada a grafos escrita en java. Con 
en tablas de hashes (clave/valor) pero       este tipo de base de datos NO SQL puedo 
que opcionalmente puede ser usada            guardar información en formato de nodos y 
como una base de datos durable o             relacionales.
persistente.                                 Otras: Hbase, CouchDB, No Sql Cloud 
Ventajas y 
desventajas bases 
 SQL
                  Ventajas                                        Desventajas
Simplicidad del modelo: Muy simple, no             Mantenimiento: difícil por acumulación de 
requiere consultas complejas                       datos en el tiempo
Fácil uso: usuarios pueden acceder/recuperar       Costo: se generan costos fijos y variables por 
fácilmente la información requerida en segundos    mantenimiento
sin caer en la complejidad.
Precisión: bien definidas y organizadas, no        Almacenamiento físico: requiere mucha 
duplicados.                                        memoria física.
Integridad de datos: brindan coherencia en         Poca escalabilidad: los datos no son 
todas las tablas.                                  escalables en diferentes servidores de 
                                                  almacenamiento físico
Normalización: se divide la información en         Estructura compleja: solo puede almacenar 
partes manejables para reducir el tamaño del       datos en forma tabular, dificultando 
almacenamiento                                     representación compleja.
Colaboración: muchos usuarios interactuando        Reducción de performance en tiempo: 
al tiempo                                          mayor complejidad
Integridad y Seguridad: Sistemas                   Menor tiempo de respuesta: muchos datos 
medianamente confiables                            poca eficiencia
                   Ventajas                                         Desventajas
Modelo flexible: puede almacenar y combinar         Falta de estandarización: No existe un 
cualquier tipo de datos, tanto estructurados        estándar que defina reglas y roles de las bases 
como no estructurados                               de datos NoSQL. 
Modelo de datos en evolución: permite               Algunos problemas de backup: No está del 
actualizar dinámicamente el esquema para            todo desarrollado este ámbito en este tipo de 
evolucionar con los requisitos cambiantes sin       bases de datos.
interrupciones.
Fácil escalamiento: pueden escalar para             Consistencia: NoSQL prioriza la escalabilidad y 
adaptarse a cualquier tipo de crecimiento de        el rendimiento, pero cuando se trata de la 
datos manteniendo un bajo costo.                    consistencia de los datos no es tan eficiente.
Alto performance: gran rendimiento, medido          Difícil mantenimiento: pueden llegar a ser 
en términos de rendimiento y latencia (retraso      costosos y requerir de personal especializado
entre la solicitud y la respuesta real).
Acceso libre: no requieren tarifas de licencia      Poco nivel de madurez: Son relativamente 
costosas y pueden ejecutarse en hardware            más nuevas que las bases relacionales por ende 
económico, lo que hace que su implementación        tienen todavía mucho por mejorar.
sea rentable.
Para pensar
¿Qué casos de implementación en la 
industria conoces o has escuchado hablar, 
tanto de SQL como de NoSQL en 
empresas?
Escribe en el chat de Zoom
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Lenguajes
Data Science
    R
R
Entorno y lenguaje de programación con 
un enfoque al análisis estadístico. Se trata 
de uno de los lenguajes de programación 
más utilizados en investigación científica. 
R proporciona una amplia variedad 
de técnicas estadísticas (modelos 
lineales y no lineales, pruebas 
estadísticas clásicas, análisis de series 
temporales, clasificación, agrupamiento, 
etc), generación de gráficos y es 
altamente extensible. 
¿Cómo se visualiza R?
¿Un poco rústico verdad?
La realidad, es que la interfaz gráfica de 
R, no es realmente muy atractiva e 
intuitiva ������ 
Es por ello, que para nuestras clases 
prácticas trabajaremos con RStudio, 
el IDE de R. 
Entorno de 
desarrollo integrado
¿Qué es un IDE?
Un entorno de desarrollo integrado o 
entorno de desarrollo interactivo, en 
inglés Integrated Development 
Environment, es una aplicación 
informática que proporciona 
servicios integrales para facilitarle al 
desarrollador o programador el 
desarrollo de software.
En este caso en particular RStudio, nos 
brindará una interfaz mucho más cómoda 
y amigable para trabajar con R.
Beneficios de RStudio
Las más importantes a mencionar son:
������ Autocompletado. 
������ Reconocimiento de sintaxis de 
programación.
������  Depurador de errores. 
������ Manual de usuarios y ayuda en línea.
       ¿Cómo se visualiza 
                RStudio?
Link de descarga: https://rstudio.com/products/rstudio/download/
Python
Python
Lenguaje de programación poderoso 
y fácil de aprender. Puede ser 
clasificado como un lenguaje interpretado 
(ejecuta las instrucciones a medida que 
las va leyendo) y de alto nivel. Python fue 
creado a finales de los años 80, por un 
programador holandés llamado Guido van 
Rossum, quien sigue siendo aún hoy el 
líder del desarrollo del lenguaje.
R vs Python
   Características                             R                           Python
   Alcance                         (Principalmente) Análisis       Diferentes propósitos: 
                                     de datos y modelado         desarrollo de aplicaciones 
                                          estadístico              web, ciencia de datos
   Usuarios                              Estadísticos,                Desarrolladores, 
                                  Investigadores, Analistas y    Ingenieros y Científicos de 
                                     Científicos de Datos.                 Datos.
   Flexibilidad                   Librerías disponibles fáciles     Fácil para construir 
                                           de usar.              nuevos algoritmos desde 
                                                                          el inicio.
   Paquetes esenciales             Tydyverse, caret, ggplot2       Numpy, pandas, scipy, 
                                                                         scikitlearn 
   Herramientas de                  Ggplot2, plotly, ggmap       Matplotlib, ploty, seaborn
   visualización
Otros lenguajes
Julia
Lenguaje de programación novedoso 
creado en 2009. Proporciona DataFrames.jl 
para trabajar con conjuntos de datos y realizar 
manipulaciones de datos comunes. Además de 
trabajar con datos tabulares, los paquetes de 
JuliaGraphs facilitan el trabajo con datos.
Julia puede trabajar con casi todas las bases 
de datos utilizando los controladores JDBC.jl y 
ODBC.jl. Se integra fácilmente con Spark y 
Python.
JavaScript
JavaScript también es otro lenguaje de 
programación de ciencia de datos popular 
para aprender. Se utiliza para el 
desarrollo web por su capacidad de 
construir páginas web interactivas. 
Puede ser la mejor opción para crear y 
diseñar visualizaciones. Aunque es un 
gran lenguaje para aprender, es más 
útil en ciencia de datos para tareas 
esenciales como visualizaciones pero 
no tanto para modelamiento.
Scala
Scala es un poderoso lenguaje de 
programación para ciencia de datos, 
adecuado para profesionales. Ideal para 
trabajar con conjuntos de datos de gran 
volumen. Permite la interoperabilidad con 
Java, lo que brinda muchas oportunidades. 
Scala también se puede usar con Spark 
para manejar grandes cantidades de datos 
en silos. Este lenguaje de programación de 
ciencia de datos también tiene una gran 
cantidad de librerías.
Visualización de
datos
          Herramientas de visualización de 
          datos
 Dash    Shiny   Power   Pentaho Tableau
                  BI
Dash
Framework de Python creado por plotly                              REEMPLAZAR 
para crear aplicaciones web interactivas.                          POR IMAGEN
Código de Flask, Plotly.js y React.jsy sin 
tener que aprender HTML, CSS y 
Javascript. Dash es de código abierto y 
la creación de la aplicación utilizando un 
framework se ve en el navegador web.
Shiny
Shiny es un paquete de R que facilita la                                REEMPLAZAR 
creación de aplicaciones web                                             POR IMAGEN
interactivas desde R. Puede alojar 
aplicaciones independientes en una 
página web o incrustarlas en 
documentos de R Markdown o crear 
paneles. Permite el uso de temas CSS, 
widgets html y acciones de JavaScript.
Ejemplos reales
Ejemplos reales
Power BI
Conjunto  de herramientas y                                      REEMPLAZAR 
servicios de business intelligence,                              POR IMAGEN
que permite conectarse a diferentes 
orígenes de datos, para ser analizados, 
visualizarlos y compartirlos con toda la 
organización y clientes. Se compone de 
varias aplicaciones y servicios (Versión 
desktop, mobile y el servidor).
Ejemplo
Tableau
Herramienta de visualización de datos                        REEMPLAZAR 
potente también utilizada en el área de                       POR IMAGEN
la Inteligencia de negocios. La esencia 
de Tableau es simple y a la vez muy 
relevante: ayudar a las personas y empresas 
a ver y comprender todos sus datos.
Tableau
Tableau funciona a través de 3 medios         Además integra otras herramientas 
principales:                                  adicionales para proporcionar una 
✓ Escritorio (Tableau Desktop)                experiencia más completa a los usuarios:
✓                                              ✓ Tableau Mobile.
  Servidor (Tableau Server)
✓                                              ✓ Tableau Public.
  En línea (Tableau Online)
                                              ✓ Tableau Prep.
 Hands on lab
Realizaremos dos actividades de clase para comprender y 
llevar a la práctica los conceptos teóricos vistos. 
¿De qué manera?
En primera instancia, trabajaremos con visualizaciones de 
datos de manera individual. Luego, trabajaremos 
colaborativamente compartiendo los avances del 
Proyecto Final.
Tiempo estimado: 35/40 minutos
 Stack tecnológico 
del Data Scientist
     Duración: 15 minutos
ACTIVIDAD EN CLASE
Stack 
tecnológico del 
Data Scientist
Considerando los tipos de base de datos, lenguajes y 
opciones para la visualización escoge uno de los 
lenguajes/herramientas de interés.
1. Investigar sobre las ventajas /desventajas que 
ofrece. 
2. ¿Qué compañías usan el lenguaje/herramienta 
escogida?
Herramienta sugerida: Miró
ACTIVIDAD EN CLASE
Herramientas de 
visualización de datos
ACTIVIDAD EN CLASE
QlikView
“QlikView es una herramienta de Business Intelligence, 
que  permite  recolectar  datos  desde  diferentes 
orígenes,  basados  en  ERP,  CRM,  data  warehouses, 
bases de datos SQL, datos de Excel, etc.”
✓ Ofrece:
Servicio de Datos y Plataformas.
Modelado de Datos e Integración.
Búsqueda Asociativa.
     ACTIVIDAD EN CLASE
Otras 
herramientas ✓ MicroStrategy es una organización 
 ✓ “IBM Cognos Analytics, es una 
     suite de inteligencia empresarial               que provee software de reporteo, 
     integrada basada en web de IBM.                 análisis y monitoreo integrados que 
     Proporciona un conjunto de                      permite a las empresas analizar 
     herramientas para informes,                     datos almacenados de la empresa y 
     análisis, cuadros de mandos y                   de todas las áreas para tomar 
     seguimiento de eventos y                        mejores decisiones.
     métricas.”
  ACTIVIDAD EN CLASE
Otras 
herramientas
✓ Google Data Studio es una           ✓ QuickSight es un servicio rápido 
   herramienta de visualización de       de análisis de negocios basado 
   datos y creación de cuadros de        en la nube que facilita la creación 
   mando. El objetivo de la              de visualizaciones; la realización 
   herramienta, es permitir un           de análisis ad-hoc y la obtención 
   análisis de los datos de manera       rápidamente de información de 
   visual de forma que sea más           negocios basada en datos   .
   fácil e inmediato obtener 
   resultados.
    ACTIVIDAD EN CLASE
Herramientas 
complementarias
Existen muchísimas herramientas               ✓ Slack
orientadas a la gestión de                    ✓ Trello
comunicación, versionado de datos y           ✓ G - Suite o M365
trabajo en equipo que tenemos que             ✓ Mural
conocer como Científicos de Datos.            ✓ Jamboard
Algunas de las más importantes son:           ✓ GitHub, GitLab
                                              ✓ Zoom, Meet, Teams, Jitsi, etc.
Actividad colaborativa
Socialización de Proyectos
Avanzaremos resolviendo consultas 
asociadas al proyecto del curso mediado 
por tutores
Realizaremos la actividad en breakout 
rooms
Duración: 20 minutos
     ACTIVIDAD COLABORATIVA
Acuerdos
Presencia                                       Apertura al aprendizaje
✓ Participar y “estar” en la clase, que          ✓ Siempre, pero siempre puedes 
    tu alrededor no te distraiga                    seguir aprendiendo. Compartir el 
                                                    conocimiento es válido, la 
Escucha activa                                       construcción colaborativa es la 
                                                    propuesta.
✓ Escuchar más allá de lo que la 
    persona está expresando 
    directamente                               Todas las voces
                                                 ✓ Escuchar a todos, todos podemos 
                                                    reflexionar. Dejar el espacio para 
                                                    que todos podamos participar.
    ACTIVIDAD COLABORATIVA
Socialización de 
proyectos                                     ✓
En esta actividad colaborativa                   Identificar fortalezas y debilidades 
interactuamos con compañeros y tutores           que pueden servir como punto de 
con eje central el proyecto del curso.           mejora para la entrega del proyecto.
✓ Se propone resolver consultas              ✓ Se propone que los estudiantes 
    metodológicas o de forma acerca del         muestren los avances a los tutores y 
    proyecto final.                             compañeros para recibir 
                                                retroalimentación de su proyecto.
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
CLASE N°20
Glosario
Esquema relacional: desarrollado por               Lenguajes de data science : son 
Frank Codd en los años 70, es el                   aquellos que nos permiten generar 
fundamento de las bases de datos                   estructuras de código para realizar 
relacionales con prioridad a la                    algoritmos, limpiar, estructurar datos (e.g 
consistencia y disponibilidad de los datos         R, Python, Julia, Java, C++) 
Esquema no relacional : desarrollado               Herramientas de visualización: son 
por Carlo Strozzi en el 98, es el                  todas aquellas herramientas que permiten 
fundamento de las bases de datos no                generar visualizaciones en entornos 
relacionales con prioridad a la                    gráficos para la presentación de 
disponibilidad y tolerancia a la partición         resultados (e.g PowerBI, Shiny, Dash, 
de datos.                                          Pentaho, Tableau)
 Primera entrega
En la clase que viene se presentará la consigna de la primera 
parte del Proyecto final,  que nuclea temas vistos entre las 
       clases 1 y 20. 
Recuerda que tendrás 7 días para subirla en la plataforma.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Bases de Datos Relacionales y No Relacionales 
      ✓ Lenguajes de Data Science 
      ✓ Visualización de Datos 
      ✓ Herramientas Complementarias.
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 11. DATA SCIENCE
 Preprocesamiento 
  estadístico de los 
         datos
Temario
               10                      11                     12
         Herramientas           Procesamiento          Introducción al 
       de visualización        estadísticos de             análisis 
                                   los datos            predictivo con 
                                                          regresión
         ✓ Introducción          ✓ Procesamiento         ✓ Nociones 
         ✓                          como concepto           básicas 
            Gráficos Univariados
         ✓ Gráficos Bivariados   ✓ Outliers              ✓ Aplicaciones 
                                 ✓ Datos ausentes           prácticas
         ✓ Forma de los datos
                                 ✓                       ✓ Interpretación
         ✓ Gráficos de dos          Reducción de 
            dimensiones             dimensionalidad      ✓ Usar el 
                                                            modelo
Objetivos de la clase
         Conocer el concepto de preprocesamiento de 
         datos.
         Identificar y tratar outliers.
         Comprender la importancia de graficar los 
         datos.
         Introducir el análisis de datos ausentes.
         Incorporar las componentes principales.
MAPA DE CONCEPTOS
                                   Concepto
             Detección y 
            tratamiento de       En una variable
              outliers
                                    En dos 
                                  dimensiones
                                  Detectar e 
                                  interpretar
Preprocesamient
o de datos                       Graficar los datos
            Datos ausentes
                                  Mundo real     ¿Qué hacemos?
            Componentes 
             Principales
¡Vamos a Kahoot!
El preprocesamiento 
como concepto
Garbage In, Garbage Out
������ En general, los datos no están 
preparados para ser analizados o para ser 
entrada o input de algoritmos.
������ Existe una serie de actividades que 
ayudan a “pulir” los datos de entrada y 
¡prepararlos para que sirvan para el 
proceso de Data Science!
Definición
El preprocesamiento de datos es una             ¿Por qué es necesario?
tarea vital. Constituye una de las técnicas       ✓ Datos incompletos 
de Data Mining. Consiste en transformar           ✓ Ruido como outliers y errores
la data cruda (raw data) a un                     ✓ Inconsistencia, diferentes 
esquema más entendible, útil y                        códigos y nombres
eficiente.
Tareas para 
preprocesamiento
Tareas
Data Cleaning: También se conoce como          Data reduction: Se pueden reducir la 
scrubbing (depuramiento), consiste en          cantidad de instancias y features 
rellenar valores nulos, suavizamiento y        (variables) 
remoción de datos con ruido y outliers         Data Discretization: Se considera 
con inconsistencias                            parte de Data reduction. Valores 
Data Integration: integración de datos         categóricos se pueden reemplazar por 
con múltiples bases de datos (relacional y     números con técnicas como One Hot 
no relacional), cubos de datos, etc. Los       Encoding o LabelEncoder
datos pueden ser estructurados, semi-
estructurados y no estructurados.
Data Transformation: Consiste en la 
normalización y agregación de acuerdo 
con las necesidades
 One Hot Encoding            LabelEncoder
Estas técnicas de Data Discretization las veremos con mayor profundidad en la 
Clase 15
Detección de 
outliers. Importancia 
de análisis
Definición
Por definición un outlier (valor atípico) es:           Dos tipos de valor atípico:
una observación que se encuentra a                        ✓ Univariados: se pueden encontrar 
una distancia anormal de otros valores                        al observar una distribución de 
en una muestra aleatoria de una                               valores en un solo espacio de 
población                                                     características. 
                                                          ✓ Multivariados: se pueden 
                                                              encontrar en un espacio de n 
                                                              dimensiones (de n características). 
Definición
Otra clasificación puede ser:                 ✓ Outliers puntuales: datos puntuales 
                                                 lejos de la distribución de los datos
                                             ✓ Outliers contextuales: pueden ser ruido 
                                                 en los datos (e.g símbolos de 
                                                 puntuación)
                                             ✓ Outliers colectivos: pueden ser 
                                                 subconjuntos de novedades en los 
                                                 datos, como una señal que puede 
                                                 indicar el descubrimiento de nuevos 
                                                 fenómenos. 
¿Por qué ocurren?
✓ Errores de entrada de datos (errores        ✓ Errores de procesamiento de datos 
    humanos)                                     (manipulación de datos o mutaciones 
✓ Errores de medición (errores del               no deseadas del conjunto de datos)
    instrumento)                              ✓ Errores de muestreo (extracción o 
✓ Errores experimentales (extracción de          mezcla de datos de fuentes incorrectas 
    datos o errores de planificación /           o diversas)
    ejecución de experimentos)                ✓ Natural (no es un error, novedades en 
✓ Intencional (valores atípicos ficticios        los datos)
    hechos para probar métodos de 
    detección)
Importancia de análisis
La detección de valores atípicos es de gran    En el aprendizaje automático y en cualquier 
importancia para casi cualquier disciplina     disciplina cuantitativa, la calidad de los 
cuantitativa (e.g física, economía, finanzas,  datos es tan o más importante que el 
aprendizaje automático, seguridad              mismo modelo de predicción o 
cibernética) con el fin de obtener insights    clasificación.
sobre cualquier fenómeno.
Outliers en una variable
Si existen outliers en una variable, podemos  A continuación, los desarrollaremos
verlos de diferentes formas: 
✓ Diagrama de caja y bigotes.
✓ Método IQR
✓ Método Z score
✓ Distancia media (Multivariada)
Boxplot
                         ¡Los bigotes marcan los valores de 
                         los inliers más extremos!
Elementos
                          La longitud de la caja es la diferencia 
                          entre los cuartiles 1 y 3 (llamados 
                          comúnmente Q1 y Q3), por eso se la 
                          denomina rango intercuartil. Todos los 
                          valores por fuera de esos límites son 
                          considerados outliers.
1.Separación entre inliers y 
   outliers                                        # Seleccionar las columnas de interés
                                                   import matplotlib.pyplot as plt
Habíamos dicho que se establecía una 
barrera que marcaba el límite entre inliers y      l=[x for x in agg_df.columns if x not in 
outliers, y que no aparece en el diagrama.         ['Date','Volume']]
                                                   plt.figure(figsize=(17,14))
Para ello, se toma el límite de la caja, y se      for x,column in zip(range(8),agg_df[l]):
le añade la longitud de una caja y media.           if column !='Symbol':
Esto se hace tanto para el límite superior de         #print(x)
la caja como para el inferior                         if x<=5:
                                                        plt.subplot(4,2,x+1)
                                                        sns.boxplot(data=agg_df[l], y=column, 
                                                   x=agg_df[l]['Symbol'])
Ejemplo
2.   Método IQR
datos= agg_df[l]
datos.head()
plt.figure(figsize=(15,14))  ������
for i,j in zip(range(8),datos.columns):
plt.subplot(4,2,i+1)
sns.boxplot(x=datos[j])
plt.title(j)
plt.xlabel('')
3.   Método Z Score
# Cargar datos
                                # Asignacion de categorias
datos= agg_df[agg_df['Symbol']=='D']
                                datos_z['Open_x']= datos_z['Open'].apply(lambda 
l=[x for x in agg_df.columns if x not in 
                                x: 'Atipico' if (x>2 or x<-2) else 'Normal')
['Volume','Symbol']]
                           ������   datos_z.head()
datos=datos[l]
                                # Plot
datos['Date']=pd.to_datetime(datos['Date'])
                                plt.figure(figsize=(10,6))
datos.head()
                                sns.scatterplot(x=datos_z.Fecha, y= 
# Convertir a z score
                                datos_z.Open, hue= datos_z['Open_x'],s= 5)
datos_z=pd.DataFrame()
for j in datos.columns[1:]:
datos_z[j] = (datos[j] - datos[j].mean()) / 
datos[j].std()
datos_z['Fecha']= datos.Date
datos_z.head()
Ejemplo Método Z 
Score
 Outliers en dos 
 dimensiones
 ✓ Puede parecer fácil encontrar               ✓ Esto puede funcionar adecuadamente 
    outliers en dos dimensiones:                  la mayoría de las veces,  pero existen 
    simplemente podrían graficarse                distribuciones de variables que 
    diagramas de caja y bigotes para              desafían este análisis.
    cada una de las dos variables. 
Ejemplo
              Si  vemos solamente los valores de la variable x, notamos 
              que  su  diagrama  de  caja  y  bigotes  horizontal  tiene  dos 
              outliers en su extremo derecho, que corresponden a los dos 
              puntos del gráfico ubicados en la parte inferior derecha. En 
              este caso, sería conveniente analizar estos dos puntos en 
              sus dos variables x e y para ver qué relación guardan con el 
              resto de los datos.
������ El código para este ejemplo en Python está disponible en este enlace.
Detectar e interpretar 
los outliers
                       Tratar de interpretar qué 
Identificar y aislar los puntos que 
                       representan esos outliers para el 
tienen cualidades de outliers
                       resto de los datos
  ������ ¡Esta última tarea es más subjetiva y requiere de un experto!
La Importancia de 
graficar los datos
No podemos dejar de destacar de la manera 
que sea posible de acuerdo a las 
características del caso en particular, para 
poder analizar sus relaciones y verificar 
si existen outliers o situaciones 
particulares.
                                Coeficiente de correlación para:
                                Dataset I: 0.81642051634484
                                Dataset II: 0.8162365060002428
                                Dataset III: 0.8162867394895984
Ejemplo Dataset IV: 0.8165214368885028
                                (Promedio de los valores de x, promedio de los valores de y) 
                                para:
Es claro que los cuatro          Dataset I: (9.0, 7.5)                   REEMPLAZAR 
casos muestran conjuntos         Dataset II: (9.0, 7.5)                  POR IMAGEN
de datos diferentes. ������          Dataset III: (9.0, 7.5)
Observemos las salidas de        Dataset IV: (9.0, 7.5)
Python para los cuatro 
datasets:                        (Varianza de los valores de x, varianza de los valores de y) 
                                para:
                                Dataset I: (11.0, 4.13)
                                Dataset II: (11.0, 4.13)
                                Dataset III: (11.0, 4.12)
                                Dataset IV: (11.0, 4.12)
“El cuarteto de 
anscombe”
Por extraño que parezca, los cuatro 
conjuntos de datos tienen casi el mismo 
coeficiente de correlación, el mismo 
promedio para x, el mismo promedio para 
y, y análogamente con las varianzas de 
ambas variables.
PARA RECORDAR
Para tener en cuenta
¡No estamos exentos de encontrarnos con 
casos similares, por lo que debemos estar 
atentos a los indicadores estadísticos 
utilizados, las distribuciones de los 
datos y los gráficos correspondientes, 
todo en conjunto!
4.    Método Distancia Media (Multivariada)
# Extraer columnas de interes
data_multiple=datos_z.drop(columns=['Open_x','Fecha'])
data_multiple.head()
# Funcion                                                       # Crear una copia de los datos
def outlier_euclideano_d(x,cutoff):                             euc_d = data_multiple.copy()
# x: dataframe con valors numericos normalizados         ������    euc_d.head()
result_ = pd.Series([0] * len(x.iloc[:,1]))                    # Aplicar la funcion
data_mean = x.mean() # media de los datos_                     euc_d['outlier']=outlier_euclideano_d(euc_d
dist = np.sqrt(np.sum(((x-data_mean) ** 2),axis=1))            ,2)
#Distancia euclideana                                           euc_d.head()
dist_mean = dist.mean() #media de las distancia
dist_zscore = np.abs((dist - dist_mean) / 
dist.std())#z-score para las distancias
result_[((dist_zscore > 3))] = 1
return result_
Ejemplo
plt.figure(figsize=(15,14))
plt.subplot(2,3,1)
sns.scatterplot(x="Open",y="High",data=euc_d,hue="outlier",p
alette=["green","red"])
plt.subplot(2,3,2)
sns.scatterplot(x="Open",y="Low",data=euc_d,hue="outlier",pa
lette=["green","red"])               ������
plt.subplot(2,3,3)
sns.scatterplot(x="Open",y="Close",data=euc_d,hue="outlier",
palette=["green","red"])
plt.subplot(2,3,4)
sns.scatterplot(x="Open",y="Adj 
Close",data=euc_d,hue="outlier",palette=["green","red"])
plt.subplot(2,3,5)
sns.scatterplot(x="Open",y="Volume_Millions",data=euc_d,hue=
"outlier",palette=["green","red"])
plt.subplot(2,3,6)
sns.scatterplot(x="Open",y="VolStat",data=euc_d,hue="outlier
",palette=["green","red"])
Outliers en una 
variable
También existen otros métodos más elaborados 
como: 
✓ Modelación probabilística
✓ Modelos de regresión
✓ Modelos basados en proximidad (no 
paramétricos)
✓ Modelos de Teoría de información y 
✓ Detección en espacios de alta dimensión 
Datos ausentes: 
imputación de datos, 
interpolación y otras 
técnicas
Datos ausentes
Datos ausentes y el mundo 
real
                  La tranquilidad de comenzar el proceso de 
                  análisis desde la parte estadística pura, es 
                  un poco diferente a lo que puede llegar a 
                  pasar en el mundo real...
Algunos ejemplos
 ✓ Cuando pedimos responder una encuesta 
     o   formulario,     puede  que  alguna                ✓ Puede ocurrir que se nos escape 
     pregunta quede sin responder.                             una tecla e ingresemos un número 
 ✓ Cuando  recolectamos  datos  de  manera                     mal.
     automatizada       o     semi-automatizada,         ¿Qué  hacemos  con  los  datos 
     puede  ser  que  los  datos  estén  mal             ausentes?
     cargados.
PARA RECORDAR
Para tener en cuenta
No todos los métodos y algoritmos brindan 
la posibilidad de trabajar con ellos. 
Afortunadamente, con Python tenemos 
gran parte del problema solucionado ������
Métodos con datos                                      ...acá denominados NaN
✓ Las operaciones vectorizadas ofrecen 
ausentes
  funciones que descartan los valores     ✓ Si  podemos  comprobar  que  los 
  NaN,  tales  como  nansum,  nanprod,       valores  faltantes  no  serán 
  nanmean, etc.                              extremos, podemos asignar un 
✓ Si  efectivamente  queremos  quitar  la     valor  determinado  al  dato 
  observación  completa  (la  fila  de       ausente.
  datos  completa),  podemos  usar  la 
  función dropna().
Para pensar
¿En qué situaciones se puede asignar 
un valor determinado al dato ausente?
Contesta mediante el chat de Zoom 
Importante
Los datos ausentes son siempre 
importantes porque pueden introducir 
irregularidades que distorsionan los 
resultados del análisis. Siempre deben ser 
reportados y analizados para entender su 
origen y tratar de minimizarlos.
Métodos de 
imputación
Manejo de datos nulos
Se deben tener estrategias para poder manejarlos.       Para aplicar estas técnicas 
En general se tienen dos metodologías                   podemos hacerlo de forma manual 
                                                        o usando la clase SimpleImputer 
       1) Introducir un valor constante para los        de ScikitLearn
          nulos o una categoría llamada 
          Desconocido en variables categóricas
       2) Reemplazar por un valor seleccionado al 
          azar de los otros registros
       3) Usar la media, mediana o moda para 
          rellenar el valor
       4) Valor estimado usando un modelo 
Manejo de datos nulos: 
SimpleImputer
Si queremos reemplazar las columnas numéricas por media 
podemos hacer esto
url='https://raw.githubusercontent.com/              0      1     2     3      4     5      6   7    8
jbrownlee/Datasets/master/pima-indians-          0  6.0  148.0  72.0  35.0    NaN  33.6  0.627  50  1.0
diabetes.csv'                               ������   1  1.0   85.0  66.0  29.0    NaN  26.6  0.351  31  NaN
df= pd.read_csv(url,sep=',', header=None)        2  8.0  183.0  64.0   NaN    NaN  23.3  0.672  32  1.0
                                              3  1.0   89.0  66.0  23.0   94.0  28.1  0.167  21  NaN
print(df.shape)                                  4  NaN  137.0  40.0  35.0  168.0  43.1  2.288  33  1.0
df.replace(0, np.nan, inplace=True)
df.head()
Manejo de datos nulos: 
SimpleImputer
Si queremos reemplazar las columnas numéricas por media 
podemos hacer esto
# reemplazar con la mediana                                         0      1     2     3      4     5     6     7    8
valores = df.values #numpy array con los valores                0  6.0  148.0  72.0  35.0  125.0  33.6  0.63  50.0  1.0
                                                             1  1.0   85.0  66.0  29.0  125.0  26.6  0.35  31.0  1.0
imputador = SimpleImputer(missing_values=np.nan,         ������     2  8.0  183.0  64.0  29.0  125.0  23.3  0.67  32.0  1.0
strategy='median') #definir el imputador                        3  1.0   89.0  66.0  23.0   94.0  28.1  0.17  21.0  1.0
# transformar el dataset                                        4  4.0  137.0  40.0  35.0  168.0  43.1  2.29  33.0  1.0
transformados = imputador.fit_transform(valores)
transformados=pd.DataFrame(transformados)
print(transformados.head().round(2))
Interpolación
Interpolación
✓ Método  de  ajustar  los  puntos  de  datos  para 
representar el valor de una función. 
✓ Tiene  varias  aplicaciones  en  ingeniería  y 
ciencia,  que  se  utilizan  para  construir  nuevos 
puntos  de  datos  dentro  del  rango  con  unos 
puntos conocidos
✓ Existen  diversos  métodos  asociados  (lineal, 
polinomial, splines)
Interpolación
Método lineal
url='https://raw.githubusercontent.com/
                                                      0      1     2     3      4     5     6   7    8
jbrownlee/Datasets/master/pima-indians-
                                                    0  6.0  148.0  72.0  35.0    NaN  33.6  0.63  50  1.0
diabetes.csv'
                                                    1  1.0   85.0  66.0  29.0    NaN  26.6  0.35  31  NaN
df= pd.read_csv(url,sep=',', header=None)          ������
                                                    2  8.0  183.0  64.0   NaN    NaN  23.3  0.67  32  1.0
print(df.shape)
                                                    3  1.0   89.0  66.0  23.0   94.0  28.1  0.17  21  NaN
df.replace(0, np.nan, inplace=True)
                                                    4  NaN  137.0  40.0  35.0  168.0  43.1  2.29  33  1.0
df.head()
print(df.interpolate(method="linear").head().rou            0      1     2     3      4     5     6   7    8
nd(2))                                             ������ 0  6.0  148.0  72.0  35.0    NaN  33.6  0.63  50  1.0
                                                    1  1.0   85.0  66.0  29.0    NaN  26.6  0.35  31  1.0
                                                    2  8.0  183.0  64.0  26.0    NaN  23.3  0.67  32  1.0
Esta forma no siempre garantiza rellenar los            3  1.0   89.0  66.0  23.0   94.0  28.1  0.17  21  1.0
valores nulos de todo el dataset                        4  3.0  137.0  40.0  35.0  168.0  43.1  2.29  33  1.0
Interpolación
Método lineal
url='https://raw.githubusercontent.com/
                                                      0      1     2     3      4     5     6   7    8
jbrownlee/Datasets/master/pima-indians-
                                                    0  6.0  148.0  72.0  35.0    NaN  33.6  0.63  50  1.0
diabetes.csv'
                                                    1  1.0   85.0  66.0  29.0    NaN  26.6  0.35  31  NaN
df= pd.read_csv(url,sep=',', header=None)          ������
                                                    2  8.0  183.0  64.0   NaN    NaN  23.3  0.67  32  1.0
print(df.shape)
                                                    3  1.0   89.0  66.0  23.0   94.0  28.1  0.17  21  NaN
df.replace(0, np.nan, inplace=True)
                                                    4  NaN  137.0  40.0  35.0  168.0  43.1  2.29  33  1.0
df.head()
print(df.interpolate(method="polynomial",order=2               0      1     2      3      4     5     6   7    8
).head().round(2))                                 ������ 0  6.00  148.0  72.0  35.00    NaN  33.6  0.63  50  1.0
                                                    1  1.00   85.0  66.0  29.00    NaN  26.6  0.35  31  1.0
Al igual que el método lineal no garantiza que         2  8.00  183.0  64.0  22.36    NaN  23.3  0.67  32  1.0
se llenen todos los valores.                           3  1.00   89.0  66.0  23.00   94.0  28.1  0.17  21  1.0
                                                    4  1.96  137.0  40.0  35.00  168.0  43.1  2.29  33  1.0
Existen otros métodos como: spline, nearest, 
krogh y Akima que pueden ayudar en algunos 
casos
Otras técnicas
✓ Eliminar filas con valores perdidos
✓ Imputar valores perdidos para variable continua
✓ Imputar valores perdidos para variable categórica
✓ Usar algoritmos que admiten valores perdidos
✓ Predicción de valores perdidos
✓ Imputación mediante la biblioteca de aprendizaje 
profundo - Datawig
Librería Datawig
Librería Datawig
df.columns= ['Col0','Col1','Col2','Col3','Col4','Col5','Col6','Col7','Col8']
import datawig
import pandas as pd
import datawig
df_train, df_test = datawig.utils.random_split(df)
#Inicializar el modelo SimpleImputer
imputer = datawig.SimpleImputer(
input_columns=['Col0','Col1','Col2'], # columnas que tienen la informacion con la columna a imputar
output_column= 'Col4', # columna que queremos imputar
output_path = 'imputer_model' # modelo y metricas
)
# Entrenar el modelo con data de entrenamiento y 50 epocas
imputer.fit(train_df=df_train, num_epochs=50)
# Imputar los missing values y devolver el dataframe original con la predicciones
imputed = imputer.predict(df_test)
print(imputed.head().round(2))
Librería Datawig
      Col0   Col1  Col2  Col3   Col4  Col5  Col6  Col7  Col8  Col4_imputed
  734   2.0  105.0  75.0   NaN    NaN  23.3  0.56    53   NaN        113.50
  213   NaN  140.0  65.0  26.0  130.0  42.6  0.43    24   1.0        175.10
  465   NaN  124.0  56.0  13.0  105.0  21.8  0.45    21   NaN        149.18
  206   8.0  196.0  76.0  29.0  280.0  37.5  0.60    57   1.0        168.51
  762   9.0   89.0  62.0   NaN    NaN  22.5  0.14    33   NaN        110.74
  542  10.0   90.0  85.0  32.0    NaN  34.9  0.82    56   1.0        108.02
  255   1.0  113.0  64.0  35.0    NaN  33.6  0.54    21   1.0        130.35
  412   1.0  143.0  84.0  23.0  310.0  42.4  1.08    22   NaN        146.46
  328   2.0  102.0  86.0  36.0  120.0  45.5  0.13    23   1.0        121.36
  583   8.0  100.0  76.0   NaN    NaN  38.7  0.19    42   NaN        131.35
  346   1.0  139.0  46.0  19.0   83.0  28.7  0.65    22   NaN        164.06
  427   1.0  181.0  64.0  30.0  180.0  34.1  0.33    38   1.0        215.98
  649   NaN  107.0  60.0  25.0    NaN  26.4  0.13    23   NaN        127.96
  757   NaN  123.0  72.0   NaN    NaN  36.3  0.26    52   1.0        155.13
  607   1.0   92.0  62.0  25.0   41.0  19.5  0.48    25   NaN         97.90
  421   2.0   94.0  68.0  18.0   76.0  26.0  0.56    21   NaN         98.36
  470   1.0  144.0  82.0  40.0    NaN  41.3  0.61    28   NaN        150.10
         ☕
       Break
       ¡10 minutos y 
        volvemos!
          ¡Lanzamos la
          Bolsa de 
          Empleos!
         Un espacio para seguir potenciando tu carrera y 
         que tengas más oportunidades de inserción 
         laboral.
         Podrás encontrar la Bolsa de Empleos en el menú 
         izquierdo de la plataforma.
         Te invitamos a conocerla y ¡postularte a tu futuro 
         trabajo!
           Conócela
Reducción de 
dimensionalidad con 
análisis de componentes 
principales
Análisis de 
componentes 
principales
¿Qué tal si pudiéramos 
simplificar la información 
que tenemos por delante?
¿Si pudiéramos “ver la sombra” de los 
datos de tal forma de poder acomodarlos 
en una “foto” de dos dimensiones y así 
poder verlos mejor?
     Veamos el siguiente 
               video
                            Acercándonos al 
                            concepto...
        REEMPLAZAR 
         POR IMAGEN
Ejemplo
Consideremos el conocido 
conjunto de datos Iris:
¿Cómo podríamos 
reducir tanta 
✓ Las componentes son nuevas variables, cada 
información?
una de las cuales surge de realizar un 
cálculo con todas las variables originales.
✓ Aplicar componentes principales es como 
encender una fuente de luz, que busca la 
sombra mejor proyectada por los datos ������.
En los gráficos 
                         Luego las componentes principales
siguientes pueden 
verse:
Primero las variables originales
                       ������
 ¿Qué hicimos?
 ✓ Simplemente        creamos      un     nuevo 
    conjunto de datos.                                 ✓ PC1       explicará    el     mayor 
                                                           porcentaje de los datos, la PC2 
 ✓ Estas  nuevas  variables  están  calculadas             un poco menos, y así en orden 
    de  tal  forma  que  cada  una  de  ellas              decreciente.
    puede explicar la variabilidad de los 
    datos      con     distinto    grado      de 
    importancia.
¿Cómo se mide esta 
importancia?
✓ PC1 explica el 92% de los datos
✓ Mientras que la PC2 explica el 5.30% y así sucesivamente. 
✓ Quiere decir que la PC1 y la PC2 juntas explican el 97.76% de los datos 
(estaríamos perdiendo solamente un 2.24% de información).
     “explained variance ratio” o tasa de variabilidad explicada
         PC1       PC2        PC3       PC4
         92.46     5.30       1.71      0.51
 ������ ¡Python provee todas estas funciones en el paquete scikit-learn!
Selección de 
variables
                      Tenemos información resumida de un 
                       conjunto de datos de 4 variables 
                        originales en 2 componentes 
                      principales que muestran claramente la 
                         separación de los datos en dos 
                         subconjuntos bien definidos. 
������ Es una selección de variables muy conveniente para graficar en dos dimensiones:
Algunas observaciones
 Las   componentes  principales  no  tienen            ✓ Utilizar       las      componentes 
 significado:  es  un  error  tratar  de  darles  una      principales con precaución.
 interpretación.
                                                       ✓ Si los datos no tienen una forma 
 Dos situaciones más comunes:                              muy           definida,        nos 
        ✓ Si  un  dato  es  outlier  en  las               encontraremos          en      una 
           componentes  principales,  también 
           será  outlier  en  el  conjunto  de             situación    similar    para    las 
           datos original.                                 componentes principales. 
        ✓ Si un conjunto de datos conforma un 
           grupo       separado,       también 
           conformará           un        grupo 
           diferenciado  en  las  variables 
           originales.
Actividad colaborativa
Análisis de correlación en acciones
Utilizaremos información de precios de 
acciones para calcular correlaciones 
Duración: 15/20 minutos
Realizaremos la actividad en grupos de 3-4 personas 
    ACTIVIDAD COLABORATIVA
Análisis de correlación 
en acciones
 Consigna: 
  1. Importar datos de Acciones Globales      3.   Calcular la matriz de correlación 
      (que están hosteados en GITHUB en       para       todas las acciones presentadas.
      el siguiente enlace Monitoreo de        4    Interpretar los resultados obtenidos
      Acciones
  2. Identificar potenciales valores 
      atípicos y posibles causas 
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
CLASE N°11
Glosario                                   Data Reduction: consiste en la 
Data Cleaning: proceso para remover        reducción de dimensionalidad de 
nulos, outliers e inconsistencias en un    variables para evitar la maldición de 
dataset                                    espacios de alta dimensión que traen 
Data Integration: proceso mediante el      problemas de multicolinealidad y 
cual se acoplan diversas fuentes de        estimadores con alta varianza
información (estructurada, no              Valores atípicos (outliers): son 
estructurada y semi estructurada)          observaciones que tienen un 
Data Transformation: consiste en la        comportamiento diferente a las demás 
adecuación de variables numéricas          dentro de un dataset, pueden ser 
(estandarización) o categóricas (one hot   univariados o multivariados 
encoding y LabelEncoder) para el manejo    Datos ausentes: valores nulos dentro de 
más eficiente de estructuras de datos      un dataset que pueden generar ruido en 
para modelos                               los análisis.

CLASE N°11
Glosario
Imputación: técnica que consiste en 
reemplazar datos ausentes por alguna 
medida representativa (media, mediana, 
moda) o por el valor correspondiente al 
individuo más cercano en distancia
Interpolación: técnica que consiste en 
reemplazar datos ausentes numéricos por 
medio de funciones matemáticas con 
límites superior e inferior definidos
ACP: técnica de reducción de 
dimensionalidad que permite encontrar 
grupos y proyectar en pocas dimensiones 
las relaciones entre individuos y variables 
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Concepto de preprocesamiento
      ✓ Outliers
      ✓ Datos ausentes
      ✓ Componentes principales
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 35. DATA SCIENCE 
Análisis univariado y 
       gráficos
Temario
                   34                         35                         36
                GIS y                      Análisis            Análisis bivariado
         Datos espaciales               univariado y 
                                           gráficos
          ✓ GIS y Datos               ✓ Tipos de análisis         ✓ Análisis 
              Espaciales                  estadísticos                bivariado
          ✓ Mapas y                   ✓ Gráficos                  ✓
              animaciones de              estadísticos                Pasos a seguir
              Python                                              ✓ Tipos
          ✓ Uso de la librería 
              Plotly
Objetivos de la clase
         Identificar las particularidades del Análisis 
         univariado de datos
         Reconocer ejemplos y aplicaciones de Python
MAPA DE CONCEPTOS                  Tipos
                 Análisis 
                univariado y 
                 gráficos         Gráficos 
                                 estadísticos
                                Pasos a seguir
Análisis          Análisis 
estadístico         bivariado        Tipos
                                 Objetivos
                 Análisis        Ventajas y 
                Multivariado     desventajas
                               Integración con R y 
                                  PowerBI
Tipos de Análisis 
Estadístico
Definición
¿Qué es el Análisis
estadístico?
 Es  la  ciencia  de  recopilar,  explorar  y 
 presentar grandes cantidades de datos para 
 descubrir     patrones      y     tendencias 
 subyacentes. 
 Las estadísticas se aplican todos los días, en 
 investigación,  industria  y  gobierno,  para 
 volverse    más     científicas  sobre    las 
 decisiones que deben tomarse. 
Tipos de datos 
análisis estadístico
Tipos de Análisis 
estadístico
  En  el  mundo  de  la  Estadística,  se 
  distinguen  tres  tipos  de  análisis 
  principales  según  el  número  de 
  variables     que       se     analicen 
  conjuntamente. 
  Podemos  destacar  el  análisis:   
  Univariado,         Bivariado          y 
  Multivariado.
   
  En  esta  clase,  nos  centraremos 
  exclusivamente      en     el   análisis 
  Univariado 
Análisis univariado
Consiste  en  el  análisis  de  cada  una  de  las 
variables estudiadas por separado, se basa 
exclusivamente en una única variable.
Es  el  análisis  más  básico  y  por  ende  más 
primario.  Se  considera  un  análisis  de  tipo 
Descriptivo y no relacional o de causalidad
Análisis univariado
Las técnicas más frecuentes de análisis 
univariado son la distribución de frecuencias 
para una tabla univariada y el análisis de las 
medidas de tendencia central de la variable 
(media, mediana, moda, varianza , desviación 
estándar, cuartiles entre otros). 
PARA RECORDAR
¿Cuándo uso el análisis univariado?
Lo podemos usar cuando deseamos entender desde la 
exploración un conjunto de datos. Por ejemplo, cuando 
decimos que una persona pesa 95 kg 
independientemente de otra variable estamos en el 
análisis univariado.
De igual forma cuando hablamos de estadística 
univariada cuando decimos que el 23% de las 
personas son de un color de piel específico.
Análisis univariado
Este tipo de análisis ha sido muy criticado por 
su carácter univariado, ya que la realidad se 
presenta interconectada y relacionada. 
Por ejemplo existe una relación entre el peso y 
la talla de las personas o entre el interés y el 
rendimiento escolar, etc. 
Análisis univariado
Entonces  como  la  realidad  es  compleja,  necesitamos  métodos  más  rigurosos  para 
analizarla.
De allí surge la necesidad de métodos bivariados o multivariados.
Medidas de 
tendencia central
Medidas de 
tendencia central
Las   medidas  de  tendencia  central  se 
representan  a  través  de  un  número  que 
permite entender la región central de un 
conjunto  de  valores  de  datos.  Las  tres 
medidas de tendencia central más utilizadas 
son la media, la mediana y la moda.
Medidas de 
tendencia central
                   En general la medida más común es la media. 
                   Sin  embargo,  esta  medida  tiene  varios 
                   problemas 
                   (como que se deja influenciar por atípicos y es 
                   poco  robusta).  Para  resolver  esto  existen 
                   medidas como la Mediana. También la Moda 
                   es una medida de lo más frecuente.
Medidas de 
tendencia central
                   Sin embargo existen otras medidas que 
                   permiten ponderar la importancia de las 
                   observaciones como por ejemplo la media 
                   ponderada. Otra medida robusta es la media 
                   o mediana recortada (removiendo un % de 
                   datos en la parte superior e inferior de los 
                   datos). También existen otras medidas como la 
                   media armónica y geometría con 
                   aplicaciones en música y finanzas.
Medidas de 
tendencia central
Imaginen que tenemos un conjunto de estaturas 
de  personas  y  deseamos  saber  cual  es  el  valor 
representativo  de  las  alturas,  en  este  caso 
podemos  hacer  uso  de  las  medidas  de 
tendencia central. Es importante recordar que 
cuando  hay  presencia  de  outliers  no  es 
conveniente  usar  la  media  sino  más  bien  la 
mediana  o  la  media  recortada  para  evitar 
problemas  de  interpretación.  Para  este  caso 
tenemos 160cm como valor representativo.
Medidas de 
Localización
Medidas de Localización
Las medidas de localización son números que 
permiten  comprender  la  distribución  de 
los datos y cantidad de datos por debajo 
de cierto umbral. La medida más conocida 
son los cuartiles, sin embargo, existen muchas 
otras como los deciles y percentiles.
Medidas de Localización
Los cuartiles representan la distribución de los 
datos en 4 secciones iguales, el segundo cuartil 
representa la mediana .
Un percentil representa a cada uno de los 100 
grupos  iguales  en  que  se  puede  dividir  una 
población según la distribución de una variable.
Medidas de Localización
 Como  ejemplo,  imaginemos  que  tenemos 
 información  de  las  compras  de  unos 
 individuos en dólares, lo cual se muestra en 
 la figura de la derecha. Al ordenar de menor a 
 mayor se nos solicita calcular el percentil 25, 
 es decir el valor en la distribución que deja 
 por debajo del 25% de los datos. Para este 
 caso  el  valor  es  10.5$.  Esto  nos  permite 
 después    comparar    cualquier   compra 
 sabiendo esta referencia.
Medidas de 
Dispersión
Medidas de Dispersión
Una medida de dispersión estadística es un 
número real no negativo que es cero si todos los 
datos  son  iguales  y  aumenta  a  medida  que  los 
datos se vuelven más diversos.
La  mayoría  de  las  medidas  de  dispersión 
tienen las mismas unidades que la cantidad que 
se mide. Pueden ser de dos tipos: Absolutas y 
Relativas
Medidas de Dispersión 
Absolutas
                Las medidas de dispersión más comunes son la 
                varianza  y  la  desviación  estándar,  que 
                representan la variabilidad alrededor de un punto 
                central (la media). La desviación estándar es más 
                utilizada  porque  se  reporta  en  las  mismas 
                unidades  que  la  variable.  También  existen 
                medidas  robustas  a  atípicos  como  el  rango 
                intercuartílico (IQR)
Medidas de Dispersión 
Absolutas
                Otra medida muy común es el rango que es un 
                buen  indicador  de  variabilidad  cuando  se  tiene 
                una  distribución  sin  valores  extremos.  Por  otro 
                lado  existen  otras  medidas  como  la  media 
                absoluta de las desviaciones y la desviación 
                de cuartil que ayudan a comprender la varianza 
                del conjunto de datos. 
Medidas de Dispersión 
Relativas
                                         Las medidas relativas de dispersión, a diferencia 
                                         de las absolutas, se calculan como proporciones o 
                                         porcentajes; por ejemplo, una medida relativa de 
                                         dispersión  es  la  relación  entre  la  desviación 
                                         estándar y la media.
                                         Existen    algunos   otros   ejemplos    como  el 
                                         coeficiente  de  variación  y  la  desviación 
                                         cuartílica.
Medidas de Dispersión
Como ejemplo si graficamos la distribución de 
dos poblaciones respecto a su peso como en 
la  figura  de  la  derecha,  podemos  ver  que 
existe un grupo con mayor dispersión (azul), 
lo cual quiere decir que los valores para ese 
grupo  pueden  ser  muy  grandes  o  muy 
pequeños.  Caso  contrario  se  observa  en  el 
grupo  rojo  con  una  varianza  mucho  más 
pequeña. 
Medidas de 
Asimetría
Medidas de Asimetría
La asimetría se refiere a la distorsión de simetría. 
Una distribución, o conjunto de datos, es simétrico 
si  se  ve  igual  a  la  izquierda  y  a  la  derecha  del 
punto central.
Son medidas de tercer momento y nos permiten 
entender  si  es  que  la  distribución  de  valores 
numéricos es normal  o no.
Medidas de Dispersión 
Relativas
                 Si  la  asimetría está entre -0.5 y 0.5 lo datos se 
                 consideran  muy  similares  a  una  distribución 
                 normal, si la asimetría está entre -1 y -0.5 y entre 
                 0.5  y  1  se  considera  moderada  y  cuando  es 
                 mayor  que  1  o  menor  que  <1  se  considera 
                 altamente asimétrica.
Medidas de Asimetría
Es  curioso  pensar  que  en  la  vida  real  la 
mayoría  de  variables  son  asimétricas.  Un 
ejemplo cotidiano es el de la variable salario 
o ingresos, como podemos ver en la figura de 
la  derecha  se  tiene  una  asimetría  a 
derecha o positiva (debido a que hay pocos 
datos  en  los  valores  más  grandes  de  la 
variable)  lo  cual  hace  que  la  mayoría  de 
datos se concentre en los valores más bajos 
de la variable 
Medidas de Curtosis
Medidas de 
Curtosis
La curtosis es una característica de la distribución 
de  frecuencia/probabilidad  para  una  variable 
aleatoria.
Una  gran  curtosis  implica  una  mayor 
concentración de valores de la variable, tanto 
muy  próximos  a  las  medias  de  la  distribución 
(pico) como muy alejados de ella (colas).
Medidas de 
Curtosis
                 En muchas ocasiones se reporta la curtosis como 
                 exceso sumándole el valor de referencia 3 (normal 
                 estándar). Es por esto que el valor de referencia 
                 para comparar es 3. 
 Medidas de Curtosis
   La curtosis es una medida que tiene menor 
   uso ya que es de cuarto momento (elevado a 
   la    4)    y    permite      ver     el    grado     de 
   apuntalamiento  de  una  variable  numérica. 
   Esta medida tiene gran aplicación cuando se 
   quiere  probar  si  una  variable  sigue  una 
   distribución normal.
         Recordemos…
                          Tipos de variables
                          En general tenemos dos 
                          grandes grupos de 
                          variables: categóricos 
                          (nominal y ordinal) y 
                          numéricos (intervalo y 
                          razón). Todas las variables 
                          se pueden clasificar de esta 
                          forma con excepción de los 
                          booleanos que son un caso 
                          especial.
Gráficos estadísticos
Tipos de gráficos
Tipos de gráficos
Si pensamos en los gráficos que podemos utilizar para realizar un análisis univariado, se 
destacan los siguientes: 
Line Plots Histogramas   Bar Charts  Box Plots
(Gráficos de             (Gráficos de 
Líneas)                  barras)
Otros tipos de gráficos
Si pensamos en los gráficos que podemos utilizar para realizar un análisis univariado, se 
destacan los siguientes: 
Piecharts    Radarplots  Violinplot   Gráfico de 
(Diagrama de                            puntos
torta)
Distribución de 
frecuencias
Distribución de frecuencias
Podemos utilizar las tablas de distribución de 
frecuencias, con las que se pueden 
representar los datos de una manera más 
fácil para analizarlos.
Distribución de frecuencias
También es posible elaborar tablas de 
distribución de frecuencias para datos no 
agrupados y para datos agrupados. 
Estas últimas se utilizan cuando tenemos muchos 
datos.
Herramientas para 
hacer gráficos
Herramientas para gráficos
Las dos librerías core de Python para hacer 
gráficos son Matplotlib que es la librería 
base y Seaborn es una versión mejorada de 
Matplotlib. En clases pasadas revisamos el 
funcionamiento de estas librerías.
Herramientas para gráficos
Además existen otras librerías que permiten 
hacer gráficos dinámicos de alta calidad con 
diferentes herramientas de customización 
como Bokeh y Plotly. Además existen 
utilidades como ggplot que se pueden utilizar 
en Python y R.
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Ejemplo en vivo
Utilizaremos el notebook en la carpeta de clase junto 
con el archivo accidents.csv. 
Repasaremos las diferentes escalas de medida, 
como crear histogramas, series de tiempo, 
Piecharts y otros gráficos. 
Además revisaremos cómo calcular medidas de 
tendencia central, dispersión, asimetría y 
curtosis utilizando la librería Scipy.
Análisis univariado y gráficos
Aplicaremos los conocimientos aprendidos hasta el 
  momento de Análisis univariado
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Análisis univariado y 
gráficos
Se les propone reunirse en grupos de 3-4 personas en 
Break-out rooms y realizar el análisis univariado del 
dataset de Gapminder (GAP Dataset) y de Properati (
Datos Properati) que utilizamos en la clase pasada. 
✔ Elegir algunas variables de cada dataset y luego 
seleccionar las medidas de resumen y gráficos 
apropiados según el caso.
✔ Generar interpretaciones de los resultados 
obtenidos
CLASE N°35
Glosario
Análisis estadístico: ciencia de              Medidas de tendencia central: número 
recopilar, explorar y presentar grandes       que permite entender la región central 
cantidades de datos para descubrir            de un conjunto de valores de datos. 
patrones y tendencias subyacentes.            Las medidas más comunes son la media, 
                                             mediana, moda y cuartiles
Tipos de análisis estadísticos: tres 
tipos de según el número de variables         Medidas de localización: números que 
que se analicen conjuntamente. Podemos        permiten comprender la distribución de 
destacar el análisis:  Univariado,            los datos y cantidad de datos por 
Bivariado y Multivariado.                     debajo de cierto umbral. Las más 
                                             comunes son los cuartiles, percentiles y 
Análisis univariado: análisis de cada         deciles.
una de las variables estudiadas por 
separado, se basa exclusivamente en una 
única variable.
¿Preguntas?
Opina y valora 
esta clase
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Tipos de análisis estadísticos
      ✓ Gráficos estadísticos
      Clase 51. DATA SCIENCE
Despliegue de Modelos 
         MLOps
Temario
               50                      51                     52
           Modelos de            Despliegue de        Introducción a Deep 
           Ensamble y            Modelos MLOps              Learning
        Boosting Models
                                ✓  Fundamentos de Cloud 
                                   Computing            ✓ Introducción a Deep 
        ✓ Métodos de ensamble   ✓  DevOps vs. DevSecOps    Learning 
                                ✓                       ✓ Perceptrón y perceptrón 
        ✓ Metodologías de          Continuous Deployment
           ensamble             ✓                          multi-capa
                                   Data team            ✓ CNN
                                ✓
                                   MLOps                ✓ RNN
Objetivos de la clase
         Abordar los alcances del Cloud Computing
         Encontrar cuales son las principales ventajas 
         de la metodología DevOps y MLOps
MAPA DE CONCEPTOS
                                       Cloud Computing
                                         DevOps
                                         DevSecOps
     Despliegue de                      Continuous 
    Modelos MLOps                       Deployment
                                        Data Team
                                          MLOps
    Repaso
             Les proponemos tomarse unos minutos 
             para realizar un repaso de los conceptos 
              aprendidos en Kahoot, ¿están listos?
                  Profe, puedes compartir el 
                   PIN o link de acceso al 
                        juego
Fundamentos de Cloud 
Computing
Definición
Cloud Computing
La computación en la nube es la entrega 
de diferentes servicios a través de 
Internet. Estos recursos incluyen 
herramientas y aplicaciones como 
almacenamiento de datos, servidores, 
bases de datos, redes y software.
El almacenamiento cloud permite guardar 
datos en una base remota. Siempre que 
un dispositivo electrónico tenga acceso a 
la web
Origen
Cloud Computing apareció en 1996 en un 
documento interno de Compaq. El término 
"Cloud" se vinculó originalmente con Cloud 
Computing, que se generalizó en General 
Magic generado por Apple a principios de los 
1990. El concepto fue discutido inicialmente 
por J.C.R. Licklider, el primer director de la 
Oficina de Técnicas de Procesamiento de la 
Información en la división ARPA del Pentágono 
en la década de 1960
Características fundamentales
1. Cloud Computing es la entrega de diferentes 
servicios a través de la internet como 
almacenamiento, servidores, bases de datos, 
conexiones y software
2. El almacenamiento en nube permite guardar 
archivos en forma remota con concurrencia
3. Los servicios pueden ser tanto públicos o 
privados, los públicos se proporcionan por 
una tarifa, mientras que los privados se 
alojan en una red para clientes específicos.
Servicios ofrecidos 
Sin importar el tipo de servicio se tienen 
diferentes utilidades como:
1. Correo
2. Almacenamiento, backup y recuperación de 
datos
3. Creación y testeo de aplicaciones
4. Análisis de datos
5. Streaming de audio y video
6. Entrega de software bajo demanda
7. Seguridad de datos
Tipos de Cloud 
Computing
Software as a service (SaaS)
SaaS: implica la concesión de 
licencias de una aplicación de 
software a los clientes. Las licencias 
generalmente se proporcionan a 
través de un modelo de pago por 
uso o bajo demanda. Este tipo de 
sistema se puede encontrar en 
Microsoft Office 365
Infrastructure as a service (IaaS)
IaaS: implica un método para 
entregar servicios y almacenamiento a 
través de conectividad basada en IP 
como parte de un servicio bajo 
demanda. Los clientes pueden evitar 
la necesidad de comprar software o 
servidores. Los ejemplos más 
populares son incluyen IBM Cloud y 
Microsoft Azure
Platform as a service (PaaS)
PaaS: Se considera la más compleja de los 
tres tipos de Cloud Computing. Comparte 
algunas similitudes con SaaS, la principal 
diferencia es que en lugar de entregar 
software en línea, en realidad es una 
plataforma para crear software que se 
entrega a través de Internet. Este modelo 
incluye plataformas como 
Salesforce.com y Heroku
Resumen
En los cuadros de arriba se ilustran las diferentes utilidades que se pueden manejar 
de acuerdo a cada Tipo de Servicio en Cloud. Evidentemente el esquema On Premise 
(Sistema local) permite el control de muchas más funciones pero tiene costos 
asociados
Acuerdo de Nivel de Servicio
Un Service Level Agreement (SLA) 
es un contrato que  describe el nivel 
de servicio que un cliente espera de 
su  proveedor.
Los SLA, sirven para  establecer unos 
indicadores que se puedan medir, 
para  regular el servicio que un 
proveedor presta y así, asegurar  el 
cumplimiento de las expectativas de 
los clientes.
Principales proveedores Cloud
Son muchos los proveedores 
de servicios Cloud pero los 
más importantes hoy en día 
son:
1. Google Cloud Platform
2. Microsoft Azure
3. Amazon Web Services
4. Alibab Cloud
Ventajas y 
desventajas
Ventajas y desventajas
                         Ventajas                                        Desventajas
       Uso de software desde cualquier dispositivo      La desventaja principal se asocia con la 
       desde apps o navegadores                         seguridad y sistemas de encriptado
       Conjunto de servicios ofrecidos (e.g email,      Los servidores cloud pueden fallar víctimas de 
       almacenamiento, back up, manipulación)           desastres, bugs o problemas de energía
       Reducción de costos de mantenimiento a           Sin conexión a internet no se puede trabajar 
       infraestructuras In House
       Tecnología de punta para realizar procesos de    Puede llegar a ser costosa por los esquemas 
       manipulación de datos                            diferenciales de cobro
       Buen servicio respecto a backups, logs y         En algunos casos la atención a requerimientos 
       recuperación de datos                            del cliente puede tardar
Arquitectura Cliente 
Servidor
Arquitectura Cliente-Servidor
Es un modelo informático en el 
que el servidor aloja, entrega y 
administra la mayoría de los 
recursos y servicios que consume 
el cliente. Este tipo de 
arquitectura tiene una o más 
computadoras cliente conectadas 
a un servidor central a través de 
una red o conexión a Internet.
Arquitectura Cliente-Servidor
Puntos clave de la 
arquitectura:
• Cliente
• Servidor
Para pensar
Ahora bien, ¿Cuál de los proveedores de servicios 
Cloud consideran que es el mejor? ¿Han utilizado 
alguno de estos proveedores antes en la empresa 
donde trabajan?
Contesta en el chat de Zoom 
DevOps vs 
DevSecOps
DevOps
DevOps
La  palabra  DevOps  es  una  contracción  de 
“Desarrollo”       (Development)         y 
“Operaciones” (Operations).
DevOps es una nueva tendencia en la industria 
TI dirigida a mejorar la agilidad del servicio de 
entregas.  El  movimiento  hace  énfasis  en  la 
comunicación  transparente,  la  colaboración 
junto  con  la  integración  entre  el  software  de 
desarrolladores y las operaciones de TI.
DevOps
Existen  muchas  nociones  de  lo  que 
realmente  significa  DevOps  pero  podemos 
establecer que DevOps:
1. No es solamente automatización
2. No solo son servicios Cloud
3. No es una herramienta implementada
4. No  es  un  equipo  de  trabajo  nuevo 
separado de las demás áreas de IT
Propósitos DevOps
Colaboración  Procesos  Herramientas
El propósito de DevOps es iterar de manera más rápida y 
eficiente durante la fase de desarrollo y su objetivo 
principal es establecer procesos de negocios alineados en 
flujo de acuerdo al concepto de  “justo a tiempo” (JIT= 
Just in Time por sus siglas en inglés).
Beneficios de implementación 
DevOps
Algunos   de    los   beneficios  de    la 
implementación de la metodología DevOps 
son:
1. Mejora la comunicación
2. Proporciona   escalabilidad   en   los 
   servicios
3. Reducción de costos
4. Seguimiento    continuado    de    las 
   aplicaciones
5. Más   publicaciones   y   con   mayor 
   frecuencia
6. Software de mejor calidad 
Métricas en DevOps
Algunos  de  las  métricas  para  seguimiento 
de la metodología DevOps son:
1. Tiempo y frecuencia de despliegue
2. Tiempo medio entre recuperación
3. Ratio de fallas de cambio
4. Service Level Agreement
5. Performance de aplicaciones
6. Tiempo medio entre recuperación
Las 3 P de DevOps
Devops  es  la  correlación  de  personas, 
procesos y productos (3P) para permitir la 
entrega  continua  de  valor  a  los  usuarios 
finales. los resultados están estrechamente 
relacionados  con  la  capacidad  de  que  se 
produzcan  lanzamientos  frecuentes  y,  al 
mismo  tiempo,  con  la  capacidad  de 
mantener el mismo nivel de calidad.
Fuente: DevSecOps
Personas
Sabemos que los equipos de trabajo 
están conformados por Personas.
Y por supuesto que no todos somos 
iguales: cada persona tiene emociones 
diferentes, pueden manejar mejor o 
peor manera dependiendo de las 
situaciones personales y laborales que 
atraviesen. 
Procesos
  Existen  4  grandes  procesos,  en  alto  nivel, 
  que  tenemos  que  tener  en  cuenta.  Estos 
  procesos,  por  supuesto,  pueden  darse 
  cíclicamente  y  no  necesariamente  en 
  cascada:
  •   Planificar:  Crear  un  backlog,  equipos 
      interdisciplinarios, planificar testing, etc.
  •   Desarrollar  +  Probar:  Avanzar  en  la 
      construcción  por  ejemplo  del  software  y 
      testearlo.
 Procesos
     •    Liberar: Lo que planifiqué y desarrollé + 
          probé, lo debo liberar a ambientes por 
          ejemplo de Desarrollo o Producción.
     •    Monitorear + Aprender: Un punto clave 
          de entender, es que el trabajo no terminó 
          con la liberación. Recién comienza aquí el 
          aprendizaje basado en un monitoreo 
          proactivo para volver a arrancar esta 
          rueda: planificar, desarrollar + probar, y 
          liberar, para  monitorear y volver a 
          aprender.
Procesos
Ciclo de Deming: el ciclo de Deming 
también conocido como ciclo PDCA (del 
inglés Plan-Do-Check-Act),, es un método 
sistemático para la resolución de problemas 
con el fin de generar una mejora continua de 
la calidad, en cuatro pasos: Planear 
(Objetivos, Recursos, Comunicar), Hacer 
(Procesos, Actividades, Productos), Verificar 
(Medidas, Auditorías, Verificaciones) y 
Actuar (Analizar, Acciones correctivas) 
Productos
Aplicación de alguna herramienta de devops. Existen múltiples herramientas 
que podríamos mencionar, las más importantes son las siguientes:
DevSecOps
DevSecOps
DevSecOps         significa      desarrollo, 
seguridad  y  operaciones.  Se  trata  de 
un  enfoque  que  aborda  la  cultura,  la 
automatización       y   el    diseño     de 
plataformas,  e  integra  la  seguridad 
como una responsabilidad compartida 
durante todo el ciclo de vida de la TI.
Diferencias entre DevSecOps y 
DevOps
DevOps  no  solo  se  ocupa  de  los 
equipos de desarrollo y operaciones. 
Si   desea  aprovechar  al  máximo  la 
agilidad y la capacidad de respuesta de 
los  enfoques  de  DevOps,  la  seguridad 
de la TI también debe desempeñar un 
papel  integrado  en  el  ciclo  de  vida 
completo de sus aplicaciones.
Diferencias entre DevSecOps y 
DevOps
En el marco de trabajo en colaboración 
de DevOps, la seguridad es una 
responsabilidad compartida e 
integrada durante todo el proceso. 
Puesto que es un enfoque tan 
importante, se acuñó el término 
"DevSecOps" para enfatizar la 
necesidad de crear una base de 
seguridad en las iniciativas de DevOps.
Diferencias entre DevSecOps y 
DevOps
Este nuevo término “DevSecOps” 
implica pensar desde el principio en la 
seguridad de las aplicaciones y de la 
infraestructura. También implica 
automatizar algunas puertas de 
seguridad para impedir que se 
ralentice el flujo de trabajo de DevOps. 
A fin de cumplir con estos objetivos, es 
necesario seleccionar las 
herramientas adecuadas para 
integrar la seguridad de manera 
permanente
Roles en DevOps
Los roles fundamentales en 
DevOps son:
1. Arquitecto de datos en nube
2. Entusiasta de DevOps
3. Desarrollador de software
4. Especialista en seguridad
5. Ingeniero DevOps
6. Coordinador 
7. Encargado de control de 
calidad
 Para pensar
¿Se puede entender DevOps como una 
metodología que se desarrolla con el 
objetivo de incrementar la productividad 
y calidad de los productos que se crean?
Verdadero/Falso 
Contesta en el chat de Zoom 
HANDS ON LAB
Exploramos el  proceso para 
crear una cuenta en Google 
Cloud y posteriormente crear un 
proyecto nuevo paso a paso, 
podemos utilizar la siguiente 
guía
Cualquier inquietud pueden 
consultar a su tutor o profesor
10-15 min
         ☕
       Break
     ¡10 minutos y 
     volvemos!
          ¡Lanzamos la
          Bolsa de 
          Empleos!
         Un espacio para seguir potenciando tu carrera y 
         que tengas más oportunidades de inserción 
         laboral.
         Podrás encontrar la Bolsa de Empleos en el menú 
         izquierdo de la plataforma.
         Te invitamos a conocerla y ¡postularte a tu futuro 
         trabajo!
           Conócela
Continuous 
Deployment
Definición
Continuous Deployment (CD)
La implementación continua es 
una estrategia para las versiones de 
software en la que cualquier 
compromiso de código que pase la 
fase de prueba automatizada se 
libera automáticamente en el 
entorno de producción, realizando 
cambios que son visibles para los 
usuarios del software.
Continuous Deployment (CD)
Sólo debe implementarse cuando los 
equipos de desarrollo y TI se 
adhieren rigurosamente a las 
prácticas de desarrollo para la 
producción y las pruebas 
exhaustivas, y cuando aplican un 
monitoreo sofisticado en tiempo real 
en la producción para descubrir 
cualquier problema con las nuevas 
versiones.
Continuous Deployment (CD)
Con la integración continua, los 
desarrolladores integran frecuentemente 
su código a la rama principal de un 
repositorio común
La idea aquí es reducir los costos de 
integración, haciendo que los 
desarrolladores realicen 
integraciones más rápidamente y 
con mayor frecuencia
Actividades 
relacionadas
Continuous Deployment
Dentro de este sistema de monitoreo se 
requiere el desarrollo de las siguientes 
actividades:
1. Despliegue en producción
2. Verificación de solución
3. Monitoreo de problemas
4. Respuesta y recuperación
A continuación explicaremos cada uno
Actividades Continuous Deployment
1) Despliegue en producción: se refiere a las prácticas necesarias para 
desplegar una solución en un ambiente productivo con prácticas como: 
Dark launches, Feature toggles, Automatización de despliegue, 
despliegue selectivo y automático, control de versiones
Actividades Continuous Deployment
2) Verificación de solución: se refiere a las prácticas necesarias para 
asegurarse que los cambios deseados estén implementados en producción 
antes de llegar al cliente final con prácticas como: Production testing, 
test de automatización, test de data management y tests de 
requerimientos no funcionales (NFRs)
Actividades Continuous Deployment
3) Monitoreo de problemas: se refiere a las prácticas necesarias para 
monitorear y reportar cualquier problema que surja en producción con 
prácticas como: Telemetría Full stack, despliegues visuales y 
monitoreo federal (visión holística de los problemas)
Actividades Continuous Deployment
4) Respuesta y recuperación: se refiere a las prácticas necesarias para 
responder rápidamente a problemas que ocurran en el despliegue con 
prácticas como: detección proactiva, colaboración cross-team, 
reproducciones de sesiones en usuarios, Rollbacks e 
Infraestructura inmutable 
DevOps y CD
DevOps y CD
CD implica actividades asociadas a 
operaciones importantes que se asocian 
frecuentemente con las operaciones en 
DevOps. Estas actividades se enfocan en 
desplegar soluciones en ambientes 
productivos, verificando su integridad 
funcional y asegurándose de que se puede 
monitorear efectivamente un producto 
luego de su lanzamiento 
Data Team
               Data Team
REEMPLAZAR      Es el grupo de cargos (roles) dentro de una 
POR IMAGEN      organización que se encargan de todo el proceso de 
               manipulación, estructuración y generación de insights 
               a partir de los datos disponibles. Está compuesto 
               usualmente de 3 roles importantes:
               ✔ Data Scientist
               ✔ Data Engineers
               ✔ Data Analysts
Data Scientist
Es capaz de tomar proyectos de Data Science desde el                 REEMPLAZAR 
inicio al fin. Pueden almacenar grandes cantidades de                 POR IMAGEN
información, crear modelos predictivos y presentar 
resultados. 
Skills: Matemáticas, Programación y Comunicación
Software comúnmente usado: SQL, Python, R
               Data Engineers
               Son personas versátiles capaces de usar la ciencia de 
               computación para procesar grandes cantidades de 
               datos. Se enfocan en procesos de codigo, limpieza de 
               datos e implementar solicitudes de los data Scientists
               Skills: Matemáticas, Programación y Big Data
               Software comúnmente usado: Hadoop, NoSQL, 
               Python
Funciones del Data Engineer
• Desarrollar, y construir las 
arquitecturas de datos.
• Alinear arquitectura con requisitos 
comerciales y del negocio.
• Identificar formas de mejorar la 
confiabilidad, eficiencia y calidad de 
los datos.
Funciones del Data Engineer
• Preparar datos para modelos 
predictivos y prescriptivos.
• Gestionar las actualizaciones de la 
infraestructura de datos.
• Garantizar la integridad de los 
procesos asociados a datos
Roadmap para ser Data Engineer
Data Analysts
Son personas que ayudan a otras personas dentro de la                REEMPLAZAR 
compañía a entender solicitudes específicas por medio                 POR IMAGEN
de gráficas y resúmenes numéricos.
Skills: Estadística, Comunicación y Entendimiento del 
negocio
Software comúnmente usado: Excel, Tableau, SQL
MLOps
Definición
MLOps
MLOps o ML Ops es un conjunto de 
prácticas que tiene como objetivo 
implementar y mantener modelos de 
aprendizaje automático en producción de 
manera confiable y eficiente. La 
palabra es un compuesto de 
"aprendizaje automático" y la 
práctica de desarrollo continuo de 
DevOps en el campo del software. 
Niveles de 
implementación
MLOps
Para poder implementar el proceso de 
MLOps se requieren realizar tres fases:
1. Diseño de aplicaciones basadas en 
ML  
2. Experimentación y Desarrollo de ML
3. Operaciones relacionadas con ML
A continuación explicaremos cada una 
de las fases
Fase 1: Diseño de aplicaciones basadas en 
ML
En esta etapa, identificamos a nuestro 
usuario potencial, diseñamos la 
solución de aprendizaje automático para 
resolver su problema y evaluamos el 
desarrollo posterior del proyecto. 
Principalmente, actuaríamos dentro de 
dos categorías de problemas: aumentar 
la productividad del usuario o 
aumentar la interactividad de nuestra 
aplicación.
Fase 2: Experimentación y desarrollo de 
ML
Se verifica la aplicabilidad de ML para 
nuestro problema usando pruebas de 
conceptos. Ejecutamos iterativamente 
diferentes pasos, como identificar o pulir 
el algoritmo ML adecuado para nuestro 
problema, ingeniería de datos e 
ingeniería de modelos. El objetivo 
principal en esta fase es entregar un 
modelo ML de calidad estable que 
ejecutaremos en producción.
Fase 3: Operaciones relacionadas con ML 
El objetivo de esta fase es lograr 
entregar un modelo ML desarrollado 
previamente en producción mediante el 
uso de prácticas DevOps establecidas 
previamente, como pruebas, control de 
versiones, entrega continua y monitoreo. 
Todo esto con el fin de que se pueden 
dar cumplimiento a los requerimientos 
del cliente
¿Qué tener en 
cuenta para usar 
MLOps?
¿Qué tener en cuenta para usar MLOps?
• Calidad de los datos: Tener en cuenta de dónde vienen, 
  calidad, si son fiables, etc.
• Degradación de los modelos: Al cabo del tiempo van 
  perdiendo calidad.
• Localidad:  En  el  momento  de  la  preparación  se  están 
  entrenando  los  modelos  con  unos  datos  específicos 
  basados en una geografía.
¿Cómo es el proceso de MLOps?
      1. Diseño:  Ajuste  de  requerimientos,  establecer  las 
        necesidades  que  tienen  los  usuarios  y  qué  queremos 
        cubrir, exploración de los datos, experimentación.
      2. Desarrollo  del  modelo:  Desarrollo  de  un  modelo 
        funcional capacitado para pasar a producción.
      3. Operaciones:    Despliegue,   automatización    de 
        entrenamientos, extracción de datos, etc.
Principios de MLOps
Principios de MLOps
• Automatización:  ML  pipeline  y 
 CI/CD pipeline.
• Versionado: Basado     en    tres 
 pilares, datos, modelo y código.
• Testing: Base  sobre  la  que  va  a 
 funcionar todo el sistema.
Principios de MLOps
• Monitorización: De  los  principales 
 indicares  de  performance  de  mi 
 modelo de manera constante.
• Reproductividad: De la ingeniería 
 de          características,         del 
 entrenamiento  del  modelo  y  del 
 despliegue del modelo. 
• Herramientas: Principalmente 
 servicios Cloud.
Revisión de pares
En esta oportunidad haremos una revisión de pares para 
     los proyectos finales
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Revisión de pares
Nos reuniremos en el break out rooms con 
cada tutor y realizaremos lo siguiente:
1. Cada uno presentará su proyecto (~2-3 
min) con el fin de recibir feedback
2. Los demás estudiantes y el tutor podrán 
dar sugerencias para mejorar los 
proyectos
3. Cualquier consulta puede ser resuelta 
con el fin de mejorar la calidad de los 
proyectos 
¿Preguntas?
CLASE N°51
Glosario
Cloud Computing: La computación en la           Continuous Deplyment: es una 
nube es la entrega de diferentes servicios      estrategia para las versiones de software 
a través de Internet.                           en la que cualquier compromiso de código 
                                               que pase la fase de prueba automatizada 
                                               se libera automáticamente en el entorno de 
DevOps: DevOps es una nueva tendencia           producción, realizando cambios que son 
en la industria TI dirigida a mejorar la        visibles para los usuarios del software.
agilidad del servicio de entregas. 
                                               MLOps: es un conjunto de prácticas que 
                                               tiene como objetivo implementar y 
DevSecOps:         significa     desarrollo,    mantener modelos de aprendizaje 
seguridad  y  operaciones.  Se  trata  de  un   automático en producción de manera 
                                               confiable y eficiente
enfoque  que  aborda  la  cultura,  la 
automatización y el diseño de plataformas 
integrando la seguridad
Opina y valora 
esta clase
           Resumen 
       de la clase hoy
      ✓ Arquitectura Cliente-Servidor
      ✓ Riesgos del Cloud Computing
      ✓ DevOps vs DevSecOps
      ✓ Continuous Deployment 
      ✓ Data Team 
      ✓ MLOps
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 55. DATA SCIENCE
       Datathon
Temario
                        54                    55
                  Introducción al          Datathon
                 Procesamiento de 
                 Lenguaje Natural II
                 ✓ Introducción a      ✓ Introducción
                    spaCY
                                       ✓ Proyecto final
                 ✓ Análisis de 
                    sentimiento 
Objetivos de la clase
         Integrar conceptos vistos a lo largo de la 
         cursada
         Poner en práctica las fases de Data 
         Acquisition, Data Wrangling, EDA, 
         Modelamiento y Validación de modelos 
 MAPA DE CONCEPTOS
                                                EDA
                                             Data Wrangling
                                              Modelos de 
                                              regresión
        Datathon                              Métricas de 
       CoderHouse                             performance
                                             Optimización de 
                                              parámetros
                                             Proyecto final
Cuestionario final
¿Te gustaría comprobar tus 
conocimientos del curso?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot
Duración: 10 minutos
Introducción
¿Qué es una 
Datathon?
Datathon
Una “Datathon” es un evento en donde se 
reúnen personas con conocimientos en manejo de 
datos (e.g científicos de datos, Data Engineers, 
Data Analysts) con el fin de trabajar en forma 
multidisciplinaria permitiendo resolver preguntas 
problema relevantes para un problema específico.
Estos eventos tienen una duración limitada y se 
busca establecer diferentes enfoques a la hora de 
resolver un problema
Objeto de estudio
spaCY
El presente Datathon busca lograr predecir los precios de las viviendas residenciales en 
Ames, Iowa, Estados Unidos. Para ello, se deberán poner en práctica los diversos 
conceptos vistos y estudiados a lo largo del curso de Data Science. 
Objetivo
Objetivo
Para  cada  Id  en  el  conjunto  de  prueba,  se  debe 
predecir  el  valor  de  la  variable  “SalePrice”.  La 
métrica que se utilizará para evaluar la performance 
del modelo deberá ser el RMSE (Root-Mean-Squared-
Error (RMSE). 
Objetivo
Como parte del desafío propuesto, se busca que los 
estudiantes  puedan  generar  un  archivo  llamado: 
“predicción_inmueble.csv”, el cual deberá contener 
para  cada  “Id”  de  registro  su  correspondiente 
predicción. En base a la siguiente estructura: 
Descripción de datos
Descripción de datos
✔ train.csv: Dataset de Train.
✔ test.csv: Dataset de Test.
✔ data_description.txt:  Descripción  completa  de 
cada columna del dataset.
✔ ejemplo_output.csv: Dataset de referencia de la 
salida que se pretende generar como resultado de 
la aplicación del modelo de regresión.
Predicción de precios de 
     inmuebles
En esta oportunidad utilizaremos lo aprendido a lo largo 
del curso para resolver un problema de la vida real
     Duración: 80-90 mins
ACTIVIDAD EN CLASE
Predicción de 
inmuebles
Nos reuniremos en breakout rooms y formaremos grupos, 
con orientaciones del profesor(a) y tutores(as) 
realizaremos lo siguiente:
1) Realizar la lectura de datos de los archivos propuestos
2) Llevar a cabo un análisis exploratorio de datos (EDA) 
con su correspondiente interpretación
3) En caso de ser necesario desarrollar el proceso de Data 
Wrangling
ACTIVIDAD EN CLASE
Predicción de 
inmuebles
4) Desarrollar al menos 3 algoritmos de regresión para 
resolver el desafío
5) Calcular métricas como RMSE, R2, MAE para cada 
modelo
6) Implementar algún método de optimización de 
hiperparametros
7) Comparar distintos modelos creados
8) Seleccionar el mejor modelo justificando la decisión
9) Debatir colaborativamente en el proceso
         ☕
       Break
     ¡10 minutos y 
     volvemos!
          ¡Lanzamos la
          Bolsa de 
          Empleos!
         Un espacio para seguir potenciando tu carrera y 
         que tengas más oportunidades de inserción 
         laboral.
         Podrás encontrar la Bolsa de Empleos en el menú 
         izquierdo de la plataforma.
         Te invitamos a conocerla y ¡postularte a tu futuro 
         trabajo!
           Conócela
Proyecto final
Proyecto final
El Proyecto Final se construye a partir de los desafíos 
que se realizan clase a clase. Se va creando a medida 
que el estudiante sube los desafíos entregables a 
nuestra plataforma y recibe la respectiva 
retroalimentación.
El objetivo es que con el trabajo en duplas pueda 
utilizar su Proyecto Final como parte de su portafolio 
personal.
Proyecto final
Contarás con 20 días, una vez finalizado tu curso, para 
entregar tu Proyecto Final en la plataforma. 
¡Atento/a!  Pasados esos días el botón se inhabilitará. 
    Entrenamiento y 
optimización de Modelos de 
Deberás entregar tu Proyecto Final. Entrenarás y optimizarás diversos 
   Machine Learning
modelos de machine learning para resolver una problemática específica, 
detectada en la instancia de entrega anterior. El objetivo es que puedan 
utilizar modelos de ML para resolver el problema de una industria o negocio
ENTREGA DEL PROYECTO 
FINAL
Entrenamiento y optimización de 
modelos de Machine Learning
Objetivos generales
✓ Utilizar modelos de Machine Learning para resolver un problema de una industria o 
negocio
Objetivos específicos
✓ Retomar el trabajo realizado en la segunda pre entrega, sumando el trabajo con 
Machine Learning 
✓ Modelar la situación como un problema de Machine Learning 
✓ Entrenar modelos de Machine Learning 
✓ Realizar ingeniería de atributos y normalización/estandarización de variables
✓ Seleccionar el modelo con mejor performance
       ENTREGA DEL PROYECTO 
       FINAL
 Entrenamiento y optimización de 
 modelos de Machine Learning
 Requisitos base
   ✓ Un Notebook (Colab o Jupyter) que debe contener:
   1.  Abstracto con motivación y audiencia:  Descripción de alto nivel de lo que motiva a analizar los datos 
       elegidos y audiencia que se podría beneficiar de este análisis.
   2.  Preguntas/Problema que buscamos resolver: Si bien puede haber más de una problemática a resolver, la 
       problemática principal debe encuadrarse como un problema de clasificación o regresión.
   3.  Breve Análisis Exploratorio de Datos (EDA): Análisis descriptivo de los datos mediante visualizaciones y 
       herramientas estadísticas, análisis de valores faltantes.
   4.  Ingeniería de atributos: Creación de nuevas variables, transformación de variables existentes (i.e 
       normalización de variables, encoding, etc.)
   5.  Entrenamiento y Testeo: Entrenamiento y testeo de al menos 2 modelos distintos de Machine Learning 
       utilizando algún método de validación cruzada.
   6.  Optimización: Utilizar alguna técnica de optimización de hiperparámetros (e.g gridsearch, randomizedsearch, 
       etc.)
   7.  Selección de modelos: utilizar las métricas apropiadas para la selección del mejor modelo (e.g AUC, MSE, etc.)
 ENTREGA DEL PROYECTO 
 FINAL
Entrenamiento y optimización de 
modelos de Machine Learning
Piezas sugeridas
✓ Librerías:
numpy - pandas - matplotlib - sklearn - xgboost - shap
✓ Claridad de código:
Estructura - Markdown - Comentarios
Modelo de Proyecto final
✓ Ejemplo proyecto final
Explicación del desafío
✓ ¡Click aquí!
¿Preguntas?
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Proyecto final
Opina y valora 
esta clase
   ¡Les damos la 
    bienvenida!
       ¿Comenzamos?
Esta clase va a ser
grabad
  a
                COMISIÓN N°####
                Presentación 
                del equipo
                 ✔ Profesor/a responsable: Juan Pérez
                 ✔ Coordinador/a: Juan Pérez
                 ✔ Tutores y tutoras:
                    ○ Juan       ○ Juan 
                      Pérez        Pérez
                    ○ …          ○ …
                    ○ …          ○ …
                    ○ …          ○ …
                    ○ …          ○ …
                    ○ …          ○ …
                    ○ …          ○ …
Presentación
de estudiantes
Por encuestas de Zoom
1. País
2. Conocimientos previos
3. ¿Por qué elegiste este curso?
 ¿Dudas sobre el
   onboarding?
        Míralo aq
        uí
!  Lo que debes 
 saber
antes de empezar
Acuerdos 
y compromisos
ACUERDOS Y COMPROMISOS
Convivencia
✓ Conoce aquí nuestro                        ✓ Ten en cuenta las normas del 
    código de conducta y ayúdanos a             buen hablante y del buen oyente, 
    generar un ambiente de clases               que nunca están de más. 
    súper ameno.
                                             ✓ Verifica el estado de la cámara y/o 
✓ Durante las clases, emplea los                el micrófono (on/off) de manera 
    medios de comunicación oficiales            que esto no afecte la dinámica de la 
    para canalizar tus dudas, consultas         clase. 
    y/o comentarios: chat Zoom 
    público y privado, y por el chat 
    de la plataforma.
ACUERDOS Y COMPROMISOS
Distractores
✓ Encuentra tu espacio y crea el momento 
oportuno para disfrutar de aprender
✓ Evita dispositivos y aplicaciones que 
puedan robar tu atención
✓ Mantén la mente abierta y flexible, los 
prejuicios y paradigmas no están invitados
ACUERDOS Y COMPROMISOS
Herramientas
✓ Mantén a tu alcance agua, mate o       ✓ Conéctate desde algún equipo 
   café                                     (laptop, tablet) que te permita 
                                            realizar las actividades sin 
                                            complicaciones.
✓ Si lo necesitas, ten a mano lápiz y 
   papel para que no se escapen las 
   ideas. Pero recuerda que en Google    ✓ Todas las clases quedarán grabadas y 
   Drive tienes archivos que te             serán compartidas tanto en la 
   ayudarán a repasar, incluidas las        plataforma de Coderhouse como 
   presentaciones.                          por Google Drive.
                     ACUERDOS Y COMPROMISOS
                     Equipo
                      ✓ ¡Participa de los After Class! Son un gran 
                         espacio para atender dudas y mostrar avances.
                      ✓ Intercambia ideas por el chat de la 
                         plataforma.
                      ✓ Siempre interactúa respetuosamente.
                      ✓ No te olvides de valorar tu experiencia 
                         educativa y de contarnos cómo te va.
Interacciones
en clase
INTERACCIONES EN CLASE
Mientras el 
profesor 
explica
Para mantener una comunicación clara y fluida a lo 
largo de la clase, te proponemos mantener 2 reglas:
1 Si tienes dudas durante la explicación, debes 
.  consultarle directamente por privado a tu tutor 
por el chat de Zoom.
INTERACCIONES EN CLASE
Espacios para 
consultas
Entre contenido y contenido, se abrirán breves 
2
.  espacios de consulta. Allí puedes escribir en el 
chat tu pregunta. 
¡Tu duda puede ayudar a otras personas!
No olvides seleccionar “todos” para que todos 
puedan leerte (y no solo tu tutor). 
INTERACCIONES EN CLASE
Funcionalidades
Para evitar saturar el chat de mensajes,            Por ejemplo: si se pregunta si se escucha 
utiliza los signos que figuran en el apartado       correctamente, debes seleccionar la opción 
Participantes, dentro de Zoom.**                    “Sí” o “No”. 
**Para quitar el signo, presiona el mismo botón nuevamente o la opción “clear all”.
After Class
AFTER CLASS
¿Qué son?
Te acompañamos para resolver tus                 Tu profesor/a está comprometido con tu 
consultas sobre el contenido en estos            educación, por lo tanto:
espacios.
                                                  ✓ Se responderán dudas puntuales 
Si hay temas que no se entendieron o                   que hayan quedado sobre los temas 
necesitan refuerzo se trabajarán en una                dados. ¡Vení preparado, queremos 
clase de 1 hs que opera como espacio de                escucharte! 
consulta.
No son obligatorias ni se toma asistencia,         ✓ Se verán temas de conocimientos 
pero son el espacio uno a uno con tu                   básicos para la nivelación de 
profesor/a** para responder dudas                      saberes. 
puntuales o reforzar conceptos.
**Los/as tutores/as también serán protagonistas, liderando 5 veces este espacio en todo el curso.
Desafíos
y entregables
DESAFÍOS Y ENTREGABLES
¿Qué 
son?
      Actividades en clase                          Desafíos entregables
Ayudan a poner en práctica los conceptos       Relacionados completamente con el 
y la teoría vista en clase. No deben ser       proyecto final. Deben ser subidos a la 
subidos a la plataforma.                       plataforma hasta 7 días luego de la 
                                              clase, para que sean corregidos.
DESAFÍOS Y ENTREGABLES
¿Qué 
son?
      Desafíos                                   Entregas del Proyecto 
      complementarios                            final
Desafíos que complementan a los             Entregas con el estado de avance del 
entregables. Son optativos, pero de ser     proyecto final, que deberán ser subidas a 
subidos a la plataforma a tiempo, y         la plataforma hasta 7 días luego de la 
habiendo sido aprobados, suman puntos       clase para ser corregidas cada tutor/a. 
para el top 10.
 DESAFÍOS Y ENTREGABLES
Proyecto 
final
El Proyecto final se construye a partir de        Se debe subir a la plataforma la ante-
los desafíos que se realizan clase a              última o última clase del curso. En caso 
clase. Se va creando a medida que el              de no hacerlo tendrás 20 días a partir 
estudiante sube los desafíos entregables          de la finalización del curso para cargarlo 
a nuestra plataforma.                             en la plataforma. Pasados esos días el 
                                                 botón de entrega se inhabilitará.
El objetivo es que cada estudiante pueda 
utilizar su Proyecto final como parte de su 
portfolio personal. Recordá que el mismo  
es de carácter individual.
¿Cuál es nuestro 
Proyecto final?
PROYECTO FINAL
Data Science
Consigna: 
El proyecto final consiste en tres entregas, donde de 
manera secuencial irás resolviendo un problema 
para una industria, negocio o proyecto personal. El 
proyecto final debe plasmar cada una de las fases 
de un proyecto de Data Science: Data Acquisition, 
Data Wrangling, Exploratory Data Analysis, 
Modelamiento, Evaluación y Despliegue. Deberás 
entregar un notebook en formato jupyter notebook 
(.ipynb) y una presentación (PDF, ppt o Google 
Slides).
El mismo es de carácter individual. 
PROYECTO FINAL
Proyectos de nuestros 
estudiantes
En este link podrán ver los Proyectos 
finales de diferentes estudiantes de este 
curso de comisiones anteriores.
¡Esperamos que les resulten 
inspiradores!
      PROYECTO FINAL
      Entrega                               Requisito                                 Fecha
1° entrega                Elección de potenciales datasets para importe        Clase 5 
                          con la librería Pandas
2° entrega                Visualizaciones en Python                            Clase 8 
3° entrega                Estructurando un proyecto de DS- Parte I             Clase 12 
4° entrega                Estructurando un proyecto de DS- Parte II            Clase 17
5° entrega                Estructurando un proyecto de DS- Parte III           Clase 19
Primera Pre-entrega       Análisis de datos con Python                         Clase 21 
     PROYECTO FINAL
      Entrega                              Requisito                               Fecha
6° entrega                Descarga de datos desde APIs públicas              Clase 27
7° entrega                Data Wrangling                                     Clase 29
8° entrega                Data StoryTelling                                  Clase 33
9° entrega                Obtención de Insights                              Clase 36
Segunda Pre-entrega       Obtención de insights a partir de                  Clase 38
                         visualizaciones
     PROYECTO FINAL
      Entrega                             Requisito                              Fecha
10° entrega               Entrenando un algoritmo de Machine Learning       Clase 42
11° entrega               Evaluando modelos de Machine Learning             Clase 45
12° entrega               Ingeniería de atributos y selección de variables  Clase 47
Entrega final             Entrenamiento y optimización de Modelos de        Clase 55
                         Machine Learning
          ¡Importante!
     Los desafíos y entregas se deben cargar hasta siete días 
    después de finalizada la clase. Te sugerimos llevarlos al día.
Completa con éxito
el programa
                           Desafíos                     Desafíos
                           Desafíos       Entrega       Desafíos      Proyecto
Kick Off      Clase 0      Desafíos     intermedia      Desafíos        final      Certificado      Top 10
              Recuerda que el primer requisito para finalizar con éxito es asistir a las 
                                                 clases.
      Clase 01. DATA SCIENCE
  La necesidad de 
  información en la 
    Industria 4.0
Temario
               00                     01                       02
         Introducción a       La necesidad de           Introducción a 
          la Ciencia de        Información en                  la 
             Datos             la Industria 4.0         programación 
                                                          con Python 
         ✓ Ciencia de datos                             ✓ Definición de 
                               ✓ Industria 4.0             (Parte I)
         ✓ Ciclo de vida de    ✓ Transformación             programa 
            los proyectos         digital               ✓ Instalación de 
                               ✓  Ciclo de vida de un       Python
         ✓ Tipos de Data          proyecto de ciencia 
            Science               de datos              ✓ Nociones básicas
                               ✓  Valor y retorno de 
         ✓ Importancia            la Ciencia de Datos
                               ✓ Estrategia data-
                                  driven
Objetivos de la clase
         Identificar los componentes importantes de 
         una estrategia de Data & Analytics.
         Comprender el rol de los datos en la 
         organización desde una perspectiva de 
         Transformación Digital y en la Industria 4.0
         Facilitar la identificación de oportunidades del 
         uso de los datos para la transformación digital 
         y la estrategia de negocios.
MAPA DE CONCEPTOS                   Industria 4.0
                                    Cuarta 
                                    Revolución 
                                    Industrial
                  Industria 4.0     El ambiente de la 
                                    industria 4.0
                                    Características de 
                  Transformación    la industria 4.0
                  Digital
Necesidad de la 
información en la 
industria 4.0       Ciclo de vida de un 
                  proyecto de ciencia 
                  de datos
                  Valor y retorno de la 
                  ciencia de datos
                  Estrategia Data 
                  Driven
Transformación 
Digital e Industria 
4.0    
Cuarta Revolución 
Industrial
Cuarta 
Revolución 
A partir de esta nueva nominación se transformó la                   REEMPLAZAR 
mirada sobre la industria de las revoluciones 
Industrial                                                            POR IMAGEN
anteriores: Industria 1.0, Industria 2.0 e Industria 
3.0. 
������ ¿Escuchaste hablar de la Primera y Segunda 
Revolución Industrial en la escuela? Bueno, ya 
vamos por la 4ta.
Fuente: Mixtrategy.com
                    Cuarta 
                    Revolución 
  REEMPLAZAR        El Banco Interamericano de Desarrollo (BID) 
   POR IMAGEN       Industrial
                    identifica la particularidad de esta Revolución Industrial 
                    en la convivencia de una gran variedad de tecnologías 
                    que se fusionan borrando los límites entre lo físico, lo 
                    digital y lo biológico.
                    Sin duda, esto implica un gran cambio de paradigma, 
                    en todos los ámbitos de la vida.
             ¿Quieres saber más? Consulta el material ampliado de la clase
Cuarta Revolución 
Industrial
Entendemos la cuarta Revolución 
Industrial como la transición hacia nuevos 
sistemas ciber-físicos que operan en 
forma de complejas redes como salto 
cualitativo a sólo 50 años de la 
Revolución digital (Industria 3.0).
Industria 4.0
¿Por qué hablamos de 
Industria 4.0?
El término “Industria 4.0” surge de un          El fundamento de la Industria 4.0 está en 
conjunto de especialistas                       el desarrollo de sistemas tipo SCADA con 
multidisciplinarios convocados por el           fácil supervisión y sustentabilidad así 
gobierno alemán a comienzos de la               como estructuras IIoT que permiten 
década de 2010 enfocados en re-diseñar          utilizar el poder de las máquinas 
un programa de mejoras para la industria        inteligentes y el análisis en tiempo 
manufacturera.                                  real.
Supervisory Control and Data Acquisition (SCADA)                  Sistemas IIoT
El ambiente de la 
Industria 4.0
               El ambiente de 
               la industria 4.0
               El ambiente de la industria 4.0 está enmarcado en 
               cuatro grandes pilares:
               ������ Internet of Things (IoT)
               ������ Internet of Services (IoS)
               ������ Internet of Data (IoD)
               ������ Internet of People (IoP)
               Estos componentes conforman lo que se conoce como 
               Smart Factory
Internet of 
Things (IoT)
✔ Describe la conectividad entre objetos físicos 
como tostadores, refrigeradores, TV al Internet 
permitiendo la comunicación entre 
dispositivos.
✔ Se requiere de diversos factores como una 
infraestructura de conectividad con alta 
velocidad y protocolos de comunicación (e.j 
Machine to Machine M2M) para la correcta 
implementación del IoT.
               Industrial 
               Internet of 
 REEMPLAZAR    ✔ Se refiere a sensores interconectados, 
                 instrumentos y otros dispositivos industriales 
 POR IMAGEN    Things (IIoT)
                 enlazados entre sí con aplicaciones 
                 computacionales para mejorar procesos 
                 industriales y de manufactura.
               Internet of 
               Services (IoS)
               ✔ Se refiere a un marketplace global de aplicaciones 
                 y software basado en internet que se ofrecen 
                 como servicios.
               ✔ IoS incluye los servicios de Blockchain que es 
                 importante en cadenas de suministros.
               ✔ IoT facilitado por IoS crea innovación disruptiva.
Internet of Data 
(IoD)
✔ IoD se fundamenta en los billones de datos 
extensivos generados por dispositivos del IoT
✔ Estos volúmenes de información tienen el 
potencial para generar $ cuando las técnicas 
Analíticas de Big Data descubren patrones en 
ellos.
Internet of 
People (IoP)
✔ Un nuevo paradigma del internet donde los 
humanos y sus dispositivos personales no son 
considerados como usuarios finales sino como 
elementos activos.
✔ IoP se desarrolla de forma descentralizada como 
Blockchain donde los datos no están bajo el 
control de una sola entidad.
✔ El lema del IoP es ser un internet “de y para las 
personas” en vez de ser uno donde se sacrifica su 
privacidad.
Características de la 
Industria 4.0
                                Características 
                                de la Industria 
                                La Industria 4.0 se caracteriza por tener nueve 
                                características principales:
                                4.0
                                 ✔ Big Data
                                 ✔ Simulación
                                 ✔ Internet of Services
                                 ✔ Realidad aumentada
                                 ✔ Sistemas ciber-físicos
                                 ✔ Manufactura automática
                                 ✔ IoT (Internet of Things)
                                 ✔ Sistemas de cómputo en la nube
                                 ✔ Sistemas de robótica autónomos 
Fuente: Tay, S. et al. (2018). An Overview of Industry 4.0: Definition, 
Components, and Government Initiatives
Características 
de la Industria 
✔ Industria 1.0: Mecanización del trabajo que solía 
4.0
realizarse de forma manual. 
✔ Industria 2.0: Transformación que se produjo 
con la introducción de la electricidad en procesos. 
✔ Industria 3.0: Llegada de la informática y la 
automatización a la escena industrial. (Uso de 
controladores lógicos programables, robots, etc).
✔ Industria 4.0: Era de Cyber Physical Systems 
(CPS). Máquinas inteligentes, sistemas de 
almacenamiento e instalaciones de producción 
capaces de intercambiar información de forma 
autónoma, 
Fuente: Tay, S. et al. (2018). An Overview of Industry 4.0: Definition, 
Components, and Government Initiatives
Transformación 
digital
 La transformación digital es el fomento de 
 evolución y nuevos modelos de 
 negocio incorporando la digitalización 
 de archivos e incorporando lo digital a 
 todas las áreas de negocio.
                                                                     -    Harvard Business Review, 2021
                              Near Field Communications        Cloud                  IA
             Blockchai                  (NFC)
                n
               Realidad virtual       Robótica            Ways of working        Tecnologí
                                                                                     a 
                                                                                   móvil
Transformación Digital
La transformación digital es el proceso          Deseamos ver mejoras dramáticas en el 
mediante el cual se realizan cambios             desempeño y cambiar las rutas para 
integrales en la estrategia, modelos             lograr el éxito.
operativos, personas, cultura y procesos.        Las amenazas para las organizaciones 
                                                hoy en día son más resistentes y más 
Problemas nuevos, exigen soluciones              robustas. 
innovadoras ������
            Como muestra el siguiente cuadro, existe un movimiento constante 
                               en el top de empresas líderes…
       Top 10 Compañías 2021           Top 10 Compañías 2011            Top 10 Compañías 2001 
              Wallmart                        Wallmart                       Exxon Mobil
             State Grid                     Exxon Mobil                       Wallmart
              Amazon                          Chevron                       General Motors
      China National Petroleum             CoconoPhillips                     Ford Motor
        Sinopec Group China                  Fannie Mae                    General Electric
               Apple                       General Electric                   Citigroup
             CVS Health                  Berkshire Hathaway                     Enron
         UnitedHealth Group                General Motors               Intl. Business Machines
       Toyota Motor Company               Bank of America.                      AT&T
            Volkswagen                       Ford Motor                Verizon Communications
Fuente: https://fortune.com/global500/search/
¿Por qué datos y por 
qué ahora?
¿Por qué datos 
y por qué 
Según Forbes (2021) “Ahora más que nunca, los 
datos, la analítica y la experiencia son de 
ahora?
importancia existencial. No es simplemente una 
cuestión de opinión. Se ha convertido en un asunto 
público y social de vida o muerte.”
Los datos impulsan las decisiones que tomamos y 
los riesgos que asumimos hoy día.
               ¿Por qué datos y 
               por qué ahora?
               En contexto de Pandemia, por ejemplo, las respuestas 
               a preguntas como: 
               ¿Abrimos negocios y escuelas? ¿Abrimos 
               establecimientos públicos? ¿Qué nos dicen los 
               datos? ¿La curva sube o se aplana? ¿Se volverá a 
               emplear a la gente? ¿Volverá el negocio? 
               Solo se pueden responder con el uso de datos.
Tendencias en Data & 
Analytics 2021-2022
  ✔ IA más inteligente, rápida y                ✔ Choques entre mundos de datos y 
      responsable                                   analytics
  ✔ Decisión Intelligence                       ✔ Data Marketplaces e Intercambios
  ✔ X Analytics: Data No Estructurada           ✔ Blockchain en data & analytics
  ✔ Gestión de datos aumentada                  ✔ MLOps (Machine Learning 
  ✔ Cloud es una realidad                           Operations)
                                                ✔ Modelos de lenguaje avanzados (e.j. 
                                                    BERT)
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Ciclo de vida de un
proyecto de Ciencia 
de Datos
Ciclo de vida de un 
proyecto 
de Ciencia de datos
         Momento 1: Definir el objetivo
         Momento 2: Recolección de la data
         Momento 3: Preparar la data
         Momento 4: Elección del Algoritmo
Ciclo de vida de un 
proyecto 
de Ciencia de datos
         Momento 5: Entrenar el modelo
         Momento 6: Validación del modelo
         Momento 7: Deployment del modelo
               1.Definir el 
                 objetivo
               Es vital entender el problema a resolver y cuáles son 
               nuestros objetivos dado las características de la 
               empresa, así como de la data que tendremos a 
               disposición. 
               Las siguientes preguntas son típicas en esta etapa:
               ✔ ¿Qué exactamente deseamos hacer?
               ✔ ¿Cómo exactamente podremos hacerlo?
               ✔ ¿Es posible lo que deseo dada la data que tengo?
2. Recolección 
de data
✔ Data First Party: Data propia de la empresa 
(ERP,CRM,BD, etc).
✔ Data Second Party: Suele ser data que comparte 
una organización con sus aliados estratégicos.
✔ Data Third Party: Datos de tercero que podemos 
obtener ya sea de forma gratuita o incurriendo en 
algún tipo de costo asociado.
               3. Preparar la 
               data
               Normalmente lo conocemos como la limpieza de los 
               datos o el formateo del dato.
               El objetivo de esta etapa es manipular y convertir la 
               data en formas que produzcan mejores resultados. 
               Algunos ejemplos serían:
               Eliminar o inferir datos perdidos, categorizar los 
               valores de las variables, normalizar los valores 
               numéricos o escalarlos para que puedan ser 
               comparables.
4. Elección del 
Algoritmo
Una vez que ya hemos preprocesado la data, nos 
corresponde elegir el algoritmo más adecuado en 
relación al problema que deseamos resolver.
En este punto tenemos que decidir por el Tipo de 
Aprendizaje que vamos a implementar. 
En las próximas clases de: Modelos Analíticos para 
Ciencia de Datos II y III se abordarán los Tipos de 
Aprendizaje en ML con mucho mayor detalle y 
profundidad.
               5. Entrenar el 
               modelo
               Este paso tiene una relación directa con conceptos que 
               abordaremos más adelante en el curso (Training y 
               Test). Sin embargo, el proceso de entrenamiento de un 
               modelo de ML, consiste en proporcionarle al modelo 
               datos de entrenamiento de los cuales pueda aprender.
6. Validar el 
modelo
Se realizará con la data de validación y procederemos 
a ¨correr” el algoritmo y a evaluar los resultados 
obtenidos. 
En el caso de que los resultados no sean satisfactorios, 
deberemos volver a la etapa 5 hasta que nuestro 
modelo se ajuste bien a las dos particiones (data de 
entrenamiento y data de validación).
                     7. Deployment 
                     del modelo
                     Implementación en producción de nuestro modelo. 
                     Generalmente, solemos ayudarnos de la nube a través 
                     de los tres vendors más conocidos que existen 
                     actualmente: 
                      ✔ AWS
                      ✔ Azure 
                      ✔ GCP
Ejemplo en vivo
¡Vamos a trabajar con datos! 
¿Qué conceptos de Data Science conocen o 
han escuchado nombrar?
 ¿Qué conceptos han 
 escuchado antes?
Por encuestas de Zoom:
✔Exploratory Data         ✔SQL 
Analysis                ✔Estadística 
✔Data Wrangling             inferencial
✔Minería de datos         ✔Machine Learning
✔Limpieza de datos        ✔Deep Learning
                      ✔Inteligencia 
                        artificial
¡Vamos a trabajar 
con datos!
                 Miremos en el notebook Clase 1.ipynb 
                 cómo crear un gráfico igual a este.
Valor y retorno de
la ciencia de Datos
Valor y retorno de la 
Ciencia de Datos
Así como cualquier inversión, la                 ¿Que queda del valor generado después 
factibilidad de un proyecto de ciencia de        de los costos se contabiliza el costo de los 
datos ocurre cuando genera más valor             errores?
que costos. Para esto se puede utilizar el 
índice ROI (Return of Investment)
Valor y retorno de la 
Ciencia de Datos
Retorno= valor -(1-accuracy)*Costo del         Accuracy: Métrica de performance del 
error                                          modelo.
                                              Costo del error: Costos adicionales por 
Retorno: Lo que se genera o el profit por      un error (e.g nos demoramos 10 min 
cada predicción.                               corrigiendo un error en el sistema)
Valor: El valor generado por cada 
predicción (e.g antes demoramos 5 
minutos en obtener una predicción 
manual,  con el algoritmo ahora toma 
0.01 s)
Valor y retorno de la 
Ciencia de Datos
Si hacemos que el Retorno =0 entonces:      Ejemplo: Si cada predicción ahorra 5 min 
                                           de trabajo pero arreglar errores te cuesta 
0= valor -(1-accuracy)*Costo del error      20 min el Break Even Accuracy sería: 1- 
accuracy= 1- (valor/Costo del error)        (5/20) = 0.75 (75%)
A esto se le conoce como Break Even         Tu modelo debe tener al menos 75% de 
Accuracy                                    accuracy para que valga la pena usarlo
Valor y retorno de la 
Ciencia de Datos
Ejemplo: Si tenemos un algoritmo que        Si estamos hablando de un call center 
tiene un accuracy de 78%, donde cada        que puede recibir 10000 llamadas al mes 
predicción ahorra 5 min de trabajo y        ahorramos 467 horas de trabajo manual 
arreglar errores cuesta 10 min, entonces:   ������Nada mal ������
Retorno= valor -(1-accuracy)*Costo del 
error
Retorno = 5- (1-0.78)*10=2.8 
Entonces, usar el algoritmo nos ahorra 2.8 
minutos (168 seg) de trabajo.
Estrategia
data-driven
               Data Driven
REEMPLAZAR      Es una disciplina que utiliza diversas técnicas y 
POR IMAGEN      herramientas de análisis para aprovechar los datos 
               generados dentro de un ámbito o empresa para su 
               beneficio y el de sus clientes.
               Dicho de otro modo, se trata de sacar valor a los 
               millones de datos de los que hoy disponemos para 
               tomar mejores decisiones basadas en ellos.
Organización Data-Driven
   Datos        Refinamiento      Uso
 Organización Data-Driven
           Datos                    Refinamiento                       Uso
    Los clientes compran        Un producto de datos es        Un producto de datos 
     el producto de datos            una aplicación            es información digital 
     una vez y continúan         informática que toma         que se puede comprar 
      usándolo tal como           entradas de datos y                 y usar.
            está.                   genera salidas, 
                               devolviéndole al entorno.
Datos
Refinamiento
Usos
Ejemplo de caso de uso
                                             ✔ Estrategia: Análisis exploratorio de 
                                                datos, metodología de agrupamiento 
                                                (clustering), obtención de 
Detección de fraude en una compañía de           conclusiones
póliza de seguros:                            ✔ Funcionamiento: La información 
Su tarea es responder si existen                 recolectada por parte de la 
patrones particulares en los grupos              aseguradora es el insumo del 
de reclamos presentados que puedan               algoritmo de segmentación que 
ser indicativos de fraude.                       permitirá detectar fraude en posibles 
                                                reclamos de los usuarios
                                             ✔ Oportunidades: Combinar con datos 
                                                de ventas e información 
                                                sociodemográfica para ofrecer 
                                                nuevos productos de pólizas para los 
                                                clientes de la empresa
Objetivos específicos:
                                         ✔ Identificar posibles grupos 
✔ Comprender el comportamiento de           problemáticos posiblemente 
   los reclamos por parte de los            asociados a eventos fraudulento.
   usuarios.                             ✔ Brindar ofertas promocionales, 
✔ Identificar grupos de acuerdo a su        cupones y ofertas en pólizas a 
   condición sociodemográfica.              diferentes usuarios.
                                         ✔ Actualizar servicios.
Datos y análisis del caso:
Fuentes de Datos: Es donde podemos 
capturar la información, que puede ser de 
tres tipos: estructurada, semi-
estructurada o no estructurada. 
Tipo de Análisis: Es la metodología que 
usamos para resolver el problema. nos 
podemos preguntar por: Descriptivo 
(¿Qué pasó?), Diagnóstico (¿Por qué 
paso?),Predictivo (¿Qué pasará?), 
Prescriptivo (¿Cómo hacer que suceda?)
Actividad colaborativa
Optimizando el stock para una PYME
Ayudamos a optimizar el stock de nuestra 
heladería de barrio
Realizaremos la actividad en la sala 
general.
Duración: 15-20 minutos
    ACTIVIDAD COLABORATIVA
Optimizando el stock 
para una PYME
Consigna: En la heladería de Pedro se          ✔ ¿Cómo piensan que esta información 
lleva mucho tiempo trabajando sin ningún           puede ayudar a tener un mejor 
tipo de estrategia enfocada al uso de los          control del stock de la heladería?
datos como oportunidad de mejora y             ✔ ¿Qué nivel de madurez tendría esta 
manejo de stocks.                                  empresa según el modelo Data 
Recientemente, debido a la crisis                  Management Maturity Model?
sanitaria, el dueño ha cambiado su 
perspectiva y piensa que su empresa 
debería hacer un mejor uso de los datos 
históricos recolectados.
CLASE N°1
Glosario                                               Ciclo de vida de un proyecto de DS: 7 fases 
                                                       que describen cómo resolver un problema 
Industria 4.0: que permiten utilizar el poder de       analitico (Definir objetivo, Recolectar datos, 
las máquinas inteligentes y el análisis en tiempo      Limpiar y preparar datos, Elección de algoritmo, 
real por medio de la interacción con sistemas          Evaluación de algoritmos y Despliegue)
ciber-físicos                                          Valor de retorno de un proyecto de DS: 
IoT (Internet of Things): conectividad entre           costo (tiempo o $) que se pierde o gana con la 
dispositivos a la Internet                             implementación de cualquier algoritmo en un 
                                                       proceso productivo
IoS (Internet of Services): conectividad entre         Estrategia Data Driven: uso de técnicas y 
dispositivos a escala industrial                       herramientas para mejorar la toma de decisiones 
IoD (Internet of Data): manejo de volúmenes            con el fin de extraer el valor de los datos
de datos gigantes interconectados                      Transformación digital: uso de tecnología en 
IoP (Internet of People): interacciones mucho          todas las áreas del negocio para mejorar 
más eficientes entre personas                          productividad.
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
  MATERIAL AMPLIADO
Recursos multimedia
✓ Industria 4.0: Fabricando el futuro | 
  Unión industrial Argentina, BID e 
  INTAL
✓ Ciclo de vida de un proyecto de Data
   Science
   | Analytics Vidhya
✓ Transformación Digital | Salesforce
Disponible en nuestro repositorio.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓  Cuarta Revolución Industrial
      ✓  El Ambiente de la Industria 4.0
      ✓ Transformación Digital
      ✓ Ciclo de vida de un proyecto de ciencia de datos
      ✓ Valor y retorno de la Ciencia de Datos
     Tu Proyecto final
      #CoderTip: Ingresa a la carpeta de Google Drive de 
       esta clase y revisa el documento de Proyectos 
      finales. Allí podrás encontrar referencias que te serán 
        útiles al momento de crear tu trabajo original.
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 10. DATA SCIENCE
    Herramientas 
   de visualización
Temario
               09                      10                     11
          Estadística           Herramientas           Procesamiento 
          Descriptiva          de Visualización        estadísticos de 
                                                          los datos
           ✓ Introducción        ✓ Introducción         ✓ Procesamiento 
           ✓ Medidas de          ✓ Gráficos                como concepto
              resumen               Univariados
                                                        ✓ Outliers 
           ✓ Distribución de     ✓ Gráficos 
              variables             Bivariados          ✓ Datos ausentes
                                 ✓ Forma de los         ✓ Reducción de 
           ✓ Intervalos de          datos
              confianza                                    dimensionalidad
                                 ✓ Gráficos de dos 
                                    dimensiones
Objetivos de la clase
         Conocer la relevancia de la visualización de 
         datos.
         Identificar cómo lograr una comunicación 
         efectiva.  
         Comprender un análisis básico, las relaciones 
         entre elementos y la distribución.
         Conocer gráficos en más de dos dimensiones.
      MAPA DE CONCEPTOS
                                                 Herramientas 
                                                 de 
                                                 visualización
     Visualización de                           Relaciones entre      Análisis de           Gráficos de más 
     datos                Análisis básico       elementos             distribuciones        de dos 
                                                                                            dimensiones
     Introducción         Bar chart             Series de tiempo      Entendimiento         Más de dos
     Elementos            Histogramas           Diagramas de          Boxplot               Más de tres
                                                Dispersión
                                                Correlación                                 Otras formas
                                                                                            Radar
                                                                                            Chernoff
Repasemos con…
Introducción a tipos 
de gráficos 
estadísticos y 
buenas prácticas de 
 uso
Introducción
     Los datos son un 
               arma 
         de doble filo
  Para hacer el bien         Desinformar y engañar
      Tenemos que ser fuertemente responsables
 “Con un gran poder viene una 
 gran responsabilidad”
                                                                                                -    Peter Parker
¿Cómo pensar con 
responsabilidad si lo único
que estoy haciendo es 
analizar datos?
 ✔ La respuesta está en la salida del          ✔ Pueden comunicarse en forma 
    proceso > Son fundamentalmente                 directa o utilizarse como entrada de 
    insumos para la toma de decisiones.            algoritmos y desarrollos de ciencia 
                                                   de datos.
Comunicación de 
resultados
✔ Prácticas y herramientas de la            Para lo que sigue, tengamos presente lo 
    visualización de datos.                 visto en la clase anterior, dado que hay 
✔ Preparación detallada para la             una conexión muy estrecha entre la 
    presentación efectiva de la             definición de una variable y su 
    información recolectada.                visualización.
✔ Un gráfico siempre debe decir 
    mucho con una economía de 
    elementos.
Propósitos de un 
gráfico
Propósitos de 
un gráfico
✔ Los gráficos suelen tener un mayor impacto                    REEMPLAZAR 
    que las tablas                                               POR IMAGEN
En las tablas es difícil enfatizar:
a) Tendencias
b) Patrones
c) Diferencias
Siempre existirá la preferencia por presentar la 
información por medio de gráficos.
Tipos de gráficos
Tipos de 
Visualizaciones Univariadas
gráficos
✔ Barras                                              REEMPLAZAR 
✔ Histogramas                                         POR IMAGEN
✔ Líneas
Visualizaciones Bivariadas
✔ Diagramas de dispersión
✔ Boxplots
✔ Barras y boxplots múltiples           Ejemplo de boxplots múltiples que permiten comparar 
                                                categorías en diversos niveles
Elementos de un 
gráfico
Características de un 
gráfico
 ✔ Un título entendible, claro y conciso        ✔ Debe transmitir un mensaje claro
    para lograr comunicar efectivamente         ✔ Debe resaltar puntos importantes
 ✔ Etiquetas en ejes (x,y,z) entendibles        ✔
    y acordes con unidades de medición              Buen formato de estilo (balance de 
    si fuera el caso                                colores, no recargada)
 ✔ Una nota que indique: a) fuente de 
    datos y b) descripción adicional de 
    ser necesaria suele ayudar a un 
    mejor entendimiento
Ejemplo
       12
       10
         8
         6                                                                            Racing 
         4                                                                            Independiente
                                                                                      Vélez
         2
         0
                Partidos         Goles             Goles          Puntos
                jugados       convertidos        recibidos
Gráficos
univariados
Gráficos de barras
Fundamentos
 ✔ Contar conjunto de datos.                      ✔ Eje      horizontal    representa     las 
                                                      categorías  y  una  barra  por  cada 
 ✔ Mostrar         valores      puntuales             una de ellas.
     asociados a una categoría.
                                                  ✔ Eje       vertical    representa       la 
 ✔ Se utilizan con datos categóricos.                 cantidad o valor de los elementos 
                                                      de la categoría en cuestión.
               Ejemplo
REEMPLAZAR      Tenemos las puntuaciones de 5 equipos, del E1 al E5, 
POR IMAGEN      separados por categorías, nombradas como Categoría 
               1 en color azul y Categoría 2 en color anaranjado.
               Recomendación: No usar gráficos de barras para 
               representar datos en el tiempo porque las relaciones y 
               tendencias son más difíciles de visualizar. ¡Más 
               adelante lo veremos con series de tiempo!
Ejemplo
  ✔ Carguemos el siguiente dataset que tiene 
      información de los pasajeros que abordaron el 
      Titanic.                                                          REEMPLAZAR 
  ✔ Hacemos un conteo por Pclass y realizamos un                        POR IMAGEN
      gráfico de barras comparativo de frecuencias.
  import seaborn as sns
  import pandas as pd
  df=pd.read_csv("https://raw.githubusercontent.com/ven-27/datasets/
  master/titanic.csv")
  df_n=df[['Survived','Sex','Pclass']].groupby(by=['Sex','Pclass']).count
  ().reset_index()
  df_n.head()
  plt.figure(figsize=(10,6))
  sns.barplot(y='Survived',x='Sex',hue='Pclass',data=df_n);
Ejemplo
Miremos un ejemplo con una librería llamada Bokeh que permite generar gráficos 
interactivos.
df_x=df[['Survived','Pclass']].groupby(by=['Pclass']).count().reset_index()
df_x['Pclass']=df_x['Pclass'].astype('str')
from bokeh.palettes import Spectral6
from bokeh.io import show, output_notebook
from bokeh.models import CategoricalColorMapper, ColumnDataSource, FactorRange
from bokeh.plotting import figure
source = ColumnDataSource(data=dict(Pclass=df_x.Pclass, counts=df_x.Survived, color=Spectral6))
p = figure(x_range=df_x.Pclass, plot_height=250, toolbar_location=None, title="Clases involucradas")
p.vbar(x='Pclass', top='counts', width=0.9, color='color', legend="Pclass", source=source)
p.xgrid.grid_line_color = None;p.xaxis.axis_label = "Clase"
p.yaxis.axis_label = "Frecuencia"
p.legend.orientation = "horizontal"
p.legend.location = "top_center";p.add_tools(HoverTool())
show(p)
Ejemplo
Las gráficas obtenidas en esta librería son de alta calidad con la posibilidad de interactuar 
de manera dinámica con el usuario por lo que se usa mucho en el desarrollo de Dashboards
  Para pensar
¿En cuál de los siguientes casos podrías 
usar un gráfico de barras?
✔ Medición de salario mínimo/máximo en 5 países de Latinoamérica. 
✔ Medición de salario mínimo en 5 países de Latinoamérica, con una 
comparación trimestral. 
✔ Evolución del salario mínimo en 5 países de Latinoamérica en el último 
semestre. 
Contesta mediante la encuesta de Zoom 
Histogramas
 Fundamentos
   ✔ Se        utiliza     usualmente          para         ✔ Muestran una distribución de datos 
       variables numéricas continuas                             (Tendencia  central  y  Simetría  vs 
   ✔ Muestran  las  frecuencias  de                              Asimetría).
       aparición para cada intervalo de                     ✔ Similares  a  los  gráficos  de  barras, 
       valores de la variable.                                   pero     muy  diferentes  en  su 
                                                                 comportamiento.
Ejemplo
Qué:      Consideremos        los    datos      Cómo: se toman todos los valores de la 
correspondientes  a  las  alturas  de  500      variable, desde el mínimo al máximo y se 
personas,    medidas     en   centímetros.      divide el rango correspondiente en 
Organizaremos  las  mediciones  de  altura      intervalos de igual tamaño. 
disponibles    en    intervalos,   también      Generalmente, los lenguajes de 
llamados bins.                                  programación ya proveen un valor 
                                               calculado.
               !  Un  histograma  muestra  la  importancia  de  cada 
               intervalo de valores con respecto al total de datos.
        Histograma de frecuencias           Histograma de frecuencias 
                absolutas                            relativas
¿Qué nos muestran?
En el eje horizontal, se muestra la               La cantidad de cada intervalo se divide 
variable a ser analizada.                         por el total de observaciones.
Cada intervalo de valores de la variable          En nuestros casos de uso, la forma es la 
tiene una altura que representa la                misma, lo que cambia es la interpretación 
cantidad de observaciones en dicho                de los resultados: en el primero hablamos 
intervalo.                                        de cantidad de observaciones para cada 
Esta cantidad puede ser:                          intervalo de la variable, en el segundo 
 ✔ ¿Cuántas unidades hay para cada               hablamos de proporción de veces de cada 
     intervalo de la variable?                   intervalo de la variable con respecto al 
 ✔ ¿Cuánto del total representa cada             total de observaciones de la variable.
     intervalo de la variable?
Ejemplo
import seaborn as sns
sns.histplot(data=df, x="Fare",bins=20)
sns.histplot(data=df, x="Age",bins=20)
                            Miremos  un  poco  la  distribución  de  las 
                            variables Fare (Tarifa) y Age (Edad) para 
                            los datos
                           Para el caso de Fare se tiene una distribución 
                           asimétrica a derecha con valores entre 0-500$. 
                           Por el contrario la distribución de edad tiende a 
                           ser más normal con menor asimetría con valores 
                           entre 0-80 años.
Ejemplo
Ahora analicemos un ejemplo con la librería Bokeh
import numpy as np
from bokeh.io import show, output_file
from bokeh.plotting import figure
data = df.Age.values
data=data[~np.isnan(data)]
hist, edges = np.histogram(data, density=True, bins=20)
p = figure(plot_height=300,plot_width=500, title='Hitograma de 
edades')
p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], 
line_color="white")
p.xaxis.axis_label = "Edad"
p.yaxis.axis_label = "Frecuencia"
p.add_tools(HoverTool())
#output_file("hist.html")
show(p)
Ejemplo
import numpy as np
from bokeh.io import show, output_file
from bokeh.plotting import figure
data = df.Fare.values
data=data[~np.isnan(data)]
hist, edges = np.histogram(data, density=True, bins=20)
p = figure(plot_height=300,plot_width=500, title='Histograma de 
tarifas')
p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], 
line_color="white")
p.xaxis.axis_label = "Tarifa (USD)"
p.yaxis.axis_label = "Frecuencia"
p.add_tools(HoverTool())
#output_file("hist.html")
show(p)
Series de tiempo
PARA RECORDAR
Para tener en cuenta…
Siempre que tengamos una variable que 
tiene algún tipo de evolución a través 
del tiempo, o donde organizar sus 
valores a lo largo del tiempo tiene 
sentido para el análisis, podemos 
graficar una serie de tiempo.
PARA RECORDAR
Para tener en cuenta…
Para este tipo de gráfico dibujaremos un 
punto cuya altura representará el valor de 
la variable y cuya posición con respecto al 
eje horizontal representará el momento en 
el tiempo en el que se mide esa variable. 
Luego, uniremos los puntos en forma 
secuencial.
¿Qué nos muestra?
Las líneas que conectan los puntos nos         ������Recomendación: usar este tipo de 
ayudan a percibir rápidamente si existe        gráfico para mostrar la evolución de una 
alguna tendencia, repetición de valores o      serie de datos, realizarla siempre con 
cualquier otra característica.                 puntos y no con barras, y que el tiempo 
                                              esté siempre en el eje horizontal, 
                                              avanzando de izquierda a derecha.
Ejemplo
Utilicemos datos estimados para el precio de Bitcoin y grafiquemos la serie de tiempo de los 
datos para precio de cierre
data=pd.read_csv('Time Series Data.csv') # Leer el archivo
price_date=data['Date'] # extraer dos vectores uno para la 
fecha
price_close=data['Close'] # extraer el precio de cierre en 
vector
plt.plot_date(price_date, price_close, linestyle='solid') # 
Graficar la serie de tiempo
plt.gcf().autofmt_xdate() # Darle formato fecha al eje x
plt.title('Precios Bitcoin')
plt.xlabel('Fecha')
plt.ylabel('Precio de cierre')
plt.tight_layout() # Ajustar los ejes
plt.show() # mostrar la grafica
from bokeh.models import DatetimeTickFormatter
data.Date= pd.to_datetime(data.Date) # Convertir a Datetime
line_plot = figure(plot_width=600, plot_height=500, 
title='Line plot',x_axis_label='Fecha', 
y_axis_label='Precio',x_axis_type='datetime')
line_plot.line(data.Date, data['Open'], legend='Open', 
line_width=2)
line_plot.line(data.Date, data['High'], legend='High', 
line_width=2,color='red')
line_plot.line(data.Date, data['Low'], legend='Low', 
line_width=2,color='green')
line_plot.line(data.Date, data['Close'], legend='Close', 
line_width=2,color='yellow')
line_plot.xaxis.formatter=DatetimeTickFormatter(
   hours=["%d %B %Y"],days=["%d %B %Y"],
   months=["%d %B %Y"],years=["%d %B %Y"])
line_plot.add_tools(HoverTool())
line_plot.legend.location = "top_left"; show(line_plot)
Gráficos
bivariados
Diagramas de 
dispersión
Fundamentos
✔ Se  utilizan   para   observar  en      ✔ Hay que tener mucho cuidado a la 
   conjunto  la  relación  entre  dos        hora  de  expresar  conclusiones 
   variables.                                en forma de relaciones simples y 
✔ Se  colocan  los  puntos  en  dos          no  en  vínculos  de  tipo  causa-
   variables  y  pueden  analizarse  las     efecto.
   relaciones entre las mismas.           ✔ Es importante tener en cuenta que 
                                             cuando hablamos de relación no 
                                             estamos                 hablando 
                                             necesariamente de dependencia 
                                             de una variable con respecto a 
                                             la otra.
¡La dependencia es objeto de estudio del análisis de Regresión, que veremos más adelante!
Ejemplo
Consideremos el caso de la Clase 5 acerca de los 
pesos en kilogramos y alturas en centímetros para                   REEMPLAZAR 
un conjunto de 50 alumnos.                                           POR IMAGEN
✔ Cada punto representa un alumno. Para cada 
    alumno podemos ver la intersección del eje 
    horizontal, que indica las alturas, con el eje 
    vertical, que indica los pesos.
✔ A mayores alturas corresponden mayores 
    pesos y viceversa.
Correlación
Fundamentos
                                                  ✔ Si el valor de la correlación es 
                                                      cercano a 1, significa que cuando los 
                                                      valores de una variable son altos, los 
Si la relación tiene alguna forma definida             de la otra variable también lo son, y 
será considerada como correlación entre                análogamente con los valores bajos. 
las variables. Comúnmente hablaremos               ✔ Si el valor de la correlación es 
de correlación lineal, que es la más                   cercano a -1 significa que cuando los 
práctica a efectos del análisis en Data                valores de una variable son bajos, 
Science.                                               los valores de la otra variable son 
                                                      altos, y análogamente con los 
La correlación lineal puede medirse a                  valores altos.
través de un indicador denominado 
coeficiente de correlación. Puede tener            ✔ Si el valor de la correlación es 
valores entre -1 y 1, y lo interpretamos de            cercano a 0, significa que no hay 
la siguiente manera:                                   una correlación lineal fuerte entre 
                                                      las variables.
 Ejemplos
  El coeficiente de correlación de 0.97 es cercano a 1            El coeficiente de correlación de -0.98 es cercano 
        y hablamos de correlación positiva                         a -1 y hablamos de correlación negativa.
               Ejemplo
 REEMPLAZAR     El coeficiente de correlación de 0.12 es cercano a 0 y 
 POR IMAGEN     hablamos de correlación nula o ausencia de 
                correlación.
Ejemplo
✔ Utilicemos datos estimados para el precio de Bitcoin y grafiquemos la serie de 
tiempo de los datos para precio de cierre
sns.set_style('whitegrid')
plt.figure(figsize=(10,6))
sns.scatterplot(x=data.Close, y= data.Open)
plt.title('Relacion entre Precio apertura y cierre')
plt.xlabel('Precio de Cierre')
plt.ylabel('Precio de apertura')
Parece existir una relación lineal entre ambas variables, 
cuando calculamos la correlación de Pearson entre las 
dos  variables  nos  da:  0.646  lo  cual  es  una  relación 
moderada
Ejemplo
Ahora analicemos un ejemplo con la librería Bokeh
# Seleccion de datos
x_scatter = data.Close # Data en x
y_scatter = data.Open # data en y
# Grafico (Fondo)
scatter_plot = figure(plot_width=700, plot_height=300, 
x_axis_label='Close', y_axis_label='Open', title='Dispersion 
Open vs Close')
# Scatter plot
scatter_plot.circle(x_scatter, y_scatter, size=5, 
line_color='navy', fill_color='orange', fill_alpha=0.5)
# Agregar opcion interactiva
scatter_plot.add_tools(HoverTool())
# Mostrar
show(scatter_plot)
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Entender la forma
de los datos
¿Qué buscamos cuando 
analizamos 
la forma de los datos?
La distribución, ¿es simétrica o                  El histograma da una idea acerca de la 
asimétrica?                                       simetría de la distribución. También da 
¿En qué parte de la distribución se ubica         una noción acerca de la concentración 
la mayor concentración de datos?                  de los datos.
Los datos, ¿están más bien dispersos o 
concentrados en torno a algún valor?
¿Puede ser que existan datos demasiado 
alejados del resto?
Diagramas de caja y 
bigotes
Boxplot
Ejemplo
¡Es muy simple en su diseño pero muy 
poderoso por toda la información 
importante que contiene para poder 
conocer la distribución de los datos!
Elementos
✔ La línea anaranjada que está dentro        ✔ También significa que el 25% de los 
    de la caja representa la mediana de          valores está por debajo del límite 
    la distribución.                             inferior de la caja, y el 25% de los 
✔ El borde inferior de la caja marca el          valores está por encima del límite 
    valor del cuartil 1 (25%) y el borde         superior de la caja.
    superior de la caja marca el valor del 
    cuartil 3 (75%). Otra forma de decir 
    esto es que el 50% de la 
    concentración más “central” de los 
    datos está delimitado por la caja.
Elementos
 ✔ Outliers:    son  valores  “demasiado”          Los outliers, si existieran, están 
    alejados  de  la  masa  central  de 
    datos. Si los valores no son outliers,         representados  por  los  círculos 
    o  sea  que  están  “más  cerca”  del          ubicados fuera de los bigotes.
    centro     de    los    datos,     los 
    denominaremos inliers.
 ✔ Bigotes: representan los valores de 
    los  últimos  inliers,  esto  es,  los 
    últimos  valores  que  están  alejados 
    del  centro  de  los  datos  pero  que 
    todavía no son outliers.
Ejemplo
                                                         plt.figure(figsize=(10,6))
 plt.figure(figsize=(10,6))                              sns.boxplot(x=df.Sex, y= df.Fare, 
 sns.boxplot(x=df.Sex, y= df.Fare, hue=df.Embarked)      hue=df.Embarked,showfliers=False)
 plt.title('Boxplot comparativo Genero vs Tarifa')       plt.title('Boxplot comparativo Genero vs Tarifa')
 plt.xlabel('Genero')                                    plt.xlabel('Genero')
 plt.ylabel('Tarifa')                                    plt.ylabel('Tarifa')
 Ejemplo
                                                                    Ahora analicemos un ejemplo con la 
   groups = df[['Sex','Fare']].groupby('Sex')                       librería Bokeh
   q1 = groups.quantile(q=0.25)
   q2 = groups.quantile(q=0.5)
   q3 = groups.quantile(q=0.75)
   iqr = q3 - q1                                                   # stems
   upper = q3 + 1.5*iqr                                            p.segment(cats, upper.Fare, cats, q3.Fare, 
   lower = q1 - 1.5*iqr                                            line_color="black")
   # Encontrar outliers en cada categoria                          p.segment(cats, lower.Fare, cats, q1.Fare, 
   def outliers(group):                                            line_color="black")
      cat = group.name                                             # boxes
      return group[(group.Fare > upper.loc[cat]['Fare']) |         p.vbar(cats, 0.7, q2.Fare, q3.Fare, 
   (group.Fare < lower.loc[cat]['Fare'])]['Fare']                  fill_color="#E08E79", line_color="black")
   out = groups.apply(outliers).dropna()                           p.vbar(cats, 0.7, q1.Fare, q2.Fare, 
   # Preparar outliers para plot                                   fill_color="#3B8686", line_color="black")
   if not out.empty:                                               # whiskers
      outx = list(out.index.get_level_values(0))                   p.rect(cats, lower.Fare, 0.2, 0.01, line_color="black")
      outy = list(out.values)                                      p.rect(cats, upper.Fare, 0.2, 0.01, line_color="black")
Ejemplo
# outliers
if not out.empty:
p.circle(outx, outy, size=6, color="#F38630", 
fill_alpha=0.6)
#p.xgrid.grid_line_color = None
p.ygrid.grid_line_color = "white"
p.grid.grid_line_width = 2
p.xaxis.major_label_text_font_size="16px"
p.xaxis.axis_label ='Genero'
p.yaxis.axis_label='Fare'
p.add_tools(HoverTool())
show(p)
Gráficos para ver 
más de dos 
dimensiones
               Más de dos 
               dimensiones
REEMPLAZAR      Cuando tenemos que comparar más de dos 
POR IMAGEN      variables estamos en un problema. Esto se da 
               porque un gráfico sobre el papel o la pantalla no puede 
               mostrar más de dos variables o dimensiones. Para 
               trabajar en ellas podemos echar mano de la 
               perspectiva o de otras características visuales que 
               permiten analizar los datos.
Ejemplo
En este caso las variables x, y y z están ubicadas en los                  REEMPLAZAR 
bordes de un cubo, y los puntos están ubicados en el                        POR IMAGEN
espacio interior del cubo. Cada punto representa la 
intersección de los valores de las variables x, y y 
z.
               Más de tres 
               dimensiones
               Cuando tenemos un dato multidimensional se pueden 
               utilizar gráficos como los de Chernov (1973) que 
               representa los atributos de la instancia por medio de 
               un rostro.  También podemos utilizar los gráficos de 
               radar 
Para pensar
Puede emplearse un gráfico de dispersión 
que contemple las variables: edad - ingreso 
- país
¿Verdadero o falso?
Contesta mediante el chat de Zoom 
¿Qué pasa si 
tenemos datos 
en más 
Aquí tenemos que apelar al ingenio y a las 
dimensiones?
características que permiten que nuestro cerebro 
“piense” en más dimensiones.
Gráfico de Radar
✔ Surge para mostrar observaciones         ✔ Luego se grafica el valor 
   en varias variables, y tratar de           correspondiente a cada variable en 
   compararlas visualmente.                   el eje, y se traza el área delimitada 
✔ Se dibuja un radar o “telaraña”,            por los puntos.
   donde cada eje representa una           ✔ Las observaciones similares tendrán 
   variable.                                  áreas parecidas, y análogamente 
                                              con las observaciones diferentes.
               Ejemplo
               Podríamos decir que los grupos A y C tienen las áreas 
               más grandes, por lo que sus variables tienen en 
               general mayores valores. Además, es claro que el 
               grupo A es muy diferente del grupo B, porque sus 
               valores se destacan en distintas variables (1, 2 y 5 para 
               el grupo A; 3 y 4 para el grupo B).
Caras de Chernoff
  ✔ Se basan en el mismo principio que                ✔ Las  caras  con  expresiones 
      los  gráficos  de  radar,  pero  en  este            similares      tienen       elementos 
      caso apelan al funcionamiento de                     similares,  por  lo  tanto  los  valores 
      nuestro cerebro y cómo reacciona                     de las variables son similares.
      frente al reconocimiento de rostros.            ✔ Son  muy  útiles,  pero  solamente 
  ✔ Asocian  cada  variable  con  una                      cuando las observaciones no son 
      característica  de  la  cara:  forma                 demasiadas.
      del  rostro,  forma  de  la  nariz, 
      expresión de las cejas, expresión de 
      la boca, forma de las orejas, etc. 
Ejemplo
Cuadrícula de 25 observaciones que muestra caras en 
forma simplificada para cada observación. Las 
expresiones pueden inferirse claramente: hay caras 
“tristes”, caras “aburridas” y caras “enojadas”, por 
ejemplo.
Ejemplo en vivo
Se han preguntado ¿cómo hacer gráficos 
en más de 3 dimensiones entendibles?
Utilizaremos en notebook Clase_10.ipynb 
para entender cómo se pueden generar 
gráficos en 1, 2, 3 o más dimensiones.
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
 MATERIAL AMPLIADO
Recursos multimedia
✓ The visual display of quantitative inf
   ormation
    | Edward Tufte
Disponible en nuestro repositorio.
CLASE N°10
Glosario
 Elementos de un buen gráfico: i) título, ii)       Cuartiles: medidas de localización 
 etiquetas en los ejes y unidades si                que permiten entender el 
 corresponde, iii) puntos importantes deben         comportamiento de la distribución 
 resaltar, iv) buen estilo.                         de valores numéricos para una 
 Paradigma de los gráficos: un buen gráfico         variable (e.g Q1 valor que deja el 
 puede ayudar a transmitir un mensaje o idea        25% de los datos por de bajo) 
 cuando se elige correctamente pero en caso         Gráficos de radar: permiten ver las 
 contrario puede desinformar y generar              relaciones multivariadas de diversos 
 imprecisiones                                      grupos o individuos limitados en 
                                                    tamaño
                                                          
CLASE N°10
Glosario
  Tipos de gráficos: pueden ser univariados 
  (e.g. lineplots), bivariados (e.g diagrama de         Gráficos de Chernov: 
  dispersión) o multivariados (e.gráficos de            representaciones multivariadas de 
  radar)                                                individuos por medio de caras 
  Correlación: medida de asociación entre dos           similares a un humano donde cada 
  variables cuantitativas usualmente                    atributo facial se asocia a diferentes 
  (coeficiente de pearson), es una medida               variables de un individuo
  acotada entre -1 y 1 donde 0 indica ausencia 
  de correlación lineal, 1 perfecta asociación 
  lineal y -1 relación inversamente proporcional              
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Visualización de datos
      ✓ Bar chart. Histogramas
      ✓ Series de tiempo. Dispersión. Correlación
      ✓ Boxplot
      ✓ Gráficos de más de dos variables
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 26. DATA SCIENCE
       Lenguaje 
   estructurado de 
consulta SQL - Parte 
            2
Temario
               25                      26                     27
           Lenguaje                Lenguaje           Data Acquisition 
         estructurado            estructurado                  II
             SQL-I                   SQL-II
         ✓ Introducción         ✓ Consultas de mayor     ✓ Intro a Github
                                   complejidad           ✓
                                                            Javascript object 
         ✓ DDL                  ✓ TCL                       notation
         ✓ DML                  ✓ Otros tipos de         ✓ Introducción APIs
                                   comandos
                                                         ✓ Conexión a 
                                ✓ Joins, agregaciones       modelos 
                                   y agrupaciones           relacionales 
                                                            usando Pandas
Objetivos de la clase
         Conocer las sentencias para la gestión de 
         transacciones (TCL)
         Conocer otros tipos de comandos y cómo 
         utilizarlos
         Realizar consultas más complejas: joins, 
         agregaciones y agrupaciones
MAPA DE CONCEPTOS              DDL
                              DML
                             Consultas 
                             complejas
Lenguaje 
estructurado de 
Consulta                        TCL
                                         Parte II
                           Otros comandos
                              Joins, 
                            Agrupaciones y 
                            Agregaciones
Cuestionario de tarea
¿Te gustaría comprobar tus 
conocimientos de la clase anterior?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
Consultas de mayor 
complejidad
 WHERE
WHERE
1. La cláusula WHERE se utiliza para filtrar 
registros.
2. Se utiliza para extraer solo aquellos registros 
que cumplen una condición específica.
      EJEMPLO EN VIVO
 WHERE ejemplos
     SELECT something FROM some table          Estructura
        WHERE some conditions are satisfied
     UPDATE some table SET something
        WHERE some conditions are satisfied
                Ejemplo 1                          Ejemplo 3
         SELECT * FROM  Customers          SELECT * FROM  Products 
         WHERE  Country = ‘Mexico’;        WHERE  Price BETWEEN 50 AND 60
               Ejemplo 2                           Ejemplo 4
         SELECT * FROM  Products           SELECT * FROM  Customers 
         WHERE  Price = 18;                WHERE  City LIKE ‘s%’ OR ‘%a’
AND, OR, NOT
AND, OR, NOT
La cláusula WHERE se puede combinar con los 
operadores AND, OR y NOT.
1. El operador AND muestra un registro si todas 
las condiciones separadas por AND son 
VERDADERAS.
2. El operador OR muestra un registro si alguna 
de las condiciones separadas por OR es 
VERDADERA.
3. El operador NOT muestra un registro si la(s) 
condición(es) NO ES VERDADERA.
             EJEMPLO EN VIVO
 AND, OR, NOT ejemplos
                SELECT column1, column2, ...
                FROM table_name                                                 Estructura 
                WHERE condition1 AND condition2 AND 
                condition3 ...;                                                 AND
                SELECT column1, column2, ...
                FROM table_name                                                  Estructura 
                WHERE condition1 OR condition2 OR condition3                     OR
                ...; 
                SELECT column1, column2, ...                                     Estructura 
                FROM table_name                                                  NOT
                WHERE NOT condition1 
           EJEMPLO EN VIVO
 AND, OR, NOT ejemplos
                 Ejemplo 1                                          Ejemplo 4
       SELECT * FROM  Customers                           SELECT * FROM  Customers 
       WHERE  City = ‘Berlin’ OR City = ‘Munchen’         WHERE NOT  Country = ‘Germany’ 
                 Ejemplo 2                                          Ejemplo 5
       SELECT * FROM  Customers                           SELECT * FROM  Customers 
       WHERE  Country = ‘Germany’ AND City =              WHERE Country = ‘Germany’ AND (City = 
       ‘Berlin’                                           ‘Berlin’ OR City = ‘Munchen’)
                 Ejemplo 3                                          Ejemplo 6
       SELECT * FROM  Customers                           SELECT * FROM  Customers 
       WHERE  Country = ‘Germany’ OR Country =            WHERE NOT Country = ‘Germany’ AND NOT 
       ‘Spain’                                            Country = ‘USA’ 
SELECT TOP
SELECT TOP
1. La cláusula SELECT TOP se usa para especificar 
el número de registros a devolver.
2. Es útil en tablas grandes con miles de registros 
si no se quiere devolver toda la cantidad de 
registros después del filtro
      EJEMPLO EN VIVO
SELECT TOP ejemplos
       SELECT column_name(s)
       FROM table_name                            Estructura
       WHERE condition
       LIMIT number; 
              Ejemplo 1                          Ejemplo 3
       SELECT * FROM  Customers          SELECT TOP 3 * FROM  Customers 
       LIMIT 3;                          WHERE  Country = ‘Germany’
             Ejemplo 2                           Ejemplo 4
       SELECT TOP 50 PERCENT * FROM      SELECT * FROM  Customers 
       Customers                         WHERE  Country =‘Germany’ 
                                         LIMIT 3 
 LIKE
LIKE
El operador LIKE se usa en una cláusula WHERE para 
buscar un patrón específico en una columna.
Existen dos formas comunes de usarlo:
1. El signo de porcentaje (%) representa cero, uno 
o varios caracteres
2. El signo de subrayado (_) representa un solo 
carácter
      EJEMPLO EN VIVO
LIKE ejemplos
     SELECT column1, column2,...               Estructura
     FROM table_name
     WHERE columnN
     LIKE pattern; 
                Ejemplo 1                          Ejemplo 3
         SELECT * FROM  Customers          SELECT  * FROM  Customers 
         WHERE CustomerName LIKE ‘a%’;     WHERE CustomerName LIKE ‘%or%’;
               Ejemplo 2                           Ejemplo 4
         SELECT  * FROM  Customers         SELECT  * FROM  Customers 
         WHERE CustomerName LIKE ‘%a’;     WHERE CustomerName LIKE ‘_r%’;
ALIASES
ALIASES
1. Los alias de SQL se utilizan para dar a una 
tabla, o a una columna de una tabla, un 
nombre temporal.
2. Se utilizan a menudo para hacer que los 
nombres de las columnas sean más legibles.
3. Un alias solo existe mientras dure esa consulta.
4. Se crea un alias con la palabra clave AS.
      EJEMPLO EN VIVO
ALIASES ejemplos
     SELECT column1. AS alias_name
     FROM table_name;                           Estructura
     SELECT column_name(s)
     FROM table_name AS aliasname
           Ejemplo 1                          Ejemplo 3
                                     SELECT  CustomerName, Address + ‘,  ’ + 
    SELECT CustomerID AS ID,         PostalCode + ‘    ’ + City +  ‘,  ’ + Country 
    CustomerName AS Customer         AS Address FROM Customers
    FROM Customers;
                                              Ejemplo 4
          Ejemplo 2
                                      SELECT o.OrderID, o.OrderDat, 
    SELECT CustomerName AS            c.CustomerName FROM  Customers 
    Customer,  ContactName AS         AS c, Orders AS o 
    [Contact Person]                  WHERE c.CustomerName=’Around the 
    FROM Customers;                   Horn’ AND c.CustomerID= o.CustomerID;
COMMENTS
COMMENTS
Los comentarios se utilizan para explicar secciones 
de instrucciones SQL o para evitar la ejecución de 
instrucciones SQL.
1. Los comentarios de una sola línea comienzan 
con --. Cualquier texto entre -- y el final de la 
línea será ignorado (no será ejecutado).
2. Los comentarios de varias líneas comienzan 
con /* y terminan con */. Cualquier texto 
entre /* y */ será ignorado.
      EJEMPLO EN VIVO
ALIASES ejemplos
     -- Comentario corto:
     SELECT * FROM table_name                   Estructura
     /* Comentario largo
     de muchas líneas */
     SELECT * FROM tablen_name:
            Ejemplo 1                       Ejemplo 3
                                    /* Comentario largo de control de cambios 
     -- SELECT * FROM Customers     para regulacion */ 
     SELECT * FROM Products;        SELECT * FROM Customers 
          Ejemplo 2                         Ejemplo 4
     SELECT * FROM Customers --     /* SELECT Comentario largo de control de 
     WHERE City = ‘Berlin’          cambios para regulacion */ 
                                    SELECT * FROM Customers 
CASE-END
CASE-END
1. Utiliza condiciones dadas y devuelve un valor 
cuando se cumple la primera condición (similar 
al if-then-else). 
2. Una vez que una condición es verdadera, 
devolverá el resultado. Si ninguna condición es 
verdadera, devuelve el valor de la cláusula 
ELSE.
3. Si no hay parte ELSE y ninguna condición es 
verdadera, devuelve NULL.
          EJEMPLO EN VIVO
CASE-END ejemplos
      CASE 
      WHEN condition1 THEN result1
      WHEN condition2 THEN result2
      WHEN conditionN THEN resultN                        Estructura
      ELSE result
      END
                      Ejemplo 1                                      Ejemplo 2
        SELECT OrderID, Quantity                        SELECT CustomerName, City, Country
        CASE                                            FROM  Customers
          WHEN Quantity > 30 THEN ‘Mayor30’             ORDER BY 
          WHEN Quantity = 30 THEN ‘Igual30’             (CASE 
          ELSE ‘Menor30’                                     WHEN City  IS NULL THEN Country
        END AS  TextoCantidad                                ELSE City
        FROM OrderDetails                               END)
Lenguaje de control 
de transacciones 
(TCL)
PARA RECORDAR
Las sentencias INSERT, UPDATE y DELETE que 
hemos visto realizan modificaciones en la 
base de datos. Cuando se ejecutan no se 
pueden revertir para obtener datos 
anteriores.
Esto hace que no se resguarde la base 
de datos ante posibles fallos que 
puedan ocurrir cuando se ejecutan 
operaciones pudiendo generar 
inconsistencias en los datos.
Consistencia de 
transacciones
              10 mil pesos
   Esteban                   Matías
      Consideremos como ejemplo la base de datos de un banco
Consistencia de 
transacciones
✔ Para el ejemplo tendríamos que ejecutar dos operaciones UPDATE en los 
registros de una misma tabla, para lo cual se deben realizar dos sentencias 
SQL distintas: una que disminuya el saldo de Matías y otra que aumente el 
saldo de Esteban. 
✔ Ahora bien, ¿Qué sucedería si, por algún fallo en el servidor, se ejecuta 
la primera sentencia UPDATE pero la segunda no? 
Transacciones
Transacciones
✔ Son una secuencia de operaciones realizadas 
(por medio de uno o mas SQL querys) en una 
base de datos bajo una unidad lógica de 
trabajo
✔ Los efectos de todas las sentencias SQL en una 
transacción pueden confirmarse (aplicarse a la 
base de datos) o revertirse (deshacerse de la 
base de datos).
Transacciones
Para indicar el comienzo de una transacción, se debe ejecutar 
START TRANSACTION;
A  continuación,  todas  las  sentencias  SQL  que  se  ejecuten 
quedarán  grabadas  temporalmente  pero  no  se  realizarán 
modificaciones físicas en la base de datos hasta que el usuario 
confirme la transacción.
Para confirmar una transacción exitosa se ejecuta  COMMIT;
¡Finalmente se pueden escribir de manera definitiva los cambios 
          realizados!
Transacciones
  Si en medio de transacción ocurre un fallo, el usuario puede 
  abortar la transacción y volver al estado previo ejecutando 
  ROLLBACK;
La gran mayoría de los sistemas de bases de datos 
soportan el manejo de transacciones, sin embargo, 
lamentablemente nuestra base de datos de ejemplo de 
W3Schools tiene deshabilitada este tipo de operaciones.
 EJEMPLO EN VIVO
Transacciones ejemplo
En  primer  lugar  se  deberá  iniciar  la  transacción  con 
START TRANSACTION;
Enseguida se deberá registrar la orden ������
INSERT  INTO  Orders  (CustomerID,  EmployeeID, 
OrderDate, ShipperID)
VALUES (15, 5, '2021-01-01', 2);
 EJEMPLO EN VIVO
Transacciones ejemplo
Hasta este momento, la nueva orden se encontraría escrita 
de manera temporal en la tabla virtual. Si la herramienta lo 
permitiese,  sería  posible  verificar  esto  realizando  un 
SELECT sobre la tabla Orders.
SELECT   *    FROM    Orders 
WHERE OrderDate = '2021-01-01';
������ fila recientemente insertada.
 EJEMPLO EN VIVO
Transacciones ejemplo
Si en este momento se aborta la transacción ejecutando 
ROLLBACK;
Se  vería  que  el  INSERT  no  se  ha  escrito  de  manera 
definitiva,  dado que al volver a consultar la tabla con la 
sentencia, no retornaría ningún resultado.
                  EJEMPLO EN VIVO
   Transacciones ejemplo
     Para simular un registro completo de la venta, habría que ejecutar el siguiente 
     conjunto de sentencias finalizando con la confirmación de la transacción:
     START TRANSACTION;
     INSERT INTO Orders (CustomerID, EmployeeID, OrderDate, ShipperID)
     VALUES (15, 5, '2021-01-01', 2);
     INSERT INTO OrderDetails (OrderID, ProductID, Quantity)
     VALUES 
              (10444, 11, 5),
              (10444, 12, 10),
              (10444, 2, 80);
     COMMIT;
 EJEMPLO EN VIVO
Transacciones ejemplo
Tras ejecutar COMMIT, ambas inserciones quedarían asentadas de manera definitiva en la 
base de datos y a partir de aquí se podría verificar esto ejecutando:
SELECT o.OrderID, 
o.OrderDate, 
od.OrderDetailID, 
od.ProductID, 
od.Quantity
FROM Orders AS o
JOIN OrderDetails AS od
ON o.OrderID = od.OrderID
WHERE o.OrderDate = '2021-01-01';
         ☕
       Break
     ¡10 minutos y 
     volvemos!
          ¡Lanzamos la
          Bolsa de 
          Empleos!
         Un espacio para seguir potenciando tu carrera y 
         que tengas más oportunidades de inserción 
         laboral.
         Podrás encontrar la Bolsa de Empleos en el menú 
         izquierdo de la plataforma.
         Te invitamos a conocerla y ¡postularte a tu futuro 
         trabajo!
           Conócela
Otros tipos de 
comandos 
 IN
IN
1. El operador IN le permite especificar varios 
valores en una cláusula WHERE para filtrado.
2. El operador IN es una abreviatura de múltiples 
condiciones OR.
            EJEMPLO EN VIVO
  IN ejemplos
           SELECT column_names(s) 
           FROM some table                                             Estructura
           WHERE column_name  IN (value1, value2,...)
                           Ejemplo 1                                         Ejemplo 3
      SELECT * FROM  Customers                                   SELECT * FROM  Customers 
      WHERE  Country IN (‘Germany’,’France’,’UK’);               WHERE  Country IN 
                                                                 (SELECT Country FROM Suppliers) 
                         Ejemplo 2
      SELECT * FROM  Customers 
      WHERE  Country NOT IN (‘Germany’,’France’,’UK’);
 BETWEEN
BETWEEN
1. El operador IN le permite especificar varios 
valores en una cláusula WHERE.
2. El operador IN es una abreviatura de múltiples 
condiciones OR.
      EJEMPLO EN VIVO
BETWEEN ejemplos
    SELECT column_names(s)                     Estructura
    FROM some table
    WHERE column_name  BETWEEN value1 AND value2
                                                   Ejemplo 3
                Ejemplo 1                  SELECT * FROM  Customers 
 SELECT * FROM  Customers                  WHERE  Price BETWEEN  10 AND 20
 WHERE  Price BETWEEN  10 AND 20;          AND Category NOT IN (1,2,3) 
               Ejemplo 2                           Ejemplo 4
 SELECT * FROM  Customers                  SELECT * FROM  Products 
 WHERE  Price  NOT BETWEEN  10 AND 20;     WHERE ProductName BETWEEN  
                                           ‘Carnarvon Tigers’ AND ‘Mozzarella 
                                           di Diobanni’
                                           ORDER BY ProductName
   ANY
ANY
1. El operador ANY devuelve un valor booleano 
como resultado VERDADERO si ALGUNO de lo 
valores de la subconsulta cumple la condición
2. ANY significa que la condición será verdadera si 
la operación es verdadera para cualquiera de 
los valores en el rango
      EJEMPLO EN VIVO
ANY ejemplos
     SELECT column_names(s) 
     FROM some table                           Estructura
     WHERE column_name operator  ANY 
     (SELECT column_name FROM table_name 
     WHERE condition);
            Ejemplo 1                             Ejemplo 2
                                          SELECT ProductName 
  SELECT ProductName                      FROM  Products
  FROM  Products                          WHERE ProductID = ANY  
  WHERE ProductID = ANY                     (SELECT ProductID 
    (SELECT ProductID                        FROM OrderDetails
     FROM OrderDetails                       WHERE Quantity >1000)
     WHERE Quantity >99)
 ALL
ALL
1. El operador ALL devuelve un valor booleano 
como resultado devuelve VERDADERO si 
TODOS los valores de la subconsulta cumplen 
la condición se usa con sentencias SELECT, 
WHERE y HAVING
2. Significa que la condición será verdadera solo 
si la operación es verdadera para todos los 
valores en el rango.
      EJEMPLO EN VIVO
ALL ejemplos
     SELECT column_names(s) 
     FROM some table                            Estructura
     WHERE column_name operator  ALL 
     (SELECT column_name FROM table_name 
     WHERE condition);
           Ejemplo 1                              Ejemplo 2
  SELECT ProductName                      SELECT ProductName 
  FROM  Products                          FROM  Products
  WHERE ProductID = ALL                   WHERE ProductID = ALL 
    (SELECT ProductID                       (SELECT ProductID 
     FROM OrderDetails                       FROM OrderDetails
     WHERE Quantity =10)                     WHERE Quantity <30)
Joins, Agregaciones 
y Agrupaciones
JOINS
JOINS
Una cláusula JOIN se usa para combinar filas de dos o más 
tablas, en función de una columna relacionada entre ellas. 
Existen 4 tipos comunes (INNER, LEFT, RIGHT, FULL)
INNER, LEFT JOIN
INNER JOIN selecciona registros que tienen valores 
coincidentes en ambas tablas.
LEFT JOIN devuelve todos los registros de la tabla de la 
izquierda (tabla1) y los registros coincidentes de la 
tabla de la derecha (tabla2).
              EJEMPLO EN VIVO
  INNER, LEFT JOIN ejemplos
                   Estructura INNER JOIN                                    Estructura LEFT JOIN
         SELECT column_name(s)                                        SELECT column_name(s) 
         FROM table1                                                  FROM table1
         INNER JOIN  table2                                           LEFT JOIN  table2
         ON table1.column_name = table2.column_name;                  ON table1.column_name = table2.column_name;
                        Ejemplo 1                                                Ejemplo 2
               SELECT Customers.CustomerName,                         SELECT Customers.CustomerName, 
               Orders.OrderID                                         Orders.OrderID
               FROM  Orders                                           FROM  Customers
               INNER JOIN  Customers ON                               LEFT JOIN  Orders ON 
               Orders.CustomerID=                                     Customers.CustomerID= 
               Customers.CustomerID                                   Orders.CustomerID 
               ORDER BY Customers.CustomerName                        ORDER BY Customers.CustomerName
RIGHT, FULL JOIN
LEFT devuelve todos los registros de la tabla de la 
derecha (tabla2) y los registros coincidentes de la tabla 
de la izquierda (tabla1).
FULL OUTER JOIN devuelve todos los registros cuando 
hay una coincidencia en los registros de la tabla 
izquierda (tabla1) o derecha (tabla2).
              EJEMPLO EN VIVO
  RIGHT, FULL JOIN ejemplos
               Estructura RIGHT                                         Estructura FULL JOIN
               JOIN
      SELECT column_name(s)                                        SELECT column_name(s) 
      FROM table1                                                  FROM table1
      RIGHT JOIN  table2                                           FULL OUTER JOIN  table2
      ON table1.column_name = table2.column_name;                  ON table1.column_name = table2.column_name;
      WHEREcondition                                               WHEREcondition
                    Ejemplo 1                                                 Ejemplo 2
           SELECT Orders.OrderID,                                 SELECT Customers.CustomerName, 
           Employees.LastName,                                    Orders.OrderID
           Employees.FirstName                                    FROM  Customers
           FROM  Orders                                           INNER JOIN  Orders ON 
           RIGHT JOIN  Employees ON                               Customers.CustomerID= 
           Orders.EmployeeID=                                     Orders.CustomerID 
           Employees.EmployeeID                                   ORDER BY Customers.CustomerName
           ORDER BY Orders.OrderID
GROUP BY
GROUP BY
1. Agrupa filas que tienen los mismos valores en 
filas de resumen, como "encontrar la cantidad 
de clientes en cada país".
2. Se usa a menudo con funciones agregadas 
(COUNT(), MAX(), MIN(), SUM(), AVG()) para 
agrupar el conjunto de resultados por una o 
más columnas.
       EJEMPLO EN VIVO
GROUP BY ejemplos
      SELECT column_names(s)                        Estructura
      FROM some table
      WHERE condition  
      GROUP BY column_name(s) 
      ORDER BY column_name(s);
              Ejemplo 1                                Ejemplo 2
                                              SELECT COUNT (CustomerID), 
    SELECT COUNT (CustomerID)                 Country
    FROM  Customers                           FROM  Customers
    GROUP BY Country                          GROUP BY Country
    DESC                                      ORDER BY COUNT (CustomerID)
                                              DESC
HAVING
HAVING
1. La cláusula HAVING se agregó a SQL porque la 
palabra clave WHERE no se puede usar con 
funciones agregadas.
2. Se usa en combinación con la cláusula GROUP 
BY para restringir los grupos de filas devueltas 
a solo aquellas cuya condición es VERDADERA.
       EJEMPLO EN VIVO
HAVING ejemplos
      SELECT column_names(s) 
      FROM some table                                Estructura
      WHERE condition  
      HAVING  condition
      ORDER BY column_name(s);
             Ejemplo 1                                  Ejemplo 2
                                               SELECT COUNT (CustomerID), 
   SELECT COUNT (CustomerID), Country          Country
   FROM  Customers                             FROM  Customers
   GROUP BY Country                            GROUP BY Country
   HAVING COUNT  (CustomerID)>5                HAVING COUNT (CustomerID) >5
                                               DESC
MIN, MAX
MIN, MAX
1. La función MIN() devuelve el valor más 
pequeño de la columna seleccionada.
2. La función MAX() devuelve el valor más grande 
de la columna seleccionada.
      EJEMPLO EN VIVO
MIN, MAX ejemplos
           Estructura MIN                     Estructura MAX
     SELECT MIN (column_name)             SELECT MAX (column_name)
     FROM table_name                      FROM table_name
     WHERE  condition                     WHERE  condition
              Ejemplo 1                          Ejemplo 2
        SELECT MIN (Price)                SELECT MAX (Price)
        AS SmallestPrice                  AS SmallestPrice
        FROM  Products                    FROM  Products
COUNT, AVG, SUM
COUNT, AVG, SUM
1. La función COUNT() devuelve el número de filas 
que coinciden con un criterio específico.
2. La función AVG() devuelve el valor promedio de 
una columna numérica.
3. La función SUM() devuelve la suma total de una 
columna numérica.
      EJEMPLO EN VIVO
COUNT,AVG, SUM ejemplos
         Estructura COUNT                   Estructura AVG
   SELECT COUNT (column_name)           SELECT AVG (column_name)
   FROM table_name                      FROM table_name
   WHERE  condition                     WHERE  condition
            Ejemplo 1                          Ejemplo 2
       SELECT COUNT (Price)             SELECT AVG (Price)
       AS SumPrice, COUNT (ProductID)   AS SumPrice, COUNT (ProductID)
       FROM  Products                   FROM  Products
   Actividad colaborativa
SQL con joins
Utilizaremos la base de datos en este enlace para responder las siguientes 
preguntas:
1. Selecciona todos los pedidos con información del cliente y del remitente 
(Pueden hacer uso de las tablas Orders y Customers, la columnas que 
permite el JOIN son CustomerID y ShipperID)
2. Obtener el número de pedidos enviados por cada remitente (Pueden hacer 
uso de las tablas Shippers y Orders, el JOIN se realiza por medio de la 
columna ShipperID)
Duración: 10-15 minutos. Grupos 3-4 personas
  SQL con JOINS
Resolveremos un problema real utilizando Joins y 
    funciones de agrupación
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
SQL con JOINS
Utilizaremos la base de datos en este enlace 
1. Selecciona todos los pedidos con información del 
cliente y del remitente (Pueden hacer uso de las 
tablas Orders y Customers, la columnas que 
permite el JOIN son CustomerID y ShipperID)
2. Obtener el número de pedidos enviados por cada 
remitente (Pueden hacer uso de las tablas 
Shippers y Orders, el JOIN se realiza por medio de 
la columna ShipperID)
CLASE N°26
Glosario
                                        Otros tipos de comandos: incluyen por 
TCL: son comandos (COMMIT,,               ejemplo cláusulas como IN, BETWEEN, ANY, 
ROLLBACK) que permiten manejar            ALL que permiten hacer filtros de tipo 
transacciones en una base de datos        booleano a la hora de obtener resultados 
relacional.                               de una consulta SQL
Consultas de mayor complejidad:           JOINS: se utilizan para combinar los 
aquellas donde se utiliza clausulas AND,  resultados de una tabla con otras teniendo 
OR, NOT, WHERE, SELECT TOP, LIKE,         en cuenta una columna en común
ALIASING, COMMENTS o CASE-END con el 
fin de obtener un resultado de utilidad   Agrupaciones y agregaciones: cláusulas 
según el caso                             que permiten agrupar por categorías y 
                                        obtener medida de resumen de las 
Transacción: secuencia de operaciones     variables deseadas
desarrolladas (con uno o más comandos 
SQL)
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Consultas de mayor complejidad
      ✓ TCL
      ✓ Conocer otros tipos de comandos y cómo 
        utilizarlos
      ✓ Realizar consultas más complejas: Joins, 
        agregaciones y agrupaciones
Opina y valora 
esta clase
Muchas 
gracias.
               Encuesta
               sobre esta clase
               Por encuestas de Zoom
               ¡Terminamos la clase! 
               Cuéntanos qué temas te resultaron más complejos de 
               entender. Puedes elegir más de uno. Vamos a 
               retomar aquellos temas que resultaron de mayor 
               dificultad en el próximo AfterClass.
Esta clase va a ser
grabad
  a
      Clase 39. DATA SCIENCE
En foco: selección y 
 mejora de modelos
Temario
             38                    39                    40
         Workshop:              En foco...        Introducción a
        Revisión de                                  Machine 
           pares                                     Learning
        ✓ Revisión de         ✓ Data Acquisition  ✓  Marco CRISP-DM y la fase 
                                                     de Machine Learning y 
           pares              ✓ Data Wrangling       modelado
        ✓ Repaso:             ✓ EDA               ✓  Scikit Learn
           Narrativa de                           ✓  Técnicas de Encoding
           presentación       ✓ Análisis 
                                 estadístico      ✓  Feature Engineering
           de datos + tips
                                                  ✓  Flujo de trabajo 
Objetivos de la clase
         Repasar las fases fundamentales en los 
         procesos de Data Acquisition y Data Wrangling
         Repasar los conceptos de Exploratory Data 
         Analysis y Estadística Descriptiva
         Proveer a los estudiantes un repaso de los 
         temas vistos en el curso Data Science II
MAPA DE CONCEPTOS
                     Repaso Data Science-II
  Data                           Análisis      Clasificación de 
 Acquisition   Data Wrangling   Estadístico      Análisis 
                                                Estadistico
 Definición      Definición      Definición     Definiciones
 Diferentes       Tareas,        Tipos de       Utilidades y 
 fuentes de     importancia y    análisis      como se aplican
  datos           fases         estadístico
Repaso: 
Data Acquisition
Definición
¿Qué es la Adquisición de 
datos?
✔ Se refiere a la recuperación de 
datos, lo cual implica decidir acerca 
de qué datos se requieren, por que 
y como para luego poder utilizarlos
✔ No existe una única forma de 
adquirir los datos, ya que se tienen 
muchas fuentes disponibles
✔ Los datos se pueden adquirir por 
medio de la organización misma 
(First Party), ser buscados de forma 
externa (Second Party) o 
comprados (Third Party)
Tipos de datos
Tipos de datos
✔ Estructurados: archivos que 
muestran configuración de filas y 
columnas con variables definidas. 
Pueden ser ordenados y procesados 
fácilmente con herramientas de 
minería de datos
✔ Semi-estructurados: Tienen 
características consistentes y definidas, 
no tienen una estructura rígida como 
en el esquema relacional
✔ No estructurados: no tienen 
estructura relacional y pueden venir 
con información dispersa y compleja de 
procesar. 
Fuente: Lawtomated
Lectura de datos con 
Pandas
Lectura de datos con 
Pandas
Recordemos que con Pandas podíamos 
leer archivos con múltiples orígenes:
✔ Github
✔ De manera local
✔ Archivos planos (csv, txt)
✔ Archivos en formato excel
✔ Bases de datos SQL (SQLLite, SQL 
Alchemy, PostgreSQL)
✔ Datos desde APIs sin o con 
credenciales
✔ Datos desde páginas web como 
Yahoo y Google Trends
Git vs Github
Diferencias Git vs. Github
                          Git                                               Github
   Instalado localmente                                Hosteado en la nube
   Lanzado en 2005 y mantenido por The Linux           Lanzado en 2008 y mantenido por Microsoft
   Foundation
   Se enfoca en control de versiones y código          Se enfoca en un hosting de código centralizado
   compartido
   Se ejecuta en la terminal                           Administrado en la web
   Provee una interface desktop llamada Git Gui        Provee una interfaz desktop llamada Github Desktop
   Pocas configuraciones externas disponibles          Marketplace diverso para integración de 
                                                       herramientas
   Licencia Open Source                                Versión gratuita y también paga
Archivos JSON
1. Definición                            2. Estructura
Se trata de un formato de datos           ● Los datos se organizan en 
semi-estructurado no tabular, es              colecciones de objetos o 
decir que los registros no tienen que         paquetes.
tener un mismo conjunto de                ● Pueden “anidarse” → Los 
atributos.                                    valores pueden ser listas u otros 
                                            objetos.
Uso de APIs
1. Definición                      2. Para qué sirven
Es una interfaz que permite la      Son una manera de obtener datos 
comunicación entre dos              de una aplicación sin conocer los 
sistemas distintos permitiendo      detalles de la bases de datos.
agregar diversas funciones a 
sitios web y aplicaciones.
Para pensar
¿Recuerdan los tipos de negocios que se 
podrían implementar con APIs? ¿Para el 
caso de Netflix como ayudaban las APIs a 
mejorar la productividad del negocio? 
Contesta la encuesta de Zoom 
Repaso:
Data Wrangling
Definición
1. Definición                             2. Tareas 
                                         involucradas
Es  el  proceso  de  limpieza  y            ● Fusión  de  múltiples  fuentes  de 
unificación de conjuntos de datos               datos
complejos  y  desordenados  para            ● Identificar filas problemáticas
facilitar  el  acceso,    análisis  y       ● Eliminar datos que son innecesarios 
modelado.                                   ● Identificar atípicos y duplicados 
3. Importancia                               4. Etapas 
                                            involucradas
El Data Wrangling puede llevar mucho            1. Descubrimiento
tiempo y agotar los recursos,                   2. Estructuración
especialmente cuando se realiza                 3. Limpieza
manualmente. Esta es la razón por la que 
muchas organizaciones instituyen                4. Enriquecimiento
políticas y mejores prácticas que ayudan        5. Validación 
a los empleados a optimizar el proceso          6. Publicación
de limpieza de datos
Data transformation
1. Definición                            2. Tareas 
                                       involucradas
La transformación de datos es el          Procesos como:
proceso de cambiar el formato, la          1. Integración de datos
estructura o los valores de los            2. Migración de datos
datos.                                     3. Almacenamiento de datos y el Data 
                                            Munging
3. Beneficios                                 4. Desafíos
 1. Los datos se transforman para que 
     estén mejor organizados.                   1. La transformación de datos puede ser 
 2. Los datos correctamente formateados            costosa.. 
     y validados mejoran la calidad de los      2. Los procesos de transformación de 
     datos y protegen de problemas                 datos pueden consumir muchos 
 3. La transformación de datos facilita la         recursos. 
     compatibilidad entre aplicaciones,         3. La falta de experiencia y el descuido 
     sistemas y tipos de datos.                    pueden presentar problemas durante 
                                                   la transformación.
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Repaso:
Exploratory Data 
Analysis (EDA)
Definición
1. Definición                            2. Utilidades del 
                                        EDA
Tiene como finalidad examinar los         Permite dar respuesta a lo siguiente:  
datos previamente a la aplicación de      ✔ ¿Existe algún sesgo en los datos 
                                            recogidos?
cualquier técnica estadística, con el fin 
de conseguir un entendimiento básico      ✔  ¿Hay errores en la codificación de los 
                                            datos?
de sus datos                              ✔
                                             ¿Cómo se sintetiza y presenta la 
                                            información contenida en un conjunto de 
                                            datos?
                                         ✔  ¿Existen datos atípicos (outliers)? 
                                            ¿Cuáles son? ¿Cómo tratarlos?
                                         ✔  ¿Hay datos ausentes (missing)? 
                                            ¿Tienen algún patrón sistemático? 
                                            ¿Cómo tratarlos?
3. ¿Que realmente                           4. Etapas del EDA
hace?
El EDA, proporciona métodos sencillos         1. Preparar los datos
para  organizar  y  preparar  los  datos,     2. Realizar un examen gráfico
detectar fallos en el diseño y recogida       3. Analizar correlaciones y dependencias
de datos, tratamiento y evaluación de         4. Evaluar supuestos distribucionales
datos ausentes, identificación de casos       5. Identificar outliers
atípicos y mucho más.                         6. Establecer el impacto de datos 
                                                 problemáticos como outliers, ausentes 
                                                 y duplicados
        Identificando 
 fortalezas/debilidades de 
Investigaremos fortalezas y debilidades de un 
            EDA
     Exploratory Data Analysis (EDA)
        Duración: 15-20 min
ACTIVIDAD EN CLASE
Identificando 
fortalezas/debilidades de 
Descripción de la actividad. 
EDA
Se propone ir al siguiente enlace: Ejemplo EDA. Luego 
de esto responder:
1. Identificar fortalezas y debilidades del análisis 
descriptivo realizado. 
2. La forma en cómo se interpretan los resultados es 
apropiada 
3. Las conclusiones que se obtienen son acordes con 
las preguntas planteadas
4. ¿Qué agregarían o quitarían del análisis 
planteado?
Tiempo estimado 15-20 min
Repaso: 
Análisis Estadístico
Definición
¿Qué es el Análisis
estadístico?
 Es  la  ciencia  de  recopilar,  explorar  y 
 presentar grandes cantidades de datos para 
 descubrir     patrones      y     tendencias 
 subyacentes. 
 Las estadísticas se aplican todos los días, en 
 investigación,  industria  y  gobierno,  para 
 volverse    más     científicas  sobre    las 
 decisiones que deben tomarse. 
Análisis univariado
1. Definición                                  2. ¿Cómo se 
                                              aplica?
Consiste en el análisis de cada una de  Se puede aplicar con las siguientes acciones:  
las     variables      estudiadas       por    ✔ Cálculo de medidas de tendencia central 
separado,  se  basa  exclusivamente  en           (media, mediana, moda, media recortada)
una única variable.                            ✔ Cálculo de medidas de Localización (e.g 
Se   considera    un    análisis   de   tipo      cuartiles) y de dispersión (e.g varianza , 
                                                  std)
Descriptivo  y  no  relacional  o  de 
causalidad                                     ✔ Cálculo de medidas de asimetría y curtosis
                                               ✔ Desarrollo de gráficos resumen (e.g 
                                                  Lineplots, Histogramas, Barcharts, 
                                                  Boxplots, Piecharts, Radarplots, 
                                                  Violinplots)
                                               ✔ Generación de tablas de resumen 
                                                  numérico y de contingencia para el 
                                                  análisis 
Análisis bivariado
1. Definición                                 2. ¿Cómo se 
Es una de las formas más simples de  aplica?
                                              Debemos aplicar los siguientes pasos:  
análisis estadístico, que se utiliza para 
averiguar  si  existe  una  relación  entre   ✔ Observar la naturaleza de las posibles 
dos  conjuntos  de  valores.  Por  lo            relaciones
general involucra las variables X e Y.
                                              ✔ Identificar valores nominales, ordinales 
                                                 o de razón
                                              ✔ Evaluar la significancia de los 
                                                 resultados
                                              ✔ Determinar si existe relación entre las 
                                                 variables
3. Tipos                              4. Herramientas 
1. Variable      numérica       vs   Contamos con diferentes herramientas:  
  numérica                          ✔ Diagramas de dispersión (Scatterplots)
2. Variable     categórica      vs   ✔ Regresión y cuantificación de dependencia 
  categórica                           lineal
3. Variable      numérica      vs. 
  categórica                        ✔ Coeficientes de correlación (Pearson, Phi, 
                                       Rango Biserial, Biserial Puntual, 
                                       Spearman)
Análisis Multivariado
1. Definición                              2. ¿Cómo se 
                                          aplica?
Es  una  rama  de  la  estadística  que  Se aplica con una amplia gama de técnicas 
                                           donde buscamos cumplir estos objetivos:  
abarca  la  observación  y  el  análisis    ✔ Optimizar datos o simplificar la estructura
simultáneos de más de una variable  ✔
                                              Ordenar y agrupar los datos
respuesta.
                                           ✔ Investigar la relación de dependencia 
                                              entre variables
                                           ✔ Relación predictiva entre variables
                                           ✔ Construccion y validacion de pruebas de 
                                              hipótesis
                                           ✔ Existen diversos metodos como (PCR, 
                                              PLSR, ANN; SVM, LDA, KNN, SVM, PCA, K-
                                              means)
3. Ventajas                         4. Desventajas 
✔ Permite a los investigadores ver la 
                                   ✔ Sus técnicas son complejas, involucran 
  relación entre variables y cuantificar 
                                      matemáticas avanzadas y requieren 
  la relación entre ellas: 
                                      procedimientos estadísticos 
✔ Muestra capacidad de obtener una 
                                   ✔ Los    resultados  del   modelado 
  visión general más realista y precisa 
                                      estadístico  no  siempre  son  fáciles 
  que cuando se analiza una sola 
                                      de entender 
  variable. 
  Accediendo a notebooks 
       del Curso DS-II
Realizaremos una copia de todos los notebooks 
disponibles del Curso de DS-II en nuestras cuentas 
           personales
        Duración: 10-15 min
ACTIVIDAD EN CLASE
Accediendo a 
notebooks del Curso 
Descripción de la actividad. 
DS-II
Se propone que los estudiantes puedan descargar los 
archivos en formato .ipynb disponibles para cada clase, 
pueden hacer una copia en sus cuentas de google.
En la carpeta de clase encontrarán dos subcarpetas: 
una llamada Notebooks a repasar donde trabajaremos 
conceptos de Data Acquisition, Data Wrangling, EDA, 
Analisis univariado, bivariado y multivariado. En la 
subcarpeta Datos se encuentra la data para trabajar
Tiempo estimado 20-25min
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
 MATERIAL AMPLIADO
Recursos multimedia
Data Acquisition
✓ Data Acquisition | ACQ |      Estadística Descriptiva
 Data Acquisition              ✓ Definition of Descriptive Statistics | 
                                  Investopedia | 
Data Wrangling                       Descriptive Statistics
✓ Data Wrangling | Medium | 
 Introduction to Data wrangling
Exploratory Data Analysis
✓ EDA | Towards Data Science | 
 Exploratory Data Analysis
Disponible en nuestro repositorio.
¿Preguntas?
CLASE N°39
Glosario
                                              Análisis Univariado: consiste en el 
Data Acquisition: Se refiere a la              análisis de cada una de las variables 
recuperación de datos, lo cual implica         estudiadas por separado, se basa 
decidir acerca de qué datos se requieren,      exclusivamente en una única variable.
por que y como para luego poder 
utilizarlos                                    Análisis bivariado: Es una de las formas 
                                              más simples de análisis estadístico, que 
Data Wrangling: proceso de limpieza y          se utiliza para averiguar si existe una 
unificación de conjuntos de datos              relación entre dos conjuntos de valores. 
complejos para facilitar el acceso,  
análisis y modelado.                           Análisis multivariado: Es una rama de 
                                              la estadística que abarca la observación y 
EDA: examinar los datos previamente a          el análisis simultáneos de más de una 
la aplicación de cualquier técnica             variable respuesta.
estadística, con el fin de conseguir un 
entendimiento básico de sus datos
           Resumen 
       de la clase hoy
      ✓ Data Acquisition
      ✓ Data Wrangling
      ✓ Exploratory Data Analysis (EDA)
      ✓ Análisis univariado 
      ✓ Análisis bivariado 
      ✓ Análisis multivariado
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 38. DATA SCIENCE 
Workshop: Revisión 
       de pares
Temario
               37                       38                       39
           Análisis           Workshop: Revisión          Introducción a 
         Multivariado               de pares                 Machine 
                                                             Learning
                                  ✓ Revisión de           ✓ Data Acquisition
         ✓ Objetivos                 pares
                                                          ✓ Data Wrangling
         ✓ Ventajas y             ✓ Narrativa de 
            desventajas                                   ✓ EDA
                                     presentación 
         ✓ Integración con R y       de datos + tips      ✓ Análisis estadístico
            PowerBI
Objetivos de la clase
         Distinguir los procesos y tareas más 
         relevantes a la hora de hacer una revisión de 
         pares
         Proveer feedback a compañeros con el fin de 
         mejorar el proyecto final utilizando revisión de 
         pares
MAPA DE CONCEPTOS
                                       Revisión de 
                                         pares
     Workshop: 
    Revisión de 
       pares
                                        Narrativa 
                                      Presentación 
                                      de datos +tips
Cuestionario final
¿Te gustaría comprobar tus 
conocimientos de lo que va del curso?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
Revisión de pares
PARA RECORDAR
Revisión de pares
En la clase 8 vimos como se hacía el 
proceso de revisión de pares. Repasemos 
un poco acerca de los pasos a seguir y 
cómo poder utilizar esta instancia para 
mejorar la calidad de un proyecto de Data 
Science.
Definición
Proceso de revisión por 
pares
Es un proceso donde se indaga acerca de 
los posibles enfoques metodológicos para 
enmarcar y resolver el problema en 
cuestión, donde podemos formular 
nuestra recomendación sobre el mejor 
camino a seguir, complementando con 
una lista de las ventajas y riesgos 
inherentes a cada elección.
Puntos de control
Motivación
Se debe evitar que elijan el enfoque o la 
dirección equivocados en esta fase inicial del 
proyecto. Estos errores, denominados fallas de 
enfoque en el flujo de trabajo de DS, son muy 
costosos de cometer y obstaculizan el proyecto 
durante el resto de su ciclo de vida.
Si tenemos suerte, descubrimos la falla del 
enfoque después de varias iteraciones de 
desarrollo del modelo y perdemos preciosas 
semanas de trabajo como científico de datos.
Objetivos
Se deben detectar errores costosos desde el 
principio, analizando aspectos centrales del 
proceso.
Aquí se pueden establecer dos objetivos 
secundarios adicionales: primero, mejorar la 
capacidad del científico de datos para explicar y 
defender sus decisiones en el próximo proceso 
de revisión de productos/negocios. Segundo, 
prepararse mejor para presentar el resultado de 
la fase de investigación al resto del equipo, 
Estructura
1. Se analiza el flujo del proyecto por medio 
de una presentación (no necesariamente 
diapositivas) del proceso de investigación 
por el que pasó.
2. Se puede establecer una reunión larga (al 
menos 60 minutos) con el revisor.
3. El revisor revisa la lista de verificación 
(antes de la reunión).
Actividad colaborativa
Revisando un proyecto de DS
En esta oportunidad les proponemos ir al siguiente link:
Proyecto Prediccion Rentas Bicicletas revisar el archivo 
ProyectoFINAL_Leon.ipynb y la presentación. 
¿Qué sugerencias podríamos generar para mejorar el 
estado actual del proyecto? ¿Qué puntos 
fuertes/débiles pueden establecer?
Duración: 15-20 minutos
Checklist
Propiedades de datos
¿Cómo se generó? ¿Cómo se muestreo? ¿Estaba 
actualizado?
¿Qué ruido, sesgo de muestreo y datos faltantes 
introdujo esto?
¿Datos perdidos?
¿Puedes modelar explícitamente el ruido, 
independientemente de un enfoque?
Si el conjunto de datos está etiquetado, ¿cómo 
fue etiquetado?
¿Se puede medir?
¿Puede modificar o aumentar el proceso de 
etiquetado para compensar el sesgo existente?
Supuestos de aproximación
¿Qué suposiciones hace cada enfoque sobre los 
datos/el proceso de generación de datos/el 
fenómeno en estudio?
¿Es razonable hacer suposiciones sobre su 
problema? ¿Hasta qué grado?
¿Cómo espera que la aplicabilidad se relacione 
con la violación de estos supuestos?  
¿Se pueden validar estos supuestos de forma 
independiente?
¿Algún caso que viole estos supuestos?
Experiencia pasada
¿Qué experiencia tiene aplicando este enfoque, 
ya sea en general o para problemas similares?
¿Encontró alguna publicación de éxito/fracaso 
de la aplicación de este enfoque a problemas 
similares?
¿Te comunicaste con tus compañeros para 
conocer su experiencia?
¿Qué lecciones se pueden aprender de lo 
anterior?
¿Qué soluciones se usaron para resolver este 
problema hasta ahora? ¿Cuáles fueron sus 
ventajas y qué problemas sufrieron?
Alineación de objetivos
Aprendizaje supervisado: 
¿Qué funciones de pérdida se pueden utilizar al 
ajustar los parámetros del modelo? ¿Cómo se 
relacionan con los KPI del proyecto?
¿Qué métricas se pueden utilizar para la selección 
de modelos/optimización de hiperparámetros? 
¿Cómo se relacionan con los KPI del proyecto? 
Aprendizaje no supervisado
¿Qué medida optimiza el método?
¿Cómo se relaciona con los KPI?
¿Cuáles son algunos casos extremos que satisfacen 
bien la métrica pero no el KPI?
Implementación
¿Existen implementaciones del enfoque en un 
lenguaje utilizado actualmente en su entorno de 
producción?
¿La implementación está actualizada? ¿Apoyado 
consistentemente? ¿En amplio uso?
¿Hay usos exitosos de esta implementación 
específica por parte de empresas/organizaciones 
similares a la suya? ¿En problemas similares al 
tuyo?
Escalamiento
¿Cómo se escala el tiempo de 
cálculo/entrenamiento con la cantidad de puntos 
de datos?
¿Con el número de características o variables?
¿Cómo se escala el almacenamiento con puntos 
de datos/características?
Requerimientos de 
información
¿En qué medida cada enfoque depende de la 
cantidad de información disponible? ¿Cuál es el 
impacto esperado en el rendimiento de 
pequeñas cantidades de información?
¿Esto se alinea bien con la cantidad de 
información actualmente disponible? ¿Con 
disponibilidad futura?
Narrativa de 
presentación de 
datos +tips
Reglas generales
Reglas generales
● Tratar de ser simple, presentar mucha información 
hace que los temas relevantes se pierdan
● Presentar desde lo más general a lo específico.
● La data debe responder a las preguntas de interés
● Escribir en tiempo pasado cuando describen 
resultados
● No repetir información en texto, tablas o gráficos
Reglas específicas
Texto
✔ Los datos que usualmente son números o figuras 
se representan de mejor manera en tablas y 
gráficos.
✔ El uso de palabras cualitativas para atraer la 
atención del lector siempre es importante. Frases 
como "notablemente" disminuido, 
"extremadamente" diferente y "obviamente" más 
alto son redundantes.
✔ Evitar usar palabras e información redundante. Las 
tablas y gráficos deben ser autoexplicativos
Tablas
✔ Las tablas son útiles para resaltar valores 
numéricos puntuales; las proporciones o 
tendencias se ilustran mejor con tablas o gráficos. 
✔ Las tablas resumen grandes cantidades de datos y 
permiten hacer comparaciones entre grupos de 
variables. En general, las tablas bien construidas 
deben explicarse por sí mismas con cuatro partes 
principales: título, columnas, filas y notas al pie.
Tablas
✔ Título: Debe ser breve y relacionar el contenido de 
la tabla. Las palabras en el título deben representar 
y resumir las variables utilizadas en las columnas y 
filas en lugar de repetir los títulos de las columnas y 
filas.
✔ Columnas y filas: Los datos similares deben 
presentarse en columnas. A menudo, estas son 
variables dependientes y permiten una comparación 
más clara entre grupos. No se debe tener muchas 
variables 
Tablas
✔ Notas al pie:  añaden claridad a los datos 
presentados. Se enumeran en la parte inferior de las 
tablas. Su uso es para definir abreviaturas, símbolos, 
análisis estadísticos y reconocimientos no 
convencionales. 
✔ Cuerpo de la tabla: Se pueden usar unidades y 
decimales correctos, evitar incluir ceros 
innecesarios, muchas líneas en la tabla y cuando se 
reporten significancias se pueden utilizar asteriscos 
para demarcar los resultados importantes
Gráficos
Son particularmente buenos para demostrar tendencias 
en los datos que no se pueden observar en las tablas. 
Generan un énfasis visual y evitan descripciones 
textuales largas.  Algunas reglas generales son:
✔ Diagramas de barras (horizontales o verticales) se 
usan para mostrar información categórica
✔ Evitar el uso de gráficos en 3D ya que pueden ser no 
la mejor forma de transmitir la información 
✔ Los gráficos de líneas son más apropiados para 
monitorear cambios en un periodo de tiempo.
Gráficos
✔ Los piecharts no deben ser usados usualmente ya 
que cualquier tipo de dato en un piechart se 
representa mejor en un gráfico de barras (e.g un 
error común es presentar la distribución del género 
en un pie chart)
✔ No sobrecargar los gráficos con muchos colores y 
efectos añadidos ya que la atención de la audiencia 
se puede perder fácilmente provocando que no se 
transmita el mensaje que se quiere.
Estadísticas
✔ Estadísticos simples como media, desviación 
estándar, mediana y tests de normalidad se pueden 
reportar de manera textual. Pruebas estadísticas 
más sofisticadas se representan mejor con tablas y 
gráficos
✔ Se debe interpretar correctamente el p valor. 
Usualmente el valor por defecto de significancia es 
0.05, por ende pv>0.05 no significativo
✔ Cuando se reporten intervalos de confianza se debe 
reportar también el p valor obtenido
Tips para presentar
Tips para presentar
✔ Asegurarse de que tu data se pueda ver bien
✔ Enfocarse en lo más importante de los gráficos
✔ Establecer un punto relevante para cada gráfico
✔ Etiquetar ejes y títulos claramente
✔ Identificar visualmente con recuadros u otros 
objetos los puntos importantes de las gráficas
✔ Escribir títulos que refuercen el punto principal que 
se quiere transmitir
✔ Presentar a tu audiencia no a tus datos
✔ Manejar fluidez verbal y estar abierto a preguntas
Preguntas importantes
✔ ¿Estoy presentando o circulando mis datos?
✔ ¿Estoy usando el gráfico/tabla apropiada?
✔ ¿Qué mensaje quiero transmitir?
✔ ¿Mis visualizaciones están acordes con los números?
✔ ¿Mis datos son relevantes, no se han mostrado 
antes?
✔ ¿Cómo pretendo que la audiencia retenga el 
mensaje principal que quiero transmitir?
✔ ¿En qué puntos debe hacer más énfasis?
Revisión rápida 
Con base en los tips presentados hasta el momento les 
brindaremos 10 minutos para mejorar las 
presentaciones de sus proyectos finales con el fin de 
llevar a cabo una revisión de pares con los compañeros y 
tutores de la cursada.
         ☕
       Break
     ¡10 minutos y 
     volvemos!
 Revisión de pares
Realizaremos una revisión de pares con el fin de 
mejorar nuestros proyectos. Grupos de 3-5 personas
     Duración: 40-45 mins
ACTIVIDAD EN CLASE
Revisión de pares
Cada alumno deberá exponer su proyecto final durante 
3-5 minutos a los demás compañeros y tutor con el fin 
de obtener feedback del mismo. 
Son bienvenidas todas las sugerencias 
constructivas que ayuden a mejorar el proyecto 
en forma y fondo.
Obtención de insights a 
partir de visualizaciones
Deberás  entregar  la  segunda  pre  entrega  de  tu  Proyecto  Final. 
Entrenarás  y  optimizarás  versos  modelos  de  Machine  Learning  para 
resolver  una  problemática  específica,  detectada  en  la  instancia  de 
entrega anterior. El objetivo es que puedas utilizar modelos de Machine 
Learning para resolver el problema de una industria o negocio. 
Recordemos…
                  Generamos visualizaciones
                  Obtuvimos insights
                  Desarrollamos una narrativa 
                  correcta
 Clase 36
Desafío entregable:
Obtención de insights
                  Responder las preguntas 
                  problema
PREENTREGA DEL PROYECTO 
FINAL
Obtención de insights a partir de 
visualizaciones
Objetivos generales
✓ Obtener datos de diversas fuentes como APIs o Bases de datos públicas para luego 
analizarlos mediante el lenguaje Python con el fin de contestar una pregunta de 
interés para una industria, negocio o proyecto personal. Se deberán utilizar datasets 
complejos implementando técnicas avanzadas para la limpieza y adquisición de 
datos
Objetivos específicos
✓ Estructurar un problema en función de múltiples pero simples preguntas/hipótesis a 
responder
✓ Importar datos crudos de APIs o bases de datos usando Python
✓ Limpiar y transformar los datos para permitir un posterior análisis
✓ Contar una historia mediante el análisis exploratorio de datos
PREENTREGA DEL PROYECTO 
FINAL
Obtención de insights a partir de 
visualizaciones
Requisitos base
✓ Un notebook (Colab o Jupyter) que debe contener:
1. Abstracto con motivación y audiencia: Descripción de alto nivel de lo que 
motiva a analizar los datos elegidos y que audiencia  se podrá beneficiar de este 
análisis
2. Preguntas/hipótesis que queremos responder: Lista de preguntas que se 
busca responder mediante el análisis de datos. Bloques de código donde se 
importan los datos desde una API o base de datos pública y los guarda en un 
archivo local csv o json. El estudiante puede luego de descargar los datos, 
comentar este bloque de código
3. Análisis exploratorio de datos (EDA): Análisis descriptivo de los datos 
mediante visualizaciones y herramientas estadísticas
 PREENTREGA DEL PROYECTO 
 FINAL
Obtención de insights a partir de 
visualizaciones
Requisitos base
✓ Una presentación (PDF; PowerPoint o Google Slides) que debe contener
1. Abstracto con motivación y audiencia: Descripción de alto nivel de lo que motiva a 
   analizar los datos elegidos y que audiencia  se podrá beneficiar de este análisis
2. Resumen de metadata: resumen de los datos a ser analizados es decir, número de 
   filas/columnas, tipos de variables, etc
3. Preguntas hipótesis que queremos responder: Lista de preguntas que se busca 
   responder mediante el análisis de datos
4. Visualizaciones ejecutivas que responden nuestras preguntas: utilización de 
   gráficos que responden las preguntas de interés de nuestro proyecto.
5. Insights: resumen de hallazgos del proyecto. Aquí consolidamos las respuestas a las 
   preguntas/hipótesis que fuimos contestando con las visualizaciones
PREENTREGA DEL PROYECTO 
FINAL
Obtención de insights a partir de 
visualizaciones
Sugerencias
Es conveniente retomar el dataset trabajado en la primera pre entrega y enriquecerlo (e.g 
joins, y creación de nuevas columnas) con información proveniente de APIs públicas 
siempre que se pueda con el fin de practicar las nuevas habilidades adquiridas. Se 
recomienda retomar la metodología de trabajo y reutilizar algoritmos ya entrenados, de 
ser necesario.
Requisitos extra
✓ Subir el proyecto a Github
PREENTREGA DEL PROYECTO 
FINAL
Obtención de insights a partir de 
visualizaciones
Dont’s
✓ Utilizar jerga demasiado técnica en la presentación (recordar que la audiencia de la 
misma son roles ejecutivos)
✓ Sobrecargar las diapositivas 
✓ Realizar una presentación con más de 12 slides de extensión
Modelo de Proyecto final
✓ Proyecto final (Notebook) (Se debe abrir con Google Collaboratory o Jupyter 
Notebook)
✓ Ejemplo Presentación 
Explicación del desafío
✓ ¡Click aquí!
CLASE N°38
Glosario
Revisión de pares: proceso constructivo     Reglas para presentación de 
donde se busca mejorar la calidad de un     resultados: se refiere a 4 puntos 
proyecto de ciencia de datos. Funciona      importantes a la hora de mostrar 
como un mecanismo de control antes de       resultados a una audiencia. Debemos tener 
presentar los resultados obtenidos          cuidado del texto, tablas, gráficos y las 
                                           estadísticas que se reportan para asegurar 
Narrativa de presentación: se refiere a     que el estudio mantenga un buen estándar 
la metodología que se utiliza para          de calidad.
transmitir información y resultados ante 
una audiencia. Se recomienda ser lo más 
simple posible a la hora de presentar 
resultados y utilizar jerarquías. 
¿Preguntas?
Opina y valora 
esta clase
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Revisión de pares
      ✓ Narrativa de presentación de datos + tips
Esta clase va a ser
grabad
  a
      Clase 27. DATA SCIENCE
 Data Acquisition II
Temario
             26                   27                    28
          Lenguaje               Data 
        estructurado         Acquisition II     Data Wrangling I
            SQL-II
      ✓ Consultas de mayor   ✓ Intro Github      ✓ Data Wrangling en 
         complejidad         ✓ Javascript object    un proyecto de DS
      ✓ TCL                    notation          ✓ Etapas de Data 
      ✓ Otros tipos de       ✓ Introducción APIs    Wrangling
         comandos            ✓ Conexión a 
      ✓ Joins, agregaciones    modelos           ✓ Combinar y 
         y agrupaciones        relacionales         fusionar objetos 
                               usando Pandas        con Pandas. Merge 
                                                    y Concat
Objetivos de la clase
         Acceder a bases de datos con Pandas
         Leer JSON con Pandas
         Conocer cómo utilizar APIs
MAPA DE CONCEPTOS            Pandas
                         Hojas de Cálculo
                           PySpark
                          Git y Github
Data Acquisition
                            JSON
                            APIs        Parte II
                         SQL con Pandas
Introducción a 
Github
 Git
Git
Git es un software de control de versiones 
diseñado por Linus Torvalds, pensando en 
la eficiencia, la confiabilidad y 
compatibilidad del mantenimiento de 
versiones de aplicaciones cuando estas 
tienen un gran número de archivos de 
código fuente
Github
Github
GitHub es una plataforma de control de versiones 
distribuida donde los usuarios pueden colaborar o 
adoptar proyectos de código fuente abierto, fork 
code, compartir ideas y más.
Su mayor contribuidor es Microsoft y ofrece un 
servicio de Software as Service (SaaS) creado en 
2008
Diferencias Git vs 
Github
Diferencias Git vs. 
Github
Git y GitHub no son lo mismo. Git es una 
herramienta de control de versiones de código 
abierto creada en 2005 (Linux); GitHub es una 
empresa fundada en 2008 que fabrica 
herramientas que se integran con git. 
No necesitan GitHub para usar git, pero no puede 
usar GitHub sin usar git. Hay muchas otras 
alternativas a GitHub, como GitLab, BitBucket
Diferencias Git vs. Github
                          Git                                               Github
   Instalado localmente                                Hosteado en la nube
   Lanzado en 2005 y mantenido por The Linux           Lanzado en 2008 y mantenido por Microsoft
   Foundation
   Se enfoca en control de versiones y código          Se enfoca en un hosting de código centralizado
   compartido
   Se ejecuta en la terminal                           Administrado en la web
   Provee una interface desktop llamada Git Gui        Provee una interfaz desktop llamada Github Desktop
   Pocas configuraciones externas disponibles          Marketplace diverso para integración de 
                                                       herramientas
   Licencia Open Source                                Versión gratuita y también paga
Cómo usar Git
EJEMPLO EN VIVO
Cómo usar Git
1. Instalar Git para esto podemos ir al siguiente enlace Instalación Git
2. En caso de que estén en una distribución de Linux debian pueden ir al terminal 
y usar el comando: $ sudo apt install git-all
3. Ahora debemos crear el nuevo repositorio usando la terminal, si no conoces de 
la terminal puedes ver el siguiente Tutorial terminal
4. Nos ubicamos en la ruta deseada y creamos el directorio con los comandos:
$ cd Descargas
$ mkdir Proyecto_Github
$ cd Proyecto_Github
EJEMPLO EN VIVO
Cómo usar Git
5. Inicializamos el repositorio con el comando: 
$ git init
Debe salir el siguiente mensaje:
Inicializado repositorio Git vacío en /home/david/Descargas/Proyecto_Github/.git/
6. Podemos crear un nuevo archivo en el repositorio con el comando:
$ touch newfile.txt
7. Verificamos que archivos hay en la ruta: $ ls
Deberá aparecer: 
newfile.txt
EJEMPLO EN VIVO
Cómo usar Git
8. Verificamos el estado usando: $ git status
Esto significa: "Notamos que creaste un nuevo archivo llamado newfile.txt, pero a menos 
que uses el comando 'git add', no haremos nada con él".
Commit: es un registro de los cambios que ha realizado desde la última vez que realizó 
una confirmación. Esencialmente, realiza cambios en su repositorio (por ejemplo, agrega 
un archivo o modifica uno) 
EJEMPLO EN VIVO
Cómo usar Git
9. Si es tu primera vez usando Git es recomendable modificar los siguientes atributos:
Lo cual modifica el email y nombre de usuario que está creando el repositorio
10. Ahora agregamos el archivo new_file.txt  con el comando:
$ git add newfile.txt
11. Hacemos el commit con el siguiente comando:
Deberá aparecer un mensaje como el anterior para saber que todo quedó en orden.
Cómo usar Github
Cómo usar Github
Si solo deseas realizar un seguimiento de tu código localmente, no necesitas usar 
GitHub. Pero si deseas trabajar con un equipo, puedes usar GitHub para modificar el 
código del proyecto de forma colaborativa.
1. Para crear un nuevo repositorio en GitHub, inicie sesión y vaya a la página de 
inicio de GitHub. Puede encontrar la opción "Nuevo repositorio" debajo del signo 
"+" junto a su foto de perfil, en la esquina superior derecha de la barra de 
navegación:
Cómo usar Github
2. Después de hacer clic en el botón, GitHub 
le pedirá que asigne un nombre a su 
repositorio y proporcione una breve 
descripción.
3. Cuando haya terminado de completar la 
información, presione el botón 'Crear 
repositorio' para crear su nuevo repositorio.
Cómo usar Github
4. GitHub le preguntará si desea crear un 
nuevo repositorio desde cero o si desea 
agregar un repositorio que haya creado 
localmente. 
En este caso, dado que ya hemos creado un 
nuevo repositorio localmente, queremos 
enviarlo a GitHub, lo podemos hacer desde la 
terminal
Javascript Object 
Notation
Definición
Javascript Object Notation 
(JSON)
✔ Se trata de un formato de datos semi-estructurado no tabular, es decir 
que los registros no tienen que tener un mismo conjunto de atributos.
✔ Se utiliza mucho para transferir datos entre servidores y clientes. La 
fuente más común donde se consumen los JSON son las interfaces de 
programación de aplicaciones web o web APIs.
✔ Los datos se organizan en colecciones de objetos o paquetes.
✔ Pueden “anidarse” → Los valores pueden ser listas u otros objetos.
Javascript Object Notation 
(JSON)
           Ejemplo de un JSON 
           anidado
           Se observa por 
           ejemplo que puede 
           contener 
           diccionarios dentro 
           de una key.
Librería JSON
https://docs.python.org/3/library/json.html
¡Python admite JSON de forma nativa! Python 
viene con un paquete integrado llamado json 
para codificar y decodificar datos JSON.
Ejemplo
Librería JSON
       Vamos a leer este 
       ejemplo guardado en 
       ejemplo_1.json
Librería JSON
Librería JSON
Ejemplo json.loads. A partir de un 
string lo convierte en un diccionario
Librería JSON con Pandas
Método: read_json():
✔ Puede aplicarse como path a un archivo JSON o una url que 
  termine .json o directamente un string.
✔ Pandas adivina el tipo de datos pero tb puede ser especificado 
  (dtype)
✔ Orientación: Consiste en el parámetro que indica cómo se 
  estructura JSON, para poderlo convertir en un DataFrame.
Librería JSON con Pandas
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Introducción a APIs
 API
API
✔ Es una interfaz que permite la comunicación 
entre dos sistemas distintos permitiendo agregar 
diversas funciones a sitios web y aplicaciones.
✔ Define como las aplicaciones se comunican con 
otros programas.
✔ Es una manera de obtener datos de una 
aplicación sin conocer los detalles de la bases de 
datos.
Ejemplos API
Fuente: William El Kaim 2016
Tipos de negocios 
con APIs
Tipos de negocios API
Existen tres modelos 
básicos de negocio. 
1.  El gratutito
2.  En donde el 
consumidor paga 
por usar la API 
3. Donde el usuario 
final interactúa con 
una aplicación 
usando una API y 
proporciona un 
beneficio
Tipos de negocios API
Es importante entender los siguientes conceptos para comprender mejor las APIS
1. Proveedor API: La compañía que crea y ofrece la API
2. Consumidor API: simple desarrollador que implementa la API
3. Usuario final: la persona que usa la aplicación que contiene la API
Desarrollador paga
Suscripción
1. Se cobra a los desarrolladores que utilicen la 
API.
2. Se necesita tener un sistema que ofrezca 
características interesantes para que los 
usuarios les interese
3. Algunas veces se ofrece gratuito pero se debe 
pagar por algunas cosas 
4. La mayor debilidad radica en que algunas 
veces se pagan por recursos innecesarios
5. Ejemplos: Pabbly API, Adapty, Bissby, Billby, 
IAPHUB, Qonversion, Viafoura Chat, Ubersmith
Pay-as-you-go
1. Se paga solo cuando se usa (ej. $1 por cada 
100 API calls)
2. Amazon Web Services y Google Maps API son 
ejemplos donde el consumidor agrega un 
crédito a su cuenta y se le cobra 
mensualmente basado en lo que use
3. Twilio por ejemplo el consumidor paga una 
cuota inicial y cada mes el sistema debita 
cierta cantidad aunque bajo este esquema 
muchas veces no se puede llevar el control de 
lo que se esta consumiendo
Transaction fees
1. Se cobra por cada transacción realizada por 
los usuarios finales
2. Sistemas como PayPal o Braintee reciben el 
dinero luego de cada transacción que ocurre 
en su app cuando se activa la API
3. Braintree tiene una tasa de cobro de 2.9% 
+0.30$ por cada transacción. 
4. No hay suscripción solo se paga por cada 
transacción
5. Algunas APIs como Sprint genera diferentes 
estrategias de cobro a los usuarios de acuerdo 
a diferentes esquemas de consumo
Desarrollador y 
proveedor ganan
Share from ads
1. El proveedor ofrece publicidad en su 
plataforma, los consumidores integran esta 
publicidad en sus apps generando revenue 
para el proveedor. A cambio el proveedor da 
una porción de la ganancia
2. Por ejemplo si una API tiene 700 socios y se 
crean 50000 apps de terceros esto genera un 
mayor tráfico y mayor impacto de la marca
3. Ejemplos Trip-advisor. facebook Ads, Amazon 
Advertising, Twitter Advertising, Bing Ads, 
Adsense, Local Search, Adzuna, Anti 
Adblockers
Programa de referidos
1. El consumidor de la API la incluye en su app y 
obtiene dinero cada vez que un usuario hace 
click
2. También existen sistemas de referidos donde 
los consumidores de la API reciben dinero 
cuando un cliente compra servicios del 
proveedor
3. Por ejemplo el sistema de afiliados de 
Skyscanner cuando un usuario compra 
tiquetes desde una API ofrece el 50% del 
revenue generado con el consumidor de la API
Qué pasaría si Netflix
no tuviera APIs
Obtener datos de 
 APIs
Librería Requests.get 
Python
✔ Consiste en una librería para hacer solicitudes a las aplicaciones.
✔ Es el método de envío y obtención de datos desde url.
✔ No está atada a ninguna API en particular.
Librería Requests.get 
Python
Hacemos un get de la url y nos da una respuesta. Si la respuesta da 200 es que la 
conexión fue exitosa 
Librería Requests.get 
   Haciendo respuesta.text devuelve la información en str
Python
                   Entonces guardando el str y luego aplicamos 
                   json.loads() devuelve una lista de 
                   diccionarios con los datos
Librería Requests.get Python
Guardamos la info en y a continuación creamos el dataframe en pandas 
para trabajarlo.  
Ejemplo en vivo
Examinaremos cómo extraer información 
de una de las Google APIs que existen en 
específico la denominada Youtube API. 
Explicaremos el proceso de como crear una 
API Key y cómo generar una consulta 
específica para obtener las estadísticas de 
un canal particular.
Conexión a modelos 
relacionales SQL 
usando Pandas
Repaso Bases de 
datos
Bases de datos 
✔ Organizan datos sobre entidades en tablas.
relacionales 
✔ Representan los datos como un conjunto de tablas en dos dimensiones, 
formadas por filas y columnas. Cada fila o registro es una instancia de una 
entidad.  Cada columna tiene información de un atributo.
✔ A menudo, las bases de datos relacionales contienen múltiples tablas 
relacionadas  entre  sí,  lo  cual  permite  que  la  información  pueda  estar 
almacenada de una forma más compacta. 
Bases de datos 
relacionales 
1. Soportan  mucha  cantidad  de  datos,  usuarios 
múltiples  de  forma  simultánea  y  controles  de 
calidad del dato.
2. Los  tipos  de  datos  están  especificados  en  cada 
columna.
3. Se  usa  sql  (structured  query  language)  para 
interactuar con la base de datos. 
Cómo Acceder a 
Bases relacionales 
desde Pandas
Librerías para conectar 
SQL-Pandas
Cargar datos a un DataFrame puede ser 
bastante directo con las siguientes librerías:
1. Sqlalchemy
2. Sqllite3
3. cx_Oracle
4. Pyodbc
5. etc.
Sqlite3
Sqlite3
Con Pandas, es muy sencillo combinar el 
motor particular que se use y transformarlo a 
un DataFrame usando sql como string.
Creamos la conexión con sqlite3 y junto con 
una query sql, la usamos de argumento con el 
método read_sql_query de pandas.
Sqlite3
1. Creamos la query sql como str 
para extraer todas las tablas de 
la base.
2. Generamos la conexión hacia la 
base nba_salary.sqlite.
3. Integramos el dataframe con la 
query y la conexión creada con 
anterioridad. 
4. Se hace un print del DataFrame 
(En este caso  solo tiene dos 
elementos) y se cierra la 
conexión.
Sqlite3
Mediante query, también 
podemos extraer la tabla 
NBA_season1718_salary
:
sqlalchemy
Sqlalchemy
Sqlalchemy puede leer cualquier tipo 
de base. Donde dice sqlite se puede 
cambiar el motor:
DataFrame
Consultando información de 
  la YouTube-API
Crearemos nuestra propia API key para poder utilizar 
   información de la YouTube-API
     Duración: 20-25 mins
ACTIVIDAD EN CLASE
Consultando 
información de la 
1. Logearse con alguna cuenta de Gmail en primera 
YouTube-API
medida
2. Ir al siguiente enlace: Google Console Developers
3. Estudiar la documentación de la API de YouTube: 
Youtube-API
4. Seguir el tutorial provisto en el archivo 
denominado Clase 5- Data Acquistion II.ipynb 
dentro de la carpeta de clase
5. Crear tu propia APIKey 
6. Obtener información estadística del canal con 
username =’sentdex’
7. Subir el notebook con los resultados en tu propio 
repositorio Github
                                6
        Descarga de datos 
      desde APIs públicas
     Deberás entregar el sexto avance de tu proyecto final. Continuaremos hablando sobre lo 
     trabajado en la primera pre entrega del proyecto final. Crearás un notebook donde se 
     seleccionará  una  API  de  interés,  luego  crearás  una  API  key  y  finalmente  extraerás  la 
     información  para  ser  almacenada  en  un  DataFrame  (se  sugiere  que  esta  información 
     complemente o enriquezca el dataset elegido en el desafío  “Elección de potenciales 
     Datasets e importe con la librería Pandas”)
     DESAFÍO 
     ENTREGABLE
Descarga de datos desde APIs 
Consigna                                          Formato
públicas
 ✓ Buscar información en APIs públicas (i.e        ✓ Se debe entregar un jupyter notebook con 
     Twitter, NewsAPI, Spotify, Google Apis,          el nombre “Desafio_APIS_+Nombre_ 
     etc).                                            +Apellido.ipynb”.
 ✓ Extraer datos e importarlos a un 
     dataframe realizando una exploración        Sugerencias
     simple (i.e filas, columnas, tipos de         ✓ No compartir sus tokens personales
     datos). Se sugiere que estos datos            ✓ Comprender primero el funcionamiento de 
     complementen el dataset elegido en el            las APIs a detalle para después utilizarla
     Desafio “Elección de potenciales              ✓ La limpieza de datos en APIS no es fácil
     Datasets e importe con la librería            ✓ Tratar de obtener datos principalmente 
     Pandas”                                          numéricos (al menos 20 columnas y 
Aspectos a incluir                                     10000 filas.
 ✓ Notebook donde se detallen todos los          Explicación del desafío
     pasos seguidos                                ✓ ¡Click aquí!
Ejemplo
 ✓ Ejemplo Desafío APIS,
CLASE N°27
Glosario
Git: software de control de versiones          JSON: Se trata de un formato de datos 
diseñado por Linus Torvalds, pensando en       semi-estructurado no tabular, es decir 
la eficiencia, la confiabilidad y              que los registros no tienen que tener un 
compatibilidad del mantenimiento de            mismo conjunto de atributos.
versiones de aplicaciones cuando estas 
tienen un gran número de archivos de           API: Es una interfaz que permite la 
código fuente                                  comunicación entre dos sistemas 
                                              distintos permitiendo agregar diversas 
Github: es una plataforma de control de        funciones a sitios web y aplicaciones.
versiones distribuida donde los usuarios 
pueden colaborar o adoptar proyectos de        sqlite y sqlalchemy: librerías para 
código fuente abierto, fork code,              realizar conexiones con bases de datos 
compartir ideas y más.                         relacionales con Pandas
¿Aún quieres conocer 
  más?
Te recomendamos el 
siguiente material
   MATERIAL AMPLIADO
Recursos multimedia
NBA Salary and Statistics 2016-17        JSON
✓ Base de datos NBA 16/17 | Kaggle |     ✓ JSON encoder y decoder | Autor | 
   NBA Kaggle                               JSON library
                                       Requests
Pandas Dataframe to sql                   ✓ Requests.get() | Github | 
✓ Pandas y SQL | Pandas Core team |          JSON file request Github
   Pandas_tosql
Disponible en nuestro repositorio.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Intro Github
      ✓ Javascript Object Notation
      ✓ Introducción APIs
      ✓ Conexión a modelos relacionales SQL usando 
        Pandas
Opina y valora 
esta clase
Muchas 
gracias.
               Encuesta
               sobre esta clase
               Por encuestas de Zoom
               ¡Terminamos la clase! 
               Cuéntanos qué temas te resultaron más complejos de 
               entender. Puedes elegir más de uno. Vamos a 
               retomar aquellos temas que resultaron de mayor 
               dificultad en el próximo AfterClass.
Esta clase va a ser
grabad
  a
      Clase 18. DATA SCIENCE 
   Introducción al 
Machine Learning y 
    la Inteligencia 
       Artificial
Temario
              17                   18                    19
         Estudio de          Introducción al        Algoritmos y 
          casos de            ML y a la IA         validación de 
          modelos                                 modelos de ML
         analíticos II        ✓ Introducción
                                                     ✓ Conceptos 
        ✓ Introducción        ✓ Tipos de IA             básicos
        ✓ Casos de            ✓ Aplicaciones en      ✓ Aprendizaje 
           accidentes            la industria           y validación
                              ✓ Riesgos              ✓ Métricas y 
                                 asociados              Evaluación
Objetivos de la clase
         Explorar las bases del Machine Learning.   
         Sistematizar las características de la 
         Inteligencia Artificial.
         Reconocer sus aplicaciones en la industria.
MAPA DE CONCEPTOS
                            Historia de la 
                            IA
                            Clasificación 
          ML e 
        Inteligencia        IA y Robótica   RPA
         Artificial
                            IA en industrias
                             Programas y 
                              Algoritmos
                               GPT - 3
Repaso 
Introducción
Como bien hemos estudiado en el bloque: 
Modelos Analíticos para Ciencia de Datos, el 
Machine Learning nos brinda y proporciona 
diferentes algoritmos que permiten que las 
máquinas pueden aprender a partir de 
nuestros datos. Pero ahora bien, ¿Qué relación 
existe entre el concepto de ML y la Inteligencia 
Artificial? 
¿Comenzamos?
¿Qué es 
inteligencia?
Podríamos definir Inteligencia como la 
habilidad para razonar, planear y 
resolver problemas, pensar de manera 
abstracta y comprender ideas complejas, 
aprender rápidamente usando la 
experiencia.
En teoría, las máquinas inteligentes 
deberían tener estos atributos pero no 
siempre ocurre.
Fuente: https://www1.udel.edu/educ/gottfredson/reprints/1997mainstream.pdf
Inteligencia Artificial
PARA RECORDAR
Inteligencia Artificial
Es la capacidad de las máquinas para usar 
algoritmos, aprender de los datos y utilizar lo 
aprendido en la toma de decisiones.
Al contrario de lo que ocurre con las personas, los 
dispositivos basados en IA no necesitan descansar y 
pueden analizar grandes volúmenes de información a la 
vez. Esto significa, que la proporción de errores es menor 
en las máquinas que en los humanos, al realizar las 
mismas tareas.
Existen dos tipos de IA 
Narrow IA (“weak IA”) el más utilizado      General IA (“strong IA”) con similitud a 
en la actualidad donde se tienen sistemas   la inteligencia humana,que su mejor 
que están diseñados para resolver solo un   versión solo se ha alcanzado en ciencia 
problema sin poder hacer transferencias     ficción (solo menos del 1% tienen algo 
de información.                             similar a este comportamiento).
La Narrow IA se compone del Machine 
Learning o Aprendizaje de Máquina y también 
del Deep Learning o Aprendizaje Profundo 
(Redes Neuronales). Aunque no es lo único, 
tenemos también Reinforcement Learning.
Para pensar
Unas clases atrás, dijimos que el ML surge 
a mediados de los 50´ para problemas de 
predicción complejos.
¿Cuándo crees que surge la IA? ¿Por qué?
Contesta mediante el chat de Zoom 
Historia de la IA
Historia de la 
Inteligencia 
El concepto de Inteligencia Artificial (IA) no es para                     REEMPLAZAR 
nada nuevo. Las primeras investigaciones datan 
Artificial                                                                  POR IMAGEN
aproximadamente del año 1943 con el trabajo 
teórico de Warren McCullogh y Walter Pitts sobre la 
computación neuronal. 
Sucesos importantes
        40´                     50´
1949: Donald O. Hebb (Psicólogo) publica “La  1950: Test de Turing.
organización  de  la  conducta”,  que  sirvió  de 
                     1956: McCarthy acuña el término 
base  para  los  algoritmos  de  aprendizaje  en 
                     “Inteligencia Artificial” en la conferencia de 
las redes neuronales artificiales.
                     Dartmouth, la primera conferencia dedicada a 
                     la IA.
                     1958: McCarthy desarrolla el lenguaje LISP, 
                     lenguaje con el que se desarrollan la mayoría 
                     de sistemas expertos.
                             60´                                                    70´
     1961: Marvin Minsky publica “Pasos hacia la  1972: Colmerauer desarrolla el lenguaje 
     inteligencia artificial”.                              PROLOG (PROgrammation in Logique).
     1963:  Feigenbaum  y  Feldman  Publicaron 
     Computers and Thought, la primera colección 
     de artículos de IA.
     1966: el programa ELIZA, desarrollado en el 
     MIT, fue uno de los primeros programas en 
     procesar  lenguaje  natural  y  conversar,  a 
     través de una serie de frases programadas.
     1968: Minsky y Papert publican Perceptrons.
                       80´                                          90´
    1986: Rumelhart, McClelland y el grupo       1997:  Deep Blue vs. Garri Kaspárov.
    PDP desarrollan el perceptrón multicapa y 
    el algoritmo de aprendizaje por 
    retropropagación (BP).
    1987: Martin Fischles y Oscar Firschein 
    describen los atributos de un agente 
    inteligente.
                         00´                                            10´
     2005: Un coche autónomo, desarrollado         2011: Watson gana Jeopardi! El ordenador 
     por la Universidad de Stanford (EE. UU),      desarrollado por IBM bate a los campeones 
     gana una competición de vehículos robot,      humanos  del  concurso  de  televisión 
     tras conducir 212 kilómetros de desierto,     estadounidense. 
     sin apoyo humano.
                                                   2014:  Un  ordenador  supera  con  éxito  el 
                                                   Test de Turing.
                                                   2016: Microsoft lanza Tay. 
                                                   2017: AlphaGo, un programa desarrollado 
                                                   por  Google  DeepMind,  vence  al  campeón 
                                                   histórico del Go. 
                                                   2017: Libratus vence al póker.
 IA
tipos de Inteligencia
Tipos de IA
Clasificación de IA. Hintze
Arend  Hintze,  profesor  de  Biología  Integrada  y  Ciencias  de  la  Computación  de  la 
Universidad de Michigan, estableció una clasificación de cuatro tipos de IA:
   1                       2                       3                         4
Máquinas                Memoria              Teoría de la              Autoconciencia
 reactivas              limitada                Mente
Máquinas 
Reactivas
Los tipos más básicos de sistemas de IA son                          REEMPLAZAR 
reactivos y no tienen la capacidad de formar                          POR IMAGEN
recuerdos. Tampoco pueden usar experiencias 
pasadas, para basar las decisiones actuales. Deep 
Blue fue un ejemplo de este tipo de inteligencia. 
(Tareas particulares)
               Memoria 
               Limitada
               En este tipo de inteligencia artificial, las máquinas 
               pueden mirar hacia el pasado, pero no puede compilar 
               la experiencia durante años, como lo hace un humano. 
               Por ejemplo, los vehículos autónomos observan la 
               velocidad y dirección de otros automóviles durante su 
               trayecto. Para que funcionen, hay que identificar 
               objetos específicos y monitorearlos a lo largo del 
               tiempo. 
Teoría de 
la Mente
Las máquinas de esta clase son más avanzadas, no 
solo forman representaciones sobre el mundo, sino 
también sobre otros agentes o entidades. 
Se enfocan en ideas, emociones y pensamientos.
Este pensamiento se asocia en psicología a la 
implicancia de la comprensión de las personas, las 
criaturas y los objetos en el mundo, respecto a su 
propio comportamiento. 
               Autoconciencia
 REEMPLAZAR    El paso final del desarrollo de la IA es construir 
 POR IMAGEN    sistemas que puedan formar representaciones sobre sí 
               mismos. Los seres conscientes de sí mismos, conocen 
               sus estados internos y pueden predecir los 
               sentimientos de los demás. 
               Este paso todavía está lejos de realizarse, aunque los 
               esfuerzos se enfocan hacia la comprensión de la 
               memoria, el aprendizaje y la capacidad de basar las 
               decisiones en experiencias pasadas.
Otras clasificaciones 
de IA
Clasificación de IA. Russell 
y Norvig
Sistemas que actúan como humanos:               Sistemas que piensan racionalmente: 
Los llamados androides y los robots. Estos      Tratan de imitar el pensamiento lógico 
intentan realizar tareas de manera similar      humano. Es decir, se trata de investigar 
a nosotros y de forma más eficiente. (No        cómo lograr que las máquinas perciban, 
quiere decir que lo hagan bien)                 razonen y actúen en consecuencia.
Clasificación de IA. Russell 
y Norvig
Sistemas que actúan racionalmente:              Sistemas que piensan como 
Aquellos capaces de percibir el entorno,        humanos: Son sistemas que imitan el 
que tratan de imitar de forma racional el       funcionamiento del sistema nervioso por 
comportamiento humano y actuar en               medio de redes neuronales artificiales.  
consecuencia.                                   (Ej .GPS)
Ejemplo en vivo
Imaginen tener a su disposición un asistente virtual 
que haga funciones predeterminadas según la 
necesidad. Exploramos un poco como poder convertir 
texto a audio, imágenes a texto y a extraer 
información de la web
Utilizaremos el notebook  Clase_18.ipynb 
dentro de la carpeta de clase.
IA y robótica 
Para pensar
Los robots forman parte del tipo de IA vinculada con 
los “Sistemas que actúan como humanos”.  En la 
actualidad, podemos encontrar un montón de robots 
distintos que realizan actividades diferentes.
¿Qué ejemplos se te ocurren? 
Contesta mediante el chat de Zoom 
La IA y el 
mundo de la 
Por otro lado, se han desarrollado varios proyectos 
de “Robots Humanoides”, entendiéndose como tal a 
Robótica
un sistema robotizado, desarrollado para simular la 
silueta y la forma de moverse de los humanos. 
        Conozcamos el caso del Robot Sophia
               IA + Robótica= 
               Sophia
               Sin duda alguna, el robot humanoide más popular hoy 
               en día es Sophia. 
               ✔ Puede mantener conversaciones complejas como 
                 así también, charlas casuales básicas. 
               ✔ Sophia puede reconocer, expresar y describir el 
                 entorno que la rodea y realizar gesticulaciones o 
                 expresiones faciales. 
    Entrevista con una IA
Miremos el video y 
tomémonos unos 
segundos para pensar     REEMPLAZAR 
que impresiones,         POR IMAGEN
dudas, sensaciones, 
preguntas, etc. nos 
despierta esta 
entrevista con Sophia
Robotic process 
automation
Otro concepto, que también se encuentra      RPA hace referencia a toda tecnología 
muy en auge es el de RPA - Robotic           orientada al uso de software, con el 
Process Automation -, también conocido       objetivo de disminuir la intervención 
como Automatización Robótica de              humana en tareas repetitivas o en tareas 
Procesos.                                    que varían muy poco en cada iteración. 
Para pensar
La robótica también se utiliza en el ámbito 
empresarial, en áreas como finanzas, 
turismo, transporte, educación y hasta 
entretenimiento.
¿Qué ejemplos se te ocurren?
Contesta mediante el chat de Zoom 
Aplicaciones
en la industria
IA en el sector productivo: 
¿Cómo la IA tiene influencia en nuestra vida 
profesional y cotidiana?
������ A través de chatbots o sistemas          ������ Seguridad financiera: Los sistemas 
robóticos automatizados: Conforme           de seguridad con inteligencia artificial 
avance esta tecnología, es probable que     pueden identificar puntos de acceso 
se reemplacen muchos puestos de trabajo     ilegales a datos o a los fondos de 
de atención al cliente.                     instituciones financieras, mediante la 
������ Servicios de inversión con robo          simulación de varias situaciones en las 
advisors: Que ofrecen asesoría              que se puede cometer un delito.
financiera y manejo de carteras sin 
intervención humana.
 La IA en las Industrias
             Turismo                                Salud                          Retail-Ecommerce
 • Reservas mediante comandos         •Revisión de los registros de salud.    • Identificación de patrones de 
 de voz.                                                                       consumo
                                      • Seguimiento y control de los 
 • Asistentes virtuales en            medicamentos.                           • Sistemas de recomendación de 
 hoteles.                                                                      productos, como los de Amazon y 
                                      • Sensores de control de la salud.       Mercadolibre
 • Chatbots en agencias de viaje.
                                      • Cirugía asistida por robots.          • Mejora en la experiencia de 
 • Check-in mediante                                                           usuario.
 reconocimiento facial.               • Asistentes de enfermería 
                                      virtuales.                              • Desarrollo de chatbots
 • Robots turísticos (en los 
 propios hoteles).                    • Artificial Intelligence for Drug 
                                      Discovery (AIDD)
 •Viajes inteligentes, en 
 ciudades inteligentes.
            Transporte                              Educación                          Entretenimiento y Arte
    • Vehículos autónomos.              • Plataformas de aprendizaje               • Trailers de películas editados por IA.
                                        personalizadas según habilidades.
    • Aviones autónomos sin                                                        • Reconocimiento y análisis facial, que 
    piloto.                             • Tutores de inteligencia artificial       determinará      la   reacción     de    la 
                                        personalizados.                            audiencia  a  determinados  tipos  de 
    • Robots como repositores.                                                     contenido.
                                        • Juegos personalizados.
    • Robots que facilitan                                                         • Canciones producidas por IA.
    información a clientes.             • Chatbots orientados a los 
                                        procesos de orientación educativa.         •  Imitación  y  creación  de  obras  de 
    • Uso de Drones a gran                                                         artistas consagrados.
    escala.
            Food Tech                               Banca                                  Manufactura
   • Sistemas capaces de             • Agentes humanos siendo                  •  IA  utilizada  en  diferentes  líneas  de 
   generar tazas de té (Arya)        reemplazados por robots o                 operación para mejorar eficiencia.
                                     chatbots.
   • Sistemas de IA para                                                       • Mantenimiento predictivo y 
   ordenamiento de alimentos y  • Recomendaciones de inversión                 preventivo con IA.
   empaquetado (Tomra                con robots.
   Systems ASA)                                                                • Identificación de errores en las 
                                     • Reducción en tiempos de                 funcionalidades de productos.
   • IA para identificar efectos     reclamos.
   en suelos fértiles.                                                         • Reducción de costos en producción y 
                                     • Detección de fraude en                  ensamblado.
                                     transacciones.
Programas y 
Algoritmos de IA
RankBrain es un algoritmo que aprende el sistema de inteligencia artificial, 
cuyo uso fue confirmado por Google el 26 de octubre de 2015. Ayuda a Google 
  a procesar los resultados de búsqueda y proporcionar resultados de 
        búsqueda más relevantes para los usuarios. 
            Alexa, Siri, Cortana                                          Watson
       La inteligencia artificial de los                  Desarrollado por IBM, este software 
         asistentes de voz de Apple,                        de comunicación fue optimizado 
           Amazon y Microsoft son                              para plantear y responder a 
        conocidos por la mayoría sus                         preguntas, en lenguaje natural.
       usuarios. La función de voz de                       Las empresas médicas lo utilizan 
      estos asistentes está basada en                           para obtener datos de los 
                tecnología IA.                                asegurados y comprobar sus 
                                                                   historiales médicos. 
         TensorFlow                    Facebook AI               Microsoft Emotion 
                                         Research                   Recognition
   En 2015, Google puso a              (FAIR)/Torch
disposición de los usuarios este    Este medio también              El sistema de 
software inteligente, de forma       pretende alentar el         reconocimiento de 
   gratuita, para impulsar el      desarrollo de métodos            emociones de 
 desarrollo de proyectos IA. Se        de aprendizaje              Microsoft es una 
 utiliza en varios productos de          profundo.                herramienta que 
    Google, incluyendo el                                         pretende detectar 
reconocimiento de voz de Gmail                                       emociones en 
       y Google Search.                                              imágenes.
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Riesgos asociados 
a la IA
Para pensar
La Inteligencia Artificial, cada vez abarca 
más ámbitos de nuestras vidas, pero un 
mal uso de ella puede conllevar peligros 
para la sociedad y para los ciudadanos.
¿Qué riesgos y desafíos podrías 
enumerar?
Contesta mediante el chat de Zoom 
Riesgos y desafíos 
asociados a la IA
Riesgos                                       Desafíos
 ✔ Armas autónomas.                            ✔ Reenfocar las habilidades de los 
 ✔ Pérdida de empleo.                             empleados.
 ✔                                             ✔ Cambio cultural y tecnológico.
    Manipulación de la opinión pública o 
    de la sociedad.                            ✔ Desarrollo de sentimientos 
 ✔ Invasión de la privacidad de las               vinculados con la compasión y la 
    personas.                                     empatía.
 ✔                                             ✔ Creación de leyes y regulaciones 
    Algoritmos discriminatorios.                  jurídicas.
Actividad colaborativa
Impacto de la IA en cargos laborales
Analizaremos la influencia de la IA en los 
trabajos de las personas
Realizaremos la actividad de 3-4 personas
Duración: 10 minutos
     ACTIVIDAD COLABORATIVA
Acuerdos
Presencia                                       Apertura al aprendizaje
✓ Participar y “estar” en la clase, que          ✓ Siempre, pero siempre puedes 
    tu alrededor no te distraiga                    seguir aprendiendo. Compartir el 
                                                    conocimiento es válido, la 
Escucha activa                                       construcción colaborativa es la 
                                                    propuesta.
✓ Escuchar más allá de lo que la 
    persona está expresando 
    directamente                               Todas las voces
                                                 ✓ Escuchar a todos, todos podemos 
                                                    reflexionar. Dejar el espacio para 
                                                    que todos podamos participar.
    ACTIVIDAD COLABORATIVA
Impacto de la IA en 
cargos laborales
Consigna: Esta actividad colaborativa          1. Se propone debatir sobre los puestos 
interactuamos con compañeros teniendo              que van a ser reemplazados por la 
como premisa el impacto de los avances             inteligencia artificial. 
en ML y AI en la vida laboral.                 2. En un futuro cercano, ¿Seguirán 
                                                  existiendo los científicos de datos? 
                                                  ¿Por qué?
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
El futuro de la IA: 
GTP 3 
               OpenAI
               Es una compañía de investigación de IA, sin fines de 
               lucro, que tiene como objetivo promover y desarrollar 
REEMPLAZAR      inteligencia artificial, de tal manera que beneficie a la 
POR IMAGEN      humanidad en su conjunto. Esta organización, fue 
               fundada entre otros por Elon Musk y su principal 
               producto es el GPT - 3.
               Muchos creen que la Inteligencia Artificial GPT - 3, es el 
               avance más importante en la tecnología de los últimos 
               años. Pero…
               ¿Qué es GPT - 3?, es el modelo de Lenguaje 
               Natural más poderoso en el mundo creado hasta 
               el día de hoy. 
El futuro de la 
IA: GPT - 3
Usa aproximadamente unos 175.000 millones de                       REEMPLAZAR 
parámetros. Para entrenar el modelo se utilizó toda la             POR IMAGEN
información de Wikipedia, 19 mil millones de páginas 
web, 67 mil millones de libros públicos y otros 410 mil 
millones de textos de la web. 
Con el desarrollo de este modelo tan 
avanzado, surgen muchas preguntas que 
hasta el día de hoy no tienen respuesta 
exacta…
������ ¿Puede la IA generar contenido de la misma 
calidad que un humano?
������ ¿Cómo afectará nuestros trabajos en el futuro?
������ ¿Puede ser peligrosa, qué riesgos representa?
������ ¿Es realmente confiable lo que aprendió o existen 
sesgos en la información?
������ ¿Podría ser creativa?
������ ¿Cómo funciona realmente?
Actividad colaborativa
Evolución de puestos laborales
Identificar cambios en entornos laborales 
con el paso del tiempo
Duración: 10 minutos
    ACTIVIDAD COLABORATIVA
Evolución de los puestos 
laborales
Consigna:                                    Responder lo siguiente:
Se propone pensar acerca de condiciones       ✔ ¿Siguen existiendo esos puestos? 
y responsabilidades en los trabajos de tus    ✔ Si aún existen, ¿Corren peligro de 
abuelos/padres.                                   ser absorbidos por la IA?
                                            Herramienta sugerida: Miró 
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
CLASE N°18
Glosario
Narrow IA (Weak IA): máquinas                   RPA- Robotic Process Automation : se 
diseñadas para resolver problemas               refiere a la automatización de procesos 
específicos con condiciones limitadas y         con el objetivo de disminuir la 
sin transferencia de información,               intervención humana en tareas repetitivas 
representan 99% de las IA actuales              ejecutada por máquinas 
General IA (Strong IA) : similares a los        Robots humanoides: son aquellos 
humanos con capacidad de toma de                robots que se asemejan a la silueta 
decisiones representan solo el 1% de las        humana y que son capaces de realizar 
IA actuales                                     tareas rutinarias sin problemas 
Desafíos asociados a las IA: desarrollo         Riesgos asociados a las IA: desarrollo 
de legislación viable y concreta para su        de armas, pérdida de empleo, 
manipulación, mejorar empatía y                 manipulacion de opinión, invasión de 
generación de conciencia.                       privacidad, algoritmos discriminatorios
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
  MATERIAL AMPLIADO
Recursos multimedia
✓ Cita entre Will Smith y la robot Sophi
  a
   | RT en Español 
✓ Cristiano deja en ridículo a robot al p
  edirle que imite su celebración
  | Televisa
Disponible en nuestro repositorio.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Intro a ML
      ✓ Inteligencia Artificial
      ✓ Tipos de Inteligencia
      ✓ Aplicaciones en la Industria
      ✓  Riesgos y Desafíos de la IA
      ✓ GPT - 3
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 09. DATA SCIENCE
      Estadística 
 Descriptiva: como 
 conocer los datos
Temario
                   08                          09                         10
           Visualizaciones               Estadística              Herramientas 
              y primeros                Descriptiva              de Visualización
           pasos con DS II
          ✓  Seaborn                   ✓ Introducción 
                                                                   ✓ Introducción
          ✓  Tipos de gráficos         ✓ Medidas de                ✓ Gráficos Univariados
          ✓  Subplot y Facetgrip           resumen 
                                       ✓                           ✓ Gráficos Bivariados
          ✓  Customizaciones sobre         Distribución de 
             gráficos                      variables               ✓ Forma de los datos
          ✓  Nutshell                  ✓ Intervalos de             ✓ Gráficos de dos 
                                           confianza                  dimensiones
          ✓  Scikit - Learn
Objetivos de la clase
         Conocer las generalidades de la Estadística 
         Descriptiva.
         Identificar los tipos de variables. 
         Diferenciar las medidas de resumen.
         Comprender la distribución: uniforme vs. 
         normal.
                                       Inferencia 
    MAPA DE CONCEPTOS estadística
                                       EDA
                                                                     Cuantitativas
                                       Tipos de variables            Cualitativas
                                                                     Media
                                                                     Mediana
Estadística 
Descriptiva                             Medidas de 
                                       resumen                       Cuartiles
                                                                     Moda
                                                                     Varianza y 
                                                                     desviación estándar
                                       Distribución de las           Uniforme
                                       variables
                                                                     Normal
Repaso en Kahoot
Introducción a la 
estadística 
descriptiva y tipos 
de variables 
Fuentes de 
información
¿Dónde están los 
datos?
               ✔ Toda vez que tomamos un conjunto de 
                 datos, asumimos que representan una 
                 parcialidad de la realidad.
               ✔ Estos datos son los que serán objeto de 
                 nuestras predicciones y estimaciones de la 
                 mano de la estadística y del data science.
Para tener en 
cuenta
��� Unidad  de  observación:  conjunto  de  variables 
 representadas con datos.
���
���   Población: universo de observaciones que consta de 
 todas las unidades de observación posibles.
���
���   Muestra: conjunto de observaciones que contiene las 
���   unidades de observación a las que tenemos acceso.
Inferencia 
estadística
     Inferencia 
    estadística
             ✓ Inferencia estadística es un conjunto de métodos y 
               técnicas que permiten inferir el comportamiento de una 
               población con base en una muestra 
             ✓ Muestra  subconjunto  de  la  población  con  el  fin  de 
               obtener inferencias de algún parámetro por medio de 
               un estadístico
             ✓ Población  conjunto  de  personas,  eventos  u  objetos 
               sobre los cuales se desea inferir un parámetro
Inferencia 
estadística
                 Inferencia 
                                              1. Establecer hipótesis y verificar 
               estadística
Existen diversos parámetros: Media ,          condiciones: Dos hipótesis sobre la 
proporción, varianza, desviación              población. Una es la hipótesis nula (H0), y 
estándar, total poblacional                   la alternativa o hipótesis de investigación 
                                              (H1)
Para la prueba de hipótesis más común         2. Decidir el nivel significancia (α): 
(Media poblacional) se tienen 6 pasos         probabilidad de corte para tomar 
básicos:                                      decisiones respecto a la hipótesis nula 
                                              (probabilidad de rechazo de H0 verdadera)
                  Inferencia 
                 estadística
  3. Calcular el estadístico de prueba:         5. Tomar una decisión: tenemos dos 
  Reunir datos de muestra y calcular una        opciones rechazamos H0 (p valor <= α)  
  estadística de prueba donde la                o no podemos rechazar H0 (p valor > α). 
  estadística de muestra se compara con         6. Escribir una conclusión: tomamos una 
  el valor del parámetro.                       decisión estadística sobre la hipótesis 
  4. Calcular el p valor (Región rechazo):      nula, entonces resumimos nuestros 
  El valor p se encuentra utilizando el         resultados en una conclusión general 
  estadístico de prueba para calcular la        para nuestra prueba al nivel de 
  probabilidad de que los datos de la           significancia seleccionado.
  muestra produzcan dicho estadístico de 
  prueba o uno más extremo. 
Inferencia 
estadística
p valor
p valor
                                                 ✓
  ✓ p valor = probability value (valor              Luego de 30 días vemos que el Grupo A 
      entre 0 y 1)                                  no ha tenido cambios en el peso y que el 
  ✓ Supongamos que queremos medir si                grupo B ha disminuido 1Kg en promedio
      una droga X reduce el peso de              ✓ Hipótesis nula: La diferencia en peso de 
      personas                                      los que reciben placebo es igual a los 
  ✓ Elegimos dos grupos A (Control- sin             que reciben la Droga X
      ninguna droga) y B (Droga X)
p valor
✓ Si nuestra hipótesis nula es verdad: 
cuál es la probabilidad de encontrar 
una reducción de 1Kg en el grupo B 
(Droga X) respecto al grupo de 
placebo (>>p-valor).
✓ El p valor es una probabilidad en favor 
de la Hipótesis nula
                      ✓ Mientras más pequeño sea el p-valor 
                       mayor evidencia tenemos en contra la 
                       hipótesis nula
                      ✓ p-valor < 0.05 (Rechazar Ho)
                      ✓ p-valor >=0.05 (No rechazar Ho)
Ejemplo
 Queremos verificar si el saldo de las                      No tenemos evidencia suficiente 
 cuentas de nuestros clientes ha variado                    para concluir que la media ha 
 respecto al valor del último año ($ 1362.27                cambiado
 USD)
  ✓ Disponemos de información 
      actualizada de nuestros clientes:
 stats.ttest_1samp(bank['balance'], popmean=1341.12)
 >>>Ttest_1sampResult(statistic=1.477137017325908, 
 pvalue=0.13964587808442475)
Ejemplo
Ahora nos interesa saber si las personas que             En este caso logramos ver una 
tienen crédito con el banco tienen una diferencia        diferencia significativa en la media 
significativa en los saldos respecto a los clientes      de los dos grupos.
que no tienen créditos
loans=bank[bank.loan=="yes"].balance
no_loans=bank[bank.loan=="no"].balance
statistic, pvalue = stats.ttest_ind(loans,no_loans, 
equal_var=False)
print('Estadistico:',round(statistic,2),'p-valor:', 
round(pvalue,2))
>>> Estadistico: -25.18 p-valor: 0.0
¿Cómo conocer los datos?
   Descripción        La estadística descriptiva 
                      es una rama de la estadística 
                      que nos ayuda a entender 
                      mejor los datos.
     Análisis
        ¡Es uno de los pilares del Análisis Exploratorio de Datos 
        o EDA que veremos más adelante!
Exploratory data analysis 
(EDA)                            Etapas del EDA
   Definición
Búsqueda de patrones resumiendo las 
características principales del conjunto 
de datos más allá de los modelos y 
pruebas de hipótesis. 
Filosofía que permite  acercarse a una 
base de datos sin suposiciones. 
Noción de variable
 Noción de variable
 ✓ Las variables son estructuras que 
    tienen valores diferentes
 ✓ Las  variables   son   unidades 
    racionales que se pueden definir 
    en  el  análisis  que  se  pueden 
    asumir  como  un  conjunto  de 
    medidas
 ✓ Básicamente, una variable siempre 
    se  puede  medir  a  través  de 
    varias escalas de medición.
 Tipos de variables
 ✓ Los  tipos  de  variables  son  análogos       ✓ La división más práctica para Data 
     en su concepto a los tipos de datos.            Science: 
 ✓ Existen  porque  hay  operaciones  que              ○ Cuantitativas
     podemos  hacer  con  algunos  de  ellos 
     pero con otros no.                                ○ Cualitativas
º
  Tipos de variables
  ✓ Las  variables  Cuantitativas  se 
      dividen    en:    Discretas    y 
      Continuas                                                       REEMPLAZAR 
  ✓ Las  variables  Cualitativas  se                                   POR IMAGEN
      dividen en Nominal y Ordinal
MAPA DE CONCEPTOS
                                                 Categorías 
                            Nominal             mutuamente 
                                               exclusivas sin un 
                                               orden implícito
          Cualitativas                           Categorías 
                            Ordinal             mutuamente 
                                              exclusivas con un 
                                               orden implícito
Tipos de 
variables 
                                              Variables numéricas 
                           Discretas           que sólo admiten 
                                              números enteros
         Cuantitativas
                                              Variables numéricas 
                           Continuas            que admiten 
                                              números reales 
Variables 
cuantitativas
Tipos de variables
✓ Son variables para las que tiene 
sentido realizar la suma, la resta o el 
promedio.
✓ En general son numéricas y 
pertenecientes al conjunto de los 
números reales o a alguno de sus 
subconjuntos.
✓ Se clasifican en continuas y discretas
 Ejemplo
 ✓ Las alturas de las personas pueden          ✓ Tener una edad de 20,4 años 
     estar medidas en centímetros y las            puede ser muy importante para 
     edades podrían estar medidas en               un estudio médico, pero no para 
     números enteros.                              edades de consumidores de un 
 ✓ Pero nos sirve a fines prácticos,               producto ������.
     nuestra edad aumenta día a día, 
     segundo a segundo.
PARA RECORDAR
Check
Si sumo, resto o promedio los valores, ¿tiene 
sentido el resultado?
Si tiene sentido para alguno de estos casos, 
estamos ante una variable numérica.
Debemos tener en cuenta que siempre depende 
del problema que estamos enfrentando ������.
  Ejemplo
������ Es válido decir que en promedio una             ¡Pueden implementarse en Python con 
familia tiene 2,5 hijos.                           los  tipos  de  datos  int,float  o 
������ Los autos de marca Ford tienen 4,5              complex!
puertas no es útil para nada. 
“Cantidad de hijos por familia” es una 
variable numérica, mientras que “cantidad 
de puertas de los autos”, si bien es un 
número no es variable cuantitativa en su 
naturaleza. ������
 Series de tiempo
  ✓ Cualidades distintivas: secuencialidad,       ✓ Este tipo de datos puede traer 
     la relación entre datos con valores              inconvenientes si los tratamos 
     sucesivos y las características                  de forma manual ������.
     particulares del devenir temporal (años      ✓ Los lenguajes de programación 
     bisiestos, estaciones del año,                   las resuelven con funciones 
     periodicidad en escalas).                        específicas (menos mal ������).
     ¿Una serie de tiempo 
     puede ser considerada 
     como una variable 
    Si!  Los  valores  instantáneos  de  la 
     cuantitativa?
    serie    de    tiempo     se    pueden 
    considerar     como  una  variable 
    cuantitativa indexada por el tiempo
Para pensar
Identifica si el índice trimestral de inflación 
podría considerarse serie de tiempo. 
¿Verdadero o falso?
Contesta mediante el chat de Zoom 
Variables 
cualitativas
    Cualitativas o 
    categóricas
     ✓ Como  su  nombre  lo  indica, 
         sirven  efectivamente  para 
         categorizar elementos.
     ✓ En general son datos de texto 
         libre.
     ✓ Se        pueden       clasificar     en 
         nominales y ordinales
Ejemplo
✓ Algunos datos comunes que se 
utilizan  en  Data  Science  para 
caracterizar a las personas: 
 ○ Sexo/género.
 ○ Ocupación/profesión. 
 ○ Lugar de procedencia.
  Variables que son 
   ✓ En el ejemplo de las puertas de los       ✓ No  tienen  sentido  al  sumarse, 
  categóricas
      autos ������, tenemos una variable que           restarse  o  promediarse,  pero  sí 
      puede  estar  escrita  con  números          tienen  sentido  si  se  cuentan. 
      (“Cantidad    de   puertas”,   con           Podría ser de mucho interés saber 
      valores  posibles  3,  4  y  5),  pero       cuántos  autos  tienen  3  puertas 
      cuya naturaleza no es numérica.              frente a cuántos tienen 4 ó 5.
PARA RECORDAR
Check
Si sumo, resto o promedio los valores, ¿tiene 
sentido el resultado?
Cuando la respuesta es “no” para todos los casos, 
entonces la variable es categórica ������.
PARA RECORDAR
Check
En general los lenguajes de 
programación ofrecen herramientas 
para detectar los tipos de variables 
automáticamente, pero no está de 
más verificarlos siempre a conciencia, 
en especial para detectar los casos 
que se muestran a continuación ������
  Variables lógicas
   ✓ Son  el  análogo  de  los  tipos  de 
       datos Booleanos o lógicos.
   ✓ Tienen  como  valores  posibles 
       verdadero (True) y falso (False) y 
       puede utilizarse para operaciones 
       particulares.
   ✓ A  los       efectos    del   análisis 
       estadístico,  siguen  siendo  una 
       variable categórica.
  Variables de alta 
  cardinalidad y metadatos
  Pueden ser numéricas o de texto, pero 
  no  tienen  sentido  ni  para  sumarse, 
  restarse   o    promediarse     ni   para 
  contarse.
  Ejemplos  típicos:  los  números  de 
  documento,  identificación  personal  o 
  fiscal, los números de serie de producto, 
  entre otros.
  Variables de alta 
  cardinalidad y metadatos
 Más que categorizar  o  cuantificar  una 
 porción  de  todo  el  conocimiento  que        Cuando  la  variable  cumple  la 
 tenemos  acerca  de  un  elemento,  los         función  de  describir  o  brindar 
 identifican de alguna manera.                   información acerca del dato en 
                                                 cuestión, es un metadato.
 Ayudan  a  localizar  a  un  elemento, 
 indispensable para su almacenamiento 
 y  manipulación  en  BBDD,  pero  no 
 tienen    utilidad   práctica   a   fines 
 estadísticos.
¿Cómo identificar 
metadatos? 
✓ A partir de su cardinalidad: medida 
de  cuántos  datos  distintos  existen 
para una variable.
✓ Los  lenguajes  de  programación 
calculan  la  cardinalidad  de  la 
variable en relación a la cantidad de 
elementos.
✓ Si la cardinalidad de una variable es 
cercana a la cantidad de elementos, 
entonces es posible que la variable 
sea un metadato.
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Medidas de resumen
¿Cómo identificar 
metadatos? 
✓ A partir de su cardinalidad: medida 
de  cuántos  datos  distintos  existen 
para una variable.
✓ Los  lenguajes  de  programación 
calculan  la  cardinalidad  de  la 
variable en relación a la cantidad de 
elementos.
✓ Si la cardinalidad de una variable es 
cercana a la cantidad de elementos, 
entonces es posible que la variable 
sea un metadato.
Algunas observaciones
✓ Nos toca echar un vistazo a las 
variables para poder interpretar las 
características del conjunto de datos 
y así obtener conclusiones 
preliminares.
✓ Pueden servir para la toma de 
decisiones en cuanto a qué 
algoritmos aplicar o qué 
consideraciones tomar con respecto 
a los datos.
   Algunas observaciones
  Calcular el promedio para entender cuál 
  es el salario más representativo para la 
  población es una mala elección, ya que 
  sería el equivalente a medir el promedio 
  de las alturas de una salita de niños de 
  jardín…
  ¡junto    con    algunos     jugadores     de 
  básquet! ������
Medidas de resumen 
para variables 
cuantitativas
Media o promedio
✓ La suma de todos los valores dividida 
entre la cantidad de datos. 
En fórmula ������
✓ El número n representa la cantidad de 
valores.  El  número  i  representa  un 
índice  que  va  desde  1  hasta  n.  El 
promedio se representa con 
  Ejemplo
  El promedio de las alturas de los             Con esto, podemos decir que en 
  niños de la salita de jardín es de            general las posiciones de los 
  100 centímetros vs. promedio de               datos para los dos grupos son 
  las alturas de los jugadores de               distintas entre sí.
  basquetbol es de 195 centímetros. 
  Mediana y cuartiles
      1. Ordenar los datos de menor a           ✓ Si la cantidad de datos es 
         mayor. Si hay valores                     impar, busquemos el valor que 
         repetidos, simplemente                    está exactamente en la mitad 
         ponerlos la cantidad de veces             de los datos. Esa es la 
         que aparezcan.                            mediana.
      2. Con  los  datos  ordenados,            ✓ Si la cantidad de datos es par, 
         ahora los contamos:                       habrá dos valores en el centro 
                                                   de los datos. Calculemos el 
                                                   promedio de estos dos valores. 
                                                   Esta será la mediana.
 Mediana y cuartiles
 Entonces :                                   La mediana es más comprensible 
       ✓ 50% de los datos tiene               cuando hablamos de salarios.
           valores < a la mediana
       ✓ 50% restante de los datos 
           tiene valores > a la mediana
 Los valores que llegan: 
       ✓ al 25% = primer cuartil
       ✓ al 75% = tercer cuartil
       ✓ la mediana es equivalente a 
           las dos cuartas partes de los 
           datos = segundo cuartil
  Ejemplo
  A partir de su uso, un % determinado de        Si este valor es igual, por ejemplo, a la 
  la población gana más o menos que              canasta básica de consumo, entonces 
  determinado valor.                             el porcentaje de la población que gane 
                                                 menos de este valor será considerada 
                                                 “por debajo de la línea de pobreza”, 
                                                 como un ejemplo de cómo se mide 
                                                 este indicador económico.
Moda
Es el valor que aparece más 
frecuentemente en un conjunto de 
datos. 
Se obtiene contando los datos y 
calculando cuál valor aparece más 
veces.
Puede servir tanto para variables 
cuantitativas como cuantitativas.
Ejemplo
Para pensar
En un plantel de fútbol ¿cómo podrías 
representar que es un equipo joven?  
¿Verdadero o falso?
Contesta mediante el chat de Zoom 
Varianza
Conociendo  los  valores  de  media, 
mediana y moda, podemos obtener un 
buen resumen acerca de la posición de 
los datos.
 ● Datos     homogéneos:    están 
    agrupados  en  general  más  cerca 
    de la media.
 ● Datos heterogéneos: significa  que 
    en  general  están  lejos  de  la 
    media.
Ejemplo
���
���
  Varianza
 Consiste en medir las distancias de                El número n representa la cantidad 
 todos los datos hasta la media, elevar             de valores. El número i representa 
 cada distancia al cuadrado, realizar la            un índice que va desde 1 hasta n. 
 suma y dividir por la cantidad de datos            La varianza muestral e 
 menos dos unidades.                                representa con s2.
 En fórmula ������
  Desvío estándar
    La varianza tiene un problema:                Para esto, simplemente se aplica la 
    dado que su fórmula involucra                 raíz cuadrada sobre la varianza y 
    elevar al cuadrado, su resultado              se obtiene un valor en las unidades 
    estará expresado en unidades al               que se denomina desvío estándar y 
    cuadrado.                                     se simboliza con la letra s.
Medidas de resumen 
para variables 
cualitativas
  Variables cualitativas
    Los cálculos que tienen sentido                ✓ El conteo de los datos por 
    son aquellos relacionados al                      categoría, esto es, para cada 
    conteo de las observaciones y                     valor posible de la variable, la 
    su categorización.                                cantidad de observaciones.
    Nos interesan:                                 ✓ El valor más frecuente, es decir 
                                                      el que tiene más observaciones. 
      ✓ El conteo del total de datos                  Esto es el cálculo de la moda, tal 
         (valor de n).                                como vimos anteriormente.
Distribución de las 
variables
Si las variables cuantitativas tienen una 
posición y una forma, entonces cada una 
de ellas puede compararse y/o 
diferenciarse de otras variables con 
distintas posiciones y/o formas. Decimos 
entonces que cada variable tiene una 
distribución diferente.
  Introducción de variables 
  ✓ De acuerdo a las formas de las           ✓ La distribución de variables sólo 
      distribuciones podemos realizar           tiene sentido en variables 
      suposiciones que nos ayudan a             cuantitativas (discretas y 
      entender mejor los datos.                 continuas)
  ✓ Una característica a tener en 
      cuenta al analizar distribuciones 
      es si son simétricas o 
      asimétricas.
Distribución de variables 
✓ Muestra una distribución 
simétrica.
✓ Podría referirse a muchas 
personas con bajos salarios.
✓ Otro ejemplos: peso recién 
nacidos, estatura de hombres, 
scores en un test.
✓ Distribución unimodal con 
media=mediana= moda
Distribución de variables 
                                   ✓ Muestra     una    distribución 
                                      asimétrica.
                                   ✓ Podría referirse a los salarios de 
                                      una población
                                   ✓ Moda  <  Mediana  <  Media 
                                      (Asimetría a derecha) 
                                   ✓ Media>    Mediana   <   Moda 
                                      (Asimetría a izquierda)
Dos  distribuciones  muy  importantes  son  la  distribución  uniforme  y  la 
distribución normal. ¡Vamos a verlas!
Distribución 
  Uniforme
 Si todos los valores posibles aparecen 
 aproximadamente  la  misma  cantidad 
 de    veces,   hablaremos    de    una 
 distribución uniforme.
 Las  funciones  de  generación  de  números  random  que  tienen  las 
 calculadoras y las planillas de cálculo siguen esta distribución.
Normal
Formada por puntos que se agrupan de 
manera simétrica en torno a un valor 
promedio, y cuya varianza se aleja no 
más de una determinada distancia del 
promedio.
La importancia de la distribución normal radica en su aparición en múltiples 
campos del mundo real.
Propiedades
✓ Los  datos  normales  son 
simétricos  con  respecto  al 
promedio.
✓ La  media,  mediana  y  moda 
tienen  aproximadamente  el 
mismo valor.
✓ Se  cumple  generalmente  la 
llamada  “regla  empírica”  a 
partir de la cual ������������������������
  Propiedades
 Se    cumple     generalmente    la         ✓ El  99,7%  de  los  datos  está 
 llamada “regla empírica”:                       alejado  a  una  distancia  de 
   ✓ El  68%  de  los  datos  está               aproximadamente  3  desvíos 
      alejado  a  una  distancia  de             estándar del promedio. 
      aproximadamente  1  desvío           Con esto, cualquier dato que esté 
      estándar del promedio.               más allá de 3 veces el valor del 
   ✓ El  95%  de  los  datos  está         desvío    estándar   alejado   del 
      alejado  a  una  distancia  de       promedio  puede  considerarse  un 
      aproximadamente  2  desvíos          valor  extremo o atípico. Veremos 
      estándar del promedio.               los    valores   extremos     más 
                                           adelante.
Intervalos de 
confianza
 Intervalos de confianza
Método     para   obtener    una 
estimación  de  algún  parámetro 
poblacional 
Existen  diversos  parámetros  que 
pueden  ser  estimados  por  este 
método  (ej.  media,  proporción, 
desviación   estándar,  varianza, 
totales)
Intervalos de confianza
En cualquier estimación por intervalo se requiere 
de:
✓ Un  estimador  del  parámetro  poblacional 
(media muestral por ejemplo)
✓ Un nivel de significancia (Error Tipo I) que 
estamos dispuestos a aceptar
✓ Margen de  error  depende  del  tamaño  de 
muestra  y  de  la  varianza  del  estimador 
poblacional
✓ A mayor tamaño de muestra (n) se espera 
tener  menor  margen  de  error  y  más 
pequeño  el  ancho  del  intervalo  de 
confianza
Ejemplo
Nos  interesa  conocer  una  estimación  por 
intervalo  del  saldo  de  cuentas  de  nuestros 
clientes al 95% de confianza.
 from scipy.stats import t
 m = bank.balance.mean()
 s = bank.balance.std()
 dof = len(bank.balance)-1
 confianza = 0.95
 t_crit = np.abs(t.ppf((1-confianza)/2,dof)) # Valor 
 critico
 print(t_crit)
 (m-s*t_crit/np.sqrt(len(bank.balance)), 
 m+s*t_crit/np.sqrt(len(bank.balance))) # Intervalo
 >>>(1334.205344373427, 1390.3387709967262)
   Ejemplo
   Si   queremos  obtener  una  estimación  por 
   intervalo al 95% de confianza de la diferencia en 
   saldos de los con y sin crédito tenemos
    import numpy as np, statsmodels.stats.api as sms
    X1, X2 = 
    bank[bank.loan=="yes"].balance,bank[bank.loan=="no"].balance
    cm = sms.CompareMeans(sms.DescrStatsW(X1), 
    sms.DescrStatsW(X2))
    print(cm.tconfint_diff(usevar='unequal'))
    >>>(-754.6437941384797, -645.6436441287349)
   Existe  una  diferencia  significativa  entre  las 
   medias de los dos grupos
Actividad colaborativa
Análisis de retorno y riesgo de acciones
Utilizaremos información de precios de 
acciones para calcular medidas de 
volatilidad y retorno
Grupos de 3-4 personas. Duración: 20/35 minutos
    ACTIVIDAD COLABORATIVA
Análisis de retorno y 
riesgo de acciones
Consigna:                                      2. Calcular volatilidad relativa por medio de 
 1. Cargar por medio de un ciclo for en       la fórmula (High-Low)/Open
    un solo dataframe los precios de las 
    siguientes acciones en la carpeta de      3. Calcular el índice de retorno por medio 
    la clase:                                 de la fórmula: (Close/Open)-1 
      ✓ Dominion Energy Inc. (Símbolo: 
          D)                                  4. Graficar ambas medidas de volatilidad y 
      ✓ Exelon Corp. (Símbolo: EXC)           determinar en qué acción se daría la mejor 
      ✓ NextEra Energy Inc. (Símbolo: 
          NEE)                                inversión 
      ✓ Southern Co. (Símbolo: SO)
      ✓ Duke Energy Corp. (Símbolo: 
          DUK)
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
CLASE N°9
Glosario                                    p-valor: probabilidad en favor de la 
                                           hipótesis nula. Mientras más grande sea 
Inferencia estadística: son un conjunto     más evidencia en favor de la hipótesis 
de métodos que permiten inferir el          nula se tendrá
comportamiento de una población con         Exploratory Data Analysis (EDA): 
base en una muestra                         búsqueda de patrones resumiendo 
Población: universo de individuos de        características principales del conjunto de 
interés sobre los que se quiere realizar    datos más allá de modelos o inferencia 
inferencia, caracterizado por los           estadística
parámetros                                  ScikitLearn: librería fundamental para el 
Muestra: subconjunto de la población        desarrollo de modelos de Machine 
sobre el que se realizan los cálculos para  Learning (Algoritmos Supervisados y No 
hacer inferencia caracterizado por          supervisados entre otros) en Python
estadísticos                                Intervalo de confianza: Estimación por 
Prueba de hipótesis: conjunto de pasos      intervalo para un parámetro de interés a 
para verificar hipótesis a priori que se    un nivel de confianza
tengan sobre un parámetro de interés 
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Qué es la Estadística Descriptiva
      ✓ Tipos de variables
      ✓ Medidas de resumen
      ✓ Distribución de variables
      ✓ Pruebas de hipótesis e intervalos de confianza
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 04. DATA SCIENCE
  Introducción a la 
  librería científica 
con Python: Pandas 
       (parte I)
Temario
             03                   04                    05
       Introducción a       Introducción a la     Programación 
             la            librería científica     con arrays: 
       programación           con Python          Introducción a 
         con Python            (parte I)             Numpy
         ✓ Estructura de     ✓ Estructura de     ✓ Estructura de datos
          (Parte II)
            control            datos en Pandas 
                                                 ✓ Numpy y ndarrays
         ✓ Funciones         ✓ Manipulación de 
         ✓                     datos en Pandas   ✓ Tipos de datos
            Tipo de datos
         ✓ IPython           ✓ Lecturas de       ✓ Indexado y acceso
                               archivos con      ✓ Operaciones 
         ✓ Instalación         Python               básicas
Objetivos de la clase
         Conocer las estructuras de datos en Pandas.
         Comprender el uso de Pandas para la 
         manipulación de grandes volúmenes de datos.
MAPA DE CONCEPTOS
                        Series
  Estructuras de                             Lectura desde 
  datos de              Data Frames          archivos
  Pandas
                        Panel
                        Selección de 
                        elementos
  Manipulación 
  de datos con 
  Pandas
                        Operaciones         Tratamiento de datos 
                                            ausentes
Cuestionario de tarea
¿Te gustaría comprobar tus 
conocimientos de la clase anterior?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
   Cuarta revolución Industrial      Transformación digital
               Ciclo de vida de un proyecto DS
Estructuras de datos 
en Pandas
¿Qué es la librería 
Pandas? 
Librería Pandas
✔ Pandas facilita la manipulación de          ✔ Es de uso libre, su primera versión 
    grandes volúmenes de datos a                  se creó en 2008.
    través de un conjunto de métodos y        ✔ Está enmarcado en un proyecto 
    estructuras diseñadas para tal fin.           llamado NUMFOCUS (2015)
✔ Extiende las funcionalidades de 
    Numpy, por lo que sus estructuras 
    de datos son totalmente 
    compatibles.
Tres estructuras 
fundamentales
✔ Series (1D)
✔ Dataframes (2D)
✔ Paneles (3D)
En este curso 
trabajaremos 
principalmente con 
Series y DataFrames.
Panda Series
Pandas Series
¿Qué son y cómo 
están 
Las series son objetos de una dimensión, con tres 
componentes principales: 
compuestas?
Índices: Pueden ser cualquier tipo de variable
Valores: Asociados a cada índice
Nombre: Que caracteriza a la serie
   Numeros = range(50, 70, 2)
   Numeros_serie = 
   pd.Series(Numeros)
   print(Numeros_serie)                … como los 
       ���                               Numpy arrays, 
       ���                               pero con índices
                                         ✔ Se construyen a partir de otros objetos 
                                             particulares, como las listas o los Numpy arrays.
   print(Numeros_serie[2])               ✔ Tienen índice propio.
            ���
            ���
Pandas Series
     Están formados por dos objetos 
     vinculados: el arreglo de índices y 
     el arreglo de valores
      print(numeros_serie.index)              ���
      print(numeros_serie.values)             ���
Pandas Series
Tanto el arreglo de índices como el 
de valores pueden modificarse
Numeros_en_texto = 
['primero','segundo','tercero','cuarto','quinto','sexto','séptimo','octavo','
noveno','décimo']                     ���
Numeros_serie_2 = pd.Series(Numeros,index=Numeros_en_texto)
Numeros_serie_2
Pandas Data Frames
Pandas Data Frames
✔ Son una extensión de los objetos 
Series.
✔ Pueden pensarse como una tabla, 
formada por varias Series de igual 
longitud.
✔ Como toda tabla, consta de filas y 
columnas (2D).
✔ Cada fila tiene un elemento índice 
asociado.
Ejemplo en vivo
A continuación veremos los pasos para 
construir un Data Frame en vivo.
Profe, te invitamos a que puedas 
compartir pantalla y mostrar cómo 
realizar esta práctica.
Pandas Data Frames
                                         modelos = ['A4 3.0 Quattro 4dr manual',
                                          'A4 3.0 Quattro 4dr auto',
Construyendo un Data Frame                  'A6 3.0 4dr',
manualmente                                 'A6 3.0 Quattro 4dr',
                                          'A4 3.0 convertible 2dr']
✔ Definir las listas que contienen la    peso = [3583, 3627, 3561, 3880, 3814]
   información                           precios = ['$33,430', '$34,480', '$36,640', 
                                         '$39,640', '$42,490']
✔ Construir las Series
                                         Autos_peso = 
                                         pd.Series(peso,index=modelos)
                                         Autos_precio = 
                                         pd.Series(precios,index=modelos) 
Pandas Data Frames
Construyendo un Data Frame 
manualmente 
Hasta ahora...
print(Autos_precio)        print(Autos_peso)
Pandas Data Frames
Construyendo un Data Frame 
manualmente                                                                            … voilà! 
 ✔ Construir el Data Frame a partir de 
     las Series:
       Autos = 
       pd.DataFrame({'Peso':Autos_peso,'Precio':Auto   ���
       s_precio})
       Autos
Pandas Data Frames
Construyendo un Data Frame 
manualmente 
✔ Construyamos ahora un tablero de 
   ajedrez ♟ 
   Ajedrez_64 = np.arange(1,65).reshape(8,8)
   Ajedrez_df = pd.DataFrame(
        Ajedrez_64,                                  ���
        columns=range(1,9),
        index=['A','B','C','D','E','F','G','H']
   ) 
   Ajedrez_df
Pandas DataFrames 
En resumen las series son objetos 2D 
(filas x columnas) con las siguientes 
propiedades:
✔ Índices: Pueden ser cualquier tipo de 
variable
✔ Valores: Asociados a cada índice y a 
cada columna
✔ Nombre: Que caracteriza al 
Dataframe
Son, entonces, una generalización de 
series
Manipulación de 
datos
en Pandas
Selección de datos
Indexamiento Series
Con Pandas, existen tres formas de 
seleccionar elementos:
                                   ��� Numeros_serie_2['quinto']
✔ Mediante el índice
✔ Mediante el método loc()         ���
                                        Numeros_serie_2.loc['quinto']
✔ Mediante el método iloc() que utiliza 
   sólo índices numéricos          ��� Numeros_serie_2.iloc[5]
Indexamiento Data Frame
También podemos seleccionar partes específicas del Data Frame, 
como índices, columnas y valores.
           Autos.inde  ���
           x
           Autos.colu  ���
           mns
           Autos.valu  ���
           es
Indexamiento Data Frame
✔ Selección de columna
✔ Selección de fila                             ���
                                   Autos['Peso'
                                   ]
          Autos.values[  ���
          1]
          Autos.loc['A4 3.0 Quattro 4dr     ���
          auto',]
Indexamiento Data Frame
  ✔ Selección con condición
Supongamos que necesitamos un listado 
de precios de aquellos autos con peso 
mayor a 3600…
 Autos.loc[Autos.Peso >= 3600,'Precio']         ���
Operaciones básicas 
 de 
datos en Pandas
Transposición
 ✔ Consiste en intercambiar filas de un       En ocasiones necesitamos ver las cosas 
    Data Frame por sus columnas.              desde otra perspectiva...
 ✔ Puede resultar más cómodo trabajar 
    con el Data Frame transpuesto que 
    con el original.
        Autos.T
Funciones Vectorizadas
Desde Numpy, en Pandas
 ✔ Las funciones vectorizadas o ufuncs 
    de Numpy pueden realizarse 
    también sobre Data Frames y Series.
 ✔ Tras ejecutar la operación se 
    conservan los índices.
 Numeros_3 = range(51,70,2)
 Numeros_serie_3 = pd.Series(Numeros_3,index=Numeros_en_texto)          ���
 Numeros_serie_3
Ufuncs sobre Data Frames
Los Data Frames también admiten                    largo = [179, 179, 192, 192, 180]
operaciones vectorizadas ������                        Autos_2 = 
                                                  pd.DataFrame({'Peso':peso,'Largo':larg
Calculemos el porcentaje de un Data                o},index=modelos)
Frame con respecto a los valores de su             Autos_2
primera fila
                                   ���
     Autos_2 / Autos_2.iloc[0] * 100
Conservación de índices
Veamos qué sucede                                              Numeros_serie_2 + 
con los índices al       Numeros_serie_2                       Numeros_serie_3
sumar series…
Conservación de índices
Otra forma de sumar Series o DataFrames 
es con el método .add()
     Numeros_serie_2.add(Numeros_serie_3)                   ���
Conservación de índices
¿Y si sumamos series con índices 
incompatibles? 
                Numeros_serie_2_porcion = Numeros_serie_2[4:7]
                Numeros_serie_3_porcion = Numeros_serie_3[5:8]
                print(Numeros_serie_3_porcion, Nnumeros_serie_2_porcion
                                                              ���
                print(Numeros_serie_2_porcion + Numeros_serie_3_porcion)
                                                              ���
                ¡Los índices que no coinciden se rellenan con 
                NaN!  
Datos ausentes
Houston… tenemos un 
problema
¿Qué hacemos con los datos 
faltantes?
Lidiando con valores 
faltantes
La mayoría de las operaciones de Pandas 
admiten un parámetro fill_value, que 
indica el valor a insertar en caso de 
resultar un NaN.
          Numeros_serie_2_porcion.add(Numeros_serie_3_porcion, 
          fill_value=0)
         En este caso, especificamos que 
           en caso de encontrar un valor  ���
            faltante lo reemplace por 0.
                             ���
¿Datos ausentes, por qué? 
Como futuros Data Scientists,                    ✔ Fallas en algún paso de la carga de 
comúnmente nos toparemos con valores                datos.
faltantes o ausentes que podrían provenir        ✔ Omisión directa de la carga de 
de las siguientes situaciones:                      datos.
                                                ✔ Reticencia de parte de un 
                                                   encuestado a dar una respuesta 
                                                   determinada.
                                              ¡Los valores faltantes son más comunes 
                                              de lo que se piensa!
               Not a Number:
               El representante 
               del valor 
              NaN significa Not a Number y es el valor faltante por 
               faltante
              defecto.
              Es un tipo de dato especial de punto flotante.
              Tiene propiedades especiales: cualquier operación que 
              involucre NaN da como resultado NaN.
   Propagación de valores 
   faltantes
   Veamos qué sucede al operar con NaN                                         Algunas funciones están preparadas para 
                                                                               trabajar con NaN:
   Probemos esta propiedad utilizando el 
   objeto NaN de Numpy:                                                              np.nanprod([2,valor_nan])              ���
         valor_nan = np.nan                                                                                                 ���
         type(valor_nan)            ���
                                    ���                                            En este caso, Numpy le asignó un 
                                                                                           valor de 1 y realizó la 
         2 * valor_nan          ���                                                    multiplicación normalmente.
                                ���
Trabajando con datos 
ausentes
✔ Estos valores podrían no ser              Primero que nada, definamos nuestro 
    adecuados para algunos algoritmos       conjunto de prueba:
    de Data Science. Por ello, deben ser 
    manejados correctamente.
✔ Pandas nos provee de herramientas 
    para trabajar con ellos.
Numeros_nan = Numeros_serie_2_porcion + Numeros_serie_3_porcion    ���
Numeros_nan
 Trabajando con datos 
 ausentes
 Veamos algunos ejemplos...
                                             Numeros_nan.isnull()            ���
   ✔ Podemos marcarlos
   ✔ Podemos reemplazarlos                   Numeros_nan.fillna(0)           ���
   ✔ Podemos eliminarlos
                                             Numeros_nan.dropna()            ���
Metódo pct_change()
Metódo 
pct_change()
El método pct_change() de Pandas se aplica en series 
con datos numéricos para calcular el cambio de 
porcentual de un valor respecto al anterior.
Resulta útil al momento de monitorear variables 
numéricas.
Si tenemos n datos numéricos entonces 
tendremos n-1 cambios porcentuales
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Ejemplo en vivo
¡Llevemos lo visto hasta el momento a la 
acción!
Utilizaremos la carpeta de Notebooks con 
los archivos Clase02_Acciones.ipynb con el 
fin de manipular series y dataframes
Lectura de archivos
con Python
Lectura de .csv 
y .txt
Lectura desde .csv
from google.colab import drive              density    pH  sulphates  alcohol  quality
import os                                 0   0.9978  3.51       0.56      9.4        5
drive.mount('/content/gdrive')       ��� 1   0.9968  3.20       0.68      9.8        5
                                          2   0.9970  3.26       0.65      9.8        5
%cd '/content/gdrive/MyDrive'             3   0.9980  3.16       0.58      9.8        6
                                          4   0.9978  3.51       0.56      9.4        5
df= pd.read_csv('winequality-
red.csv',sep=',')
print(df[['density','pH','sulphates
','alcohol','quality']].head())
Lectura desde .txt
from google.colab import drive
import os
drive.mount('/content/gdrive')        ���
                                                             Name  Type 1  HP  Attack  Defense
                                          0              Bulbasaur  Grass  45      49       49
%cd '/content/gdrive/MyDrive'              1                Ivysaur  Grass  60      62       63
df=                                        2               Venusaur  Grass  80      82       83
pd.read_csv('pokemon_data.txt',delim       3  VenusaurMega Venusaur  Grass  80     100      123
iter='\t')                                 4             Charmander   Fire  39      52       43
print(df[['Name','Type 
1','HP','Attack','Defense']].head())
Lectura de .xlsx
Lectura desde .xlsx
   from google.colab import drive
   import os
   drive.mount('/content/gdrive')      ���
                                       ���   index    ID  Year_Birth   Education   Income
   %cd '/content/gdrive/MyDrive'            0      0  5524        1957  Graduation  58138.0
                                            1      1  2174        1954  Graduation  46344.0
   df=                                      2      2  4141        1965  Graduation  71613.0
   pd.read_excel('defaultoutput.xlsx'       3      3  6182        1984  Graduation  26646.0
   )                                        4      4  5324        1981         PhD  58293.0
   print(df[['index','ID','Year_Birth
   ','Education','Income']].head())
Lectura de .xlsx
Lectura desde Github
 import pandas as pd
 url =                                ���
 'https://raw.githubusercontent.com/                     AMZN    MCD  SBUX   GOOG  MSFT
 JJTorresDS/stocks-ds-edu/main/stock     formatted_date                                 
                                         2016-01-01      587.0  106.3  54.4  743.0  49.9
 s.csv'                                  2016-02-01      552.5  100.7  52.1  697.8  46.0
 df = pd.read_csv(url, index_col=0)      2016-03-01      593.6  108.8  53.6  745.0  50.3
 print(df[['AMZN','MCD','SBUX','GOOG     2016-04-01      659.6  109.5  50.5  693.0  45.5
                                         2016-05-01      722.8  105.6  49.3  735.7  48.3
 ','MSFT']].head(5).round(1))
Lectura desde APIs
 import pandas as pd
 !wget -O cars_clus.csv https://s3-        manufact    model   sales  resale
 api.us-                                   0    Acura  Integra  16.919  16.360
                                       ��� 1    Acura       TL  39.384  19.875
 geo.objectstorage.softlayer.net/cf-       2    Acura       CL  14.114  18.225
 courses-data/CognitiveClass/              3    Acura       RL   8.588  29.725
 ML0101ENv3/labs/cars_clus.csv             4     Audi       A4  20.397  22.255
 filename = 'cars_clus.csv'
 #Lectura
 pdf = pd.read_csv(filename)
 print ("Shape: ", pdf.shape)
 orint(pdf.head(5))
Actividad colaborativa
Manejo de datos según cotización en 
la bolsa
Manipulación de datos según cotizaciones 
de diversas compañías por medio de 
Pandas Data Frames
Duración: 15-20min en grupos 3-4 personas
    ACTIVIDAD COLABORATIVA
Manejo de datos según 
cotización en la bolsa
¿Cómo lo hacemos?
                                             Cualquier duda pueden consultar a 
Importar datos de Acciones Globales (que      su tutor o profesor.
están hosteados en GITHUB en el 
siguiente enlace Monitoreo de Acciones
Graficar cotizaciones y recomendar elegir 
2 acciones para comprar. 
Justificar la decisión.
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
 CLASE N°4
 Glosario
 Pandas: librería fundamental que nos permite              Operaciones vectorizadas: toda operación 
 trabajar con archivos planos (.csv, .txt, xlsx) en        matemática consistente que se puede ejecutar 
 Python                                                    en vectores y matrices 
 Series: estructuras 1D en Pandas que                      Datos ausentes: Conocidos como NaN en 
 almacenan vectores con índice, nombre y                   Python representan valores sin registro que 
 valores                                                   pueden llegar a ser problemáticos en los análisis. 
 DataFrames: estructuras 2D (filas x columnas)             Metódo pct_change: Implícito en pandas que 
 que son la generalización de Series en Pandas             permite calcular variaciones porcentuales de 
 Lectura de datos en pandas: se pueden leer                observaciones consecutivas para monitorear 
 archivos planos con las funciones pd_read_csv o           cambios
 pd_read _excel pero también se pueden leer                Indexación de datos en Pandas: métodos loc 
 archivos en formatos tipo .JSON y .SQL.                   y iloc que permiten extraer filas con base en 
 Panel: estructuras 3D (filas x columnas x                 condiciones
 profundidad) que permiten almacenar varios 
 Dataframes 
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Nociones básicas de Pandas
      ✓ Las estructuras de datos en Pandas
      ✓ Series y Data Frames 
      ✓ Selección de elementos
      ✓ Operaciones con datos
      ✓ Datos ausentes
      ✓ Lectura de archivos con pandas
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 25. DATA SCIENCE
       Lenguaje 
  estructurado de 
consulta SQL - Parte 
           1
Temario
               24                      25                     26
         Fundamentos               Lenguaje               Lenguaje 
          de bases de           estructurado            estructurado 
             datos                   SQL-I                  SQL-II
         ✓ Bases de datos                               ✓ Consultas de 
         ✓ Sistemas DBMS        ✓ Introducción             mayor complejidad
         ✓ Tipos de sistemas                            ✓ TCL
                                ✓ DDL
            de gestión de bases                         ✓ Tipos de comandos
            de datos            ✓ DML
                                                        ✓ Joins, agregaciones 
         ✓ Backup,                                         y agrupaciones
            conexiones, 
            auditoría
Objetivos de la clase
         Realizar la introducción al Lenguaje 
         Estructurado de Consulta (SQL)
         Conocer las sentencias para la definición y 
         modificación de la base de datos
         Conocer las sentencias para la manipulación 
         de los datos almacenados (DML)
MAPA DE CONCEPTOS              DDL
                                         Parte I
                              DML
                             Consultas 
                             complejas
Lenguaje 
estructurado de 
Consulta                        TCL
                           Otros comandos
                              Joins, 
                            Agrupaciones y 
                            Agregaciones
Introducción
Structured Query 
Language (SQL)
Structured Query Language 
(SQL)
Es un lenguaje para la manipulación de bases de 
datos relacionales. 
Se dice que es un lenguaje declarativo porque a 
través de él se indica qué se quiere hacer, pero no 
cómo hay que hacerlo.
Structured Query Language 
(SQL)
No es propiamente un lenguaje de programación 
sino más bien un sublenguaje para la data.
Se  fundamenta  en  DDL  (Data  Definition  Language), 
DML (Data Manipulation Language), DQL (Data Query 
Language) , TCL (Transaction Control Language), DCL 
(Data Control Language).
Structured Query Language 
(SQL)
Cualquier persona con conocimiento del modelo de 
datos puede interactuar con la base de datos para 
realizar consultas, añadir nuevos registros, eliminarlos, 
actualizarlos, modificar los objetos que conforman la 
base de datos e incluso modificar la estructura de la 
misma.
Sublenguajes (SQL)
DD Conjunto de sentencias para la    DCL Conjunto de sentencias para la 
        definición y modificación de          administración de los 
 L      la base de datos y sus tablas.        privilegios de los distintos 
                                              usuarios que se conectarán a la 
                                              base de datos.
DM Conjunto de sentencias para la      TCL Conjunto de sentencias para la 
        manipulación de los datos             para la gestión de 
 L      almacenados.                          transacciones.
       Para esta clase…
A lo largo del curso se mostrarán ejemplos 
utilizando la base de datos de W3School 
provista en este enlace. 
Si bien la herramienta cuenta con varias 
limitaciones en comparación con un sistema 
de gestión de bases de datos real, será 
suficiente para permitirnos un primer 
acercamiento a SQL.
Lenguaje de 
definición de datos 
(DDL)
Sentencias básicas de DDL
    CREATE
                   Mediante este tipo de operaciones es 
     ALTER         posible definir nuevas tablas, 
                   modificarlas, eliminarlas o vaciarlas.
     DROP
   TRUNCATE
CREATE
CREATE
La  creación  de  la  base  de  datos  y  su  esquema 
puede realizarse de dos maneras:
1. Utilizando  la  interfaz  gráfica  del  Sistema 
Gestor de Base de Datos (SGBD)
2. Por medio de la sentencia CREATE SCHEMA.
        EJEMPLO EN VIVO
CREATE ejemplos
                                                Estructura
            CREATE TABLE DAVID (
              'CustomerID' INT,                              CREATE TABLE visits (
Ejemplo 1   'CustomerName' VARCHAR(40),           Ejemplo      visit_id INT PRIMARY KEY,
              'ContactName' VARCHAR(40),             2           first_name VARCHAR (50) 
              'Address' VARCHAR(45),                         NOT NULL,
              'City' VARCHAR(25),                                last_name VARCHAR (50) 
              'PostalCode' VARCHAR(15),                      NOT NULL,
              'Country' VARCHAR(25),                             visited_at DATETIME,
              PRIMARY KEY ('CustomerID'));                       phone VARCHAR(20),
                                                                 store_id INT NOT NULL);
CREATE ejemplos
         CREATE TABLE 'shop'.'Customers' (
                 'CustomerID' INT,
                 'CustomerName' VARCHAR(40),             ������ Para crear una tabla de 
                 'ContactName' VARCHAR(40),              clientes con columnas 
                 'Address' VARCHAR(45),                  CustomerID, CustomerName, 
                 'City' VARCHAR(25),                     ContactName, Address, City, 
                 'PostalCode' VARCHAR(15),               PostalCode y Country ejecute
                 'Country' VARCHAR(25),
                 PRIMARY KEY ('CustomerID')
  Tipos de datos
    Algunos de los tipos de datos más comunes son:
       ✔ INT  o  INTEGER:  Un  entero  de  longitud 
            media.
       ✔ CHAR(n):  Una  cadena  de  caracteres  de 
            longitud fija n.
       ✔ VARCHAR(n):  Una  cadena  de  caracteres 
            de  longitud  variable.  Es  posible  fijar  una 
            longitud máxima n entre los paréntesis.
       ✔ BOOL  o  BOOLEAN:  Un  valor  booleano, 
            donde 0 indica falso y 1 indica verdadero.
       ✔ DECIMAL(size, d): Un número decimal de 
            punto       fijo.    Opcionalmente  se  puede 
            especificar  la  cantidad  total  de  dígitos  en 
            size y la cantidad de dígitos posteriores al 
            punto decimal en d.
Restricciones de columnas
Puede  haber  una,  ninguna  o  varias  de  las  siguientes 
restricciones:
✔ NOT NULL: indica que la columna no puede tener 
valores nulos.
✔ UNIQUE: indica  que  la  columna  no  puede  tener 
valores repetidos.
✔ PRIMARY KEY: indica que la columna es la clave 
primaria de la tabla.
✔ REFERENCES:  indica  que  la  columna  es  clave 
foránea  a  la  columna  referenciada  de  la  tabla 
referenciada.
✔ CHECK: indica  que  la  columna  debe  cumplir  las 
condiciones especificadas.
  DROP
DROP
1. Se  usa  para  eliminar  una  base  de  datos 
completa o solo una tabla. 
2. Destruye  los  objetos  como  una  base  de 
datos, tabla, índice o vista existente. 
3. También se pueden remover índices, triggers, 
constantes y permisos
        EJEMPLO EN VIVO
DROP ejemplos
                                                         Estructura
         Ejemplo 1
   DROP TABLE DAVID;                          CREATE TABLE shop(
                                                'CustomerID' INT NOT NULL,
         Ejemplo 2                              'CustomerName' VARCHAR(40) 
   DROP TABLE visits;                         NOT NULL,
                                                'ContactName' VARCHAR(40) 
                                              NOT NULL,
                      Ejemplo 3                 'Address' VARCHAR(45) NOT 
                  Creación de tabla con       NULL,
                  restricciones                 'City' VARCHAR(25) NOT NULL,
                  especiales                    'PostalCode' VARCHAR(15) NOT 
                                              NULL,
                                                'Country' VARCHAR(25) NOT 
                                              NULL,
                                                PRIMARY KEY ('CustomerID')
                                              );
DROP ejemplos
     DROP TABLE shop;
     CREATE TABLE 'shop'.'Customers' (
            'CustomerID' INT NOT NULL,
            'CustomerName' VARCHAR(40) NOT NULL,              ������ Elimina la tabla con DROP 
            'ContactName' VARCHAR(40) NOT NULL,              TABLE,  e inmediatamente crea 
            'Address' VARCHAR(45) NOT NULL,                     otra con las restricciones 
            'City' VARCHAR(25) NOT NULL,                                necesarias.
            'PostalCode' VARCHAR(15) NOT NULL,
            'Country' VARCHAR(25) NOT NULL,
            PRIMARY KEY ('CustomerID')
     );
ALTER y TRUNCATE
ALTER TABLE
1. ALTER TABLE se usa para agregar, eliminar o 
modificar columnas en una tabla existente.
2. ALTER TABLE también se usa para agregar y 
eliminar  varias  restricciones  en  una  tabla 
existente.
ALTER TABLE 
ejemplos
������ Modifica la tabla existente para incorporar las restricciones con ALTER 
                 TABLE
ALTER TABLE 'shop'.'Customers' 
CHANGE COLUMN 'CustomerName' 'CustomerName' VARCHAR(45) NOT NULL ,
CHANGE COLUMN 'ContactName' 'ContactName' VARCHAR(45) NOT NULL ,
CHANGE COLUMN 'Address' 'Address' VARCHAR(45) NOT NULL ,
CHANGE COLUMN 'City' 'City' VARCHAR(45) NOT NULL ,
CHANGE COLUMN 'PostalCode' 'PostalCode' VARCHAR(15) NOT NULL ,
CHANGE COLUMN 'Country' 'Country' VARCHAR(25) NOT NULL ;
Definiciones por defecto
Es posible asignar valores por defecto para determinadas columnas agregando DEFAULT, 
tanto en la creación de la tabla como en la modificación de la misma. ������
ALTER TABLE 'shop'.'Consumer' 
CHANGE COLUMN 'Country' 'Country' VARCHAR(25) NOT NULL DEFAULT 'Argentina' ;
EJEMPLO EN VIVO
ALTER ejemplos
Ejemplo 1       Estructura
ALTER TABLE Customers 
ADD EMAIL varchar(20);
Ejemplo 2       Estructura
ALTER TABLE Customers 
DROP EMAIL;
TRUNCATE
1. TRUNCATE  TABLE  se  usa  para  eliminar  datos 
completos de una tabla existente.
2. Permite  eliminar  la  tabla  completa,  pero 
eliminaría la estructura de la tabla completa de 
la base de datos y se necesitaría volver a crear 
esta  tabla  una  vez  más  si  desea  almacenar 
datos.
TRUNCATE
✔ En ocasiones se requiere vaciar por completo el contenido de una tabla para luego 
seguir utilizándose. 
✔ Una opción es borrar por completo la tabla con la sentencia DROP TABLE y luego volver 
a crearla con la sentencia CREATE TABLE. 
✔ Sin  embargo,  existe  una  opción  más  conveniente:  eliminar  el  contenido  con 
TRUNCATE. ������
           TRUNCATE TABLE Products; 
EJEMPLO EN VIVO
TRUNCATE ejemplos
Ejemplo 1         Estructura
ALTER TABLE Customers 
ALTER COLUMN EMAIL;
Ejemplo 2         Estructura
TRUNCATE TABLE 
Customers;
Ejemplo
     CREATE TABLE 'shop'.'Products' (
       'ProductID' INT NOT NULL,
       'ProductName' VARCHAR(20) NOT NULL,                  ������ Deberá especificar una 
       'SupplierID' INT NOT NULL,                          columna CategoryID al 
       'CategoryID' INT NOT NULL,                          momento de creación de la 
       'Unit' VARCHAR(15) NOT NULL,                        tabla Products. Esta columna 
       'Price' DECIMAL() NOT NULL,
       PRIMARY KEY ('ProductID'),                          será clave foránea a la 
       CONSTRAINT 'fk_Products_Category'                   columna CategoryID de la 
         FOREIGN KEY ('CategoryID')                        tabla Categories.
         REFERENCES'shop'.'Categories' ('CategoryID')
         ON DELETE RESTRICT
         ON UPDATE CASCADE);
Algunas consideraciones
✔ ON DELETE RESTRICT indica a la base de datos que deberá dar 
un error si se intenta eliminar una categoría para la cual existen 
productos asociados. Se realiza para mantener la integridad de 
los datos almacenados.
✔ ON UPDATE CASCADE indica a la base de datos que deberá 
permitir realizar actualizaciones de la columna CategoryID de la 
tabla  Categories,  pero  que  estas  modificaciones  deberán 
propagarse en cascada hacia el resto de tablas.
✔ A la clave foránea se le debe asignar un nombre único, en este 
caso ‘fk_Products_Category’.
Lenguaje de 
Manipulación de datos 
(DML)
Sentencias básicas de DML
SELECT FROM      para consultar una o más columnas de una 
                tabla
                para insertar de nuevas filas a una tabla.
   INSERT
                para actualizar las filas de una tabla.
  UPDATE
                para eliminar las filas de una tabla.
  DELETE
PARA RECORDAR
La sentencia SELECT FROM tiene la 
particularidad de que no produce ningún 
cambio en el estado de la base de datos. 
En cambio, las tres restantes sí tienen 
la capacidad de producir algún cambio 
en los registros almacenados, por lo que se 
deberá prestar especial atención.
SELECT
SELECT 
1. La  instrucción  SELECT  se  utiliza  para 
seleccionar datos de una base de datos.
2. Los  datos  devueltos  se  almacenan  en  una 
tabla de resultados, denominada conjunto de 
resultados.
SELECT ejemplos 
SELECT CustomerID, CustomerName, Address FROM Customers; 
������ Para seleccionar un subconjunto específico de columnas.
SELECT * FROM Customers; 
������ O bien se pueden consultar todas la columnas utilizando la wildcard * 
SELECT ejemplos 
SELECT ProductID, ProductName, CategoryID
FROM Products
WHERE CategoryID = 5 
������ Para añadir filtros sobre la consulta se añade la cláusula WHERE seguida de una 
condición  de  guarda  booleana  que  se  evalúa  para  cada  fila  resultado  de  la 
consulta.
SELECT ejemplos 
������ En toda cláusula WHERE, es posible filtrar por varias condiciones 
mediante los operadores AND y OR. 
Por ejemplo:
SELECT ProductID, ProductName, CategoryID,  Price
FROM Products
WHERE CategoryID = 5 
AND       Price     <        20; 
SELECT p.ProductID, p.ProductName, p.CategoryID
FROM Products AS p 
WHERE p.CategoryID = 5 
OR p.CategoryID = 7;
    EJEMPLO EN VIVO
SELECT ejemplos 
                          Estructura
Ejemplo 1
SELECT ProductID, 
ProductName, CategoryID 
FROM Products 
WHERE CategoryID =5;
Ejemplo 2
SELECT ProductID, 
ProductName, CategoryID 
FROM Products 
WHERE CategoryID =1 OR 
CategoryID =5 AND 
ProductName = 'Chais' OR 
ProductName= 'Inlagd Sill';
    EJEMPLO EN VIVO
SELECT ejemplos 
                          Estructura
Ejemplo 3
SELECT ProductID, 
ProductName, CategoryID 
FROM Products 
WHERE (CategoryID =1 OR 
CategoryID =5) AND 
(ProductName = 'Chais' OR 
ProductName= 'Inlagd Sill’);
Ejemplo 4
 SELECT ProductID, ProductName, 
 CategoryID, Price FROM Products 
 WHERE CategoryID =5 AND Price <20;
    EJEMPLO EN VIVO
SELECT-Alias ejemplos 
Ejemplo 1
SELECT p.ProductID,       Estructura
p.ProductName, p.CategoryID
FROM Products AS p 
WHERE p.CategoryID = 5 
OR p.CategoryID = 7
       ¿Qué pasa si 
       tenemos el código 
       de abajo?
Ejemplo 2
SELECT p.ProductID,        Ejemplo 3
p.ProductName, p.CategoryID
FROM Products AS p         SELECT DISTINCT CustomerName 
WHERE p.CategoryID = 5     FROM  Customers ORDER BY 
OR p.CategoryID = 7        CustomerName, Address, City;
   #FindTheBug
Encuentra el error
      SELECT ProductName,Unit, SUM(Price) as 
      Suma, AVG(Price) as Promedio , 'Lote 1' as 
      Lote  FROM [Products]
      WHERE (ProductName LIKE 'A%') OR 
      (ProductName LIKE '%d') OR (ProductName 
      LIKE '%s')
      ORDER BY Promedio DESC
      GROUP BY ProductName
Duración: 4 minutos
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Predicados simples
Sentencias básicas de DML
         BETWEEN                  Retorna verdadero si los valores de una columna 
                                  están entre los dos valores dados.
              IN                  Retorna  verdadero  si  los  valores  de  una 
                                  determinada    columna    están     en    un 
                                  determinado conjunto de valores.
          IS NULL                 Retorna verdadero si el campo está vacío.
             LIKE                 Retorna verdadero si un determinado campo de 
                                  tipo CHAR o VARCHAR cumple con un patrón 
                                  determinado.
BETWEEN, IN ejemplos 
BETWEEN: Por ejemplo, puede ser útil para consultar un rango de productos
SELECT ProductID, ProductName, 
FROM Products
WHERE ProductID BETWEEN 3 AND 15;
IN: Es útil filtrar aquellos clientes que tienen un ID específico:
SELECT CustomerID, CustomerName FROM Customers
WHERE CustomerID IN (1, 7, 9);
IS NULL ejemplos 
Se puede utilizar para consultar aquellos productos que aún no tienen un precio asignado:
SELECT ProductID, ProductName
FROM Products WHERE Price IS NULL;
O bien, para consultar únicamente aquellos productos que sí tienen un precio 
asignado:
SELECT ProductID, ProductName
FROM Products
WHERE Price IS NOT NULL;
IS NULL ejemplos 
   Resulta útil para buscar palabras clave en una determinada columna. 
SELECT * FROM PRODUCTS
WHERE ProductName LIKE 'Chef%';
  Retornará sólo aquellos productos cuyo nombre comienza con ‘Chef’. ������
          EJEMPLO EN VIVO
Predicados simples 
ejemplos 
  Ejemplo 1
  SELECT ProductID, ProductName        Estructura
  FROM Products
  WHERE ProductID BETWEEN 3 AND 
  15;
  Ejemplo 2                            Estructura
  SELECT CustomerID, CustomerName 
  FROM Customers
  WHERE CustomerID IN (1, 7, 9);
      EJEMPLO EN VIVO
Predicados simples 
ejemplos                         Estructura
Ejemplo 3
SELECT ProductID, ProductName
FROM Products
WHERE Price IS NULL;
Ejemplo 4                      Estructura
SELECT * FROM PRODUCTS
WHERE ProductName LIKE 'Chef
%';
INSERT
INSERT
La sentencia INSERT INTO se utiliza para insertar nuevos 
registros en una tabla. Es posible escribir la declaración 
INSERT INTO de dos maneras:
1. Especificar  tanto  los  nombres  de  las  columnas 
como los valores que se insertarán
2. Si está agregando valores para todas las columnas 
de la tabla, no necesita especificar los nombres de 
las  columnas  en  la  consulta  SQL.  Sin  embargo, 
asegúrese de que el orden de los valores sea el 
mismo que el de las columnas de la tabla.
           EJEMPLO EN VIVO
INSERT ejemplos 
  Ejemplo 1                              ������ Insertamos en las columnas 
  INSERT INTO Customers                 CustomerName, City y Country 
  (CustomerName, City, Country)         de la tabla Customers los 
  VALUES (’Cardinal’, ‘Stavanger’,      valores especificados
  ‘Norway’)
  Ejemplo 2                              ������ Insertamos en las columnas 
                                        CustomerName, ContactName, 
  INSERT INTO Customers                 Adress City, PostalCode, Country 
  (CustomerName, ContactName,           los valores especificado
  Address, City, PostalCode, Country)
  VALUES (’Cardinal’, ‘Tom B.’, ‘Skag’, 
  ‘Stavanger’, ‘4000’, ‘Norway’)
UPDATE
UPDATE
1. La instrucción UPDATE se utiliza para modificar los 
registros existentes en una tabla.
2. ¡Tengan  cuidado  al  actualizar  registros  en  una 
tabla!. La cláusula WHERE especifica qué registros 
deben actualizarse. Si omite la cláusula WHERE, ¡se 
actualizarán todos los registros de la tabla!
             EJEMPLO EN VIVO
 UPDATE ejemplos 
    Ejemplo 1
    UPDATE Customers                            ������ Modificamos las columnas 
    SET ContactName =’Alfred                   ContactName y City donde el 
    Schmidt’, City =’Frankfurt’,               CustomerId es igual a 1
    WHERE CustomerId =1;
    Ejemplo 2                                   ������ Modificamos todas las filas de 
                                               la columna ContactName a 
    UPDATE Customers                           Juan siempre y cuando la 
    SET ContactName=’Jusn’  WHERE              columna Country sea igual a 
    Country = ’Mexico’;                        Mexico
DELETE
DELETE
1. La  declaración  DELETE  se  usa  para  eliminar 
registros existentes en una tabla.
2. ¡Tengan cuidado al eliminar registros en una tabla! 
Observe  la  cláusula  WHERE  en  la  instrucción 
DELETE.  La  cláusula  WHERE  especifica  qué 
registros  deben  eliminarse.  Si  omite  la  cláusula 
WHERE,  ¡se  eliminarán  todos  los  registros  de  la 
tabla!
         EJEMPLO EN VIVO
DELETE ejemplos 
  Ejemplo 1
                                      ������ Eliminamos la fila completa 
  DELETE FROM Customers              donde la columna 
  WHERE CustomerName =’Alfred        CustomerName sea igual a 
  Schmidt’                           ‘Alfred Schmidt’
  Ejemplo 2                          ������ Eliminar todas las filas de una 
                                    tabla sin eliminar el objeto 
  DELETE FROM Customers             tabla, lo cual significa que la 
                                    estructura, atributos e indices 
                                    están intactos
PARA RECORDAR
Borrado de contenido DROP y 
DELETE
No se debe olvidar que el DROP es un 
comando de DDL que elimina una base de 
datos o una tabla, mientras que el DELETE 
es un comando de DML que elimina los 
registros de una tabla pero mantiene la 
estructura general
¡Ambos comandos se deben usar con 
cuidado!
   Actividad colaborativa
Uso de lenguaje DML
Utilizaremos la base de datos en este enlace para responder las siguientes 
preguntas:
1. Identificar a los clientes de la tabla Customers que viven en París o 
Londres (8 registros)
2. Encontrar en la tabla OrderDetails las órdenes cuya cantidad está 
entre 10 y 15 y además que el OrderDetailID sea mayor que 6. (122 
registros)
3. Seleccionar todos los registros de la tabla Products donde el 
ProductName empiezan por la letra C (9 registros)
Duración: 10-15 minutos. Grupos 3-4 personas
Uso del lenguaje DML
Resolveremos un problema real utilizando el lenguaje 
         DML
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Uso del lenguaje 
DML
Utilizaremos la base de datos en este enlace 
1. Identificar a los clientes de la tabla Customers 
que viven en París o Londres (8 registros)
2. Encontrar en la tabla OrderDetails las órdenes 
cuya cantidad está entre 10 y 15 y además que el 
OrderDetailID sea mayor que 6. (122 registros)
3. Seleccionar todos los registros de la tabla 
Products donde el ProductName empiezan por la 
letra C (9 registros)
CLASE N°25
Glosario
DML: lenguaje que permite a los usuarios     DDL: lenguaje para crear y modificar la 
manipular datos en una base de datos         estructura de los objetos de base de datos 
(insertar, recuperar, eliminar y modificar   en una base de datos (vistas, esquemas, 
datos existentes).                           tablas, índices)
CREATE: se utiliza para crear una nueva      SELECT: selecciona datos de una base de 
tabla en una base de datos. Los              datos. Los datos obtenidos se almacenan 
parámetros de columna especifican los        en una tabla de resultados (conjunto de 
nombres de las columnas de la tabla.         resultados).
DROP: se usa para eliminar una tabla         INSERT: La sentencia INSERT INTO se 
existente en una base de datos.              utiliza para insertar nuevos registros en 
                                            una tabla de una base de datos.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ DDL (Data Definition Language)
      ✓ DML (Data Manipulation Language)
      ✓ Ejemplos de aplicación
Opina y valora 
esta clase
               Encuesta
               sobre esta clase
               Por encuestas de Zoom
               ¡Terminamos la clase! 
               Cuéntanos qué temas te resultaron más complejos de 
               entender. Puedes elegir más de uno. Vamos a 
               retomar aquellos temas que resultaron de mayor 
               dificultad en el próximo AfterClass.
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 13. DATA SCIENCE
 Modelos Analíticos 
       para DS I
Temario
               12                      13                     14
        Introducción al            Modelos                 Modelos 
            análisis            analíticos para        analíticos para 
         predictivo con               DS I                   DS II
           regresión             ✓ Modelo analítico
         ✓ Nociones                                      ✓ Recapitulación
            básicas              ✓ Machine               ✓ Aprendizaje 
         ✓ Aplicaciones             Learning                supervisado
            prácticas            ✓ Ciencia de            ✓ Clasificación
         ✓ Interpretación           datos: etapas        ✓ KNN
         ✓ Usar el modelo        ✓ Conceptos             ✓ Regresión
                                    complementario
                                    s
Objetivos de la clase
        Definir modelo analítico en Ciencia de Datos y 
        sus etapas.
        Realizar una introducción en conceptos sobre 
        Machine Learning.
        Describir tipos de Aprendizajes dentro de ML.
      MAPA DE CONCEPTOS
  Modelos 
Analíticos para                        Machine                                   Objetivo
 Ciencia de                           Learning
   Datos I
                                                                              Recolección
                                      Evolución                               Preparación
                                                                                                   Aprendizaje 
                                                                              Elección del         Supervisado
 Definición                         Importancia                                 Algoritmo
                                                                                                   Aprendizaje 
                                      Ventajas                                Entrenamient              No 
                                                                                    o              Supervisado
   Etapas                             Usuarios                                 Validación
                                        Fases                                    Deploy
Modelo analítico
¿Qué es un modelo?
¡Es una representación de una idea! 
(TODOS LOS MODELOS TIENEN ERROR)
Tipos de modelos
Modelos analíticos: representan una 
solución matemática de forma cerrada 
a unas ecuaciones sujeto a las 
condiciones iniciales.
Modelos numéricos: se basan en un 
procedimiento numérico como la 
diferencia finita o el método de 
elementos finitos para resolver 
ecuaciones complejas
¿Qué es un Modelo 
Analítico?
¿Qué es un Modelo 
Analítico?  
Es un proceso que permite 
combinar datos heterogéneos de muchas 
fuentes diferentes. 
Por lo tanto, facilita que distintos tipos 
de datos sean fusionados para 
posteriormente poder ser analizados de 
manera conjunta”. 
Existen múltiples definiciones al 
respecto. Sin embargo, elegimos esta 
definición para el desarrollo del 
curso ������
Etapas 
En la creación de 
Productos Analíticos con 
Ciencia de Datos
ERP(Enterprise Resource Planning)    OLTP (Online Transaction Processing)       ODS 
(Operational data store)
Etapas 
Este proceso está basado en una metodología 
estructurada de pasos:
Etapas 
Para pensar
Para crear un modelo analítico, utilizamos 
generalmente técnicas vinculadas con Data 
Science y con el Machine Learning, pero entonces:
¿Qué es el Machine Learning? 
¿Cuáles son sus particularidades y características 
principales?
Contesta mediante el chat de Zoom 
Inteligencia 
Artificial
                  IA se refiere a la capacidad de una 
                  máquina de realizar tareas y/o 
                  procesos, los cuales requieren de una 
                  “inteligencia” similar a la de un ser humano, 
                  desarrollándolas de la misma forma o mejor 
                  que una persona.
Componentes de IA
Para pensar
¿Alguna vez se han preguntado los diferentes 
niveles de capacidad de la IA? ¿Cuáles serían?
Contesta mediante el chat de Zoom 
Machine Learning
Machine Learning
✓ Método de análisis de datos que 
automatiza la construcción de 
“Modelos Analíticos”. 
✓ Rama de la Inteligencia Artificial 
(AI), basada en la idea de que los 
sistemas pueden aprender de 
datos, identificar patrones y 
tomar decisiones con una 
mínima intervención humana.
Aprender en este contexto quiere decir: 
identificar patrones complejos en 
millones de datos ������
Nacimiento y evolución
La Inteligencia artificial surge 
como un mecanismo para suplir 
necesidades y nace con el 
desarrollo de los primeros 
sistemas autónomos capaces de 
tomar decisiones con sistemas 
basados en recompensas
Línea de tiempo 
Fuente: Adaptado de https://www.javatpoint.com/history-of-artificial-intelligence
Tipos de algoritmos de ML
✓ Supervisados: Son aquellos donde se tiene etiqueta o 
variable respuesta. Se clasifican en problemas de 
regresión y clasificación. Se fundamenta en crear 
modelos con información histórica para luego poder 
predecir en el futuro
✓ No supervisados: Son aquellos donde no se tiene 
variable respuesta. Tiene como fin encontrar patrones 
y asociaciones ocultas en los datos, por ejemplo en 
una campaña de marketing
✓ Aprendizaje por refuerzo: funciona con un 
mecanismo de recompensa. Un agente (máquina) que 
interactúa con el entorno y prueba una variedad de 
métodos para alcanzar un resultado. El agente es 
recompensado o castigado en cada acción que realiza
Ejemplos de 
aplicación de ML
Ejemplos 
✓ ¿El automóvil de conducción 
                    ✓ ¿Saber lo que los clientes dicen 
autónoma de Google? 
                     acerca de nosotros en Twitter? 
������ La esencia del Machine Learning.
                      ������ Machine learning combinado con 
✓ ¿Ofertas de recomendación en 
                      creación de reglas lingüísticas.
línea como las de Amazon y 
                    ✓ ¿Detección de fraudes? 
Netflix? 
                      ������ Uno de los usos más importantes en 
������ Aplicaciones de ML para la vida 
                      la actualidad del ML.
diaria.
Ejemplos de aplicación       
✓ Google RankBrain: componente del 

algoritmo central de Google que utiliza 
ML para determinar los resultados más 
relevantes para las consultas del motor 
de búsqueda. (ML)
✓ Google DeepMind: DeepMind ha creado 
una red neuronal que aprende a jugar 
videojuegos de una manera similar a la 
de los humanos como una máquina de 
Turing neuronal (DL)
Ejemplos de aplicación       
         Algoritmos Facebook         Algoritmos Spotify
          ● DeepFace                  ● BART (Bandit for 
          ● EdgeRank                     Recommendations as 
          ● FBLearner flow               Treatments)
                                      ● Last.fm
         Algoritmos OpenAI           Algoritmos Microsoft
          ● OpenAI Five               ● DMLTK (Distributed 
                                         Machine Learning Toolkit) 
         Algoritmos                   Algoritmos Apple
         Netflix                       ● Proyecto CALO (Cognitive 
          ● Cinematch                    Assistant that Learns and 
                                         Organize)
        Algoritmos Twitter
          ●  Smart Image             Algoritmos Amazon
            Cropping                  ● A9
Importancia de ML
Importancia de ML
  ✓ El interés por el ML se debe a los            ✓ Es posible producir a través de 
     volúmenes y variedades                           este, generar modelos de 
     crecientes de datos                              manera rápida y  automática que 
     disponibles, el procesamiento                    puedan analizar datos grandes y 
     computacional más económico y                    complejos. 
     poderoso y el almacenaje de datos            ✓ De esta forma, una 
     asequible y mucho menos costoso.                 organización tiene una mejor 
                                                      oportunidad de identificar 
                                                      oportunidades rentables o de 
                                                      evitar riesgos desconocidos.
Beneficios
✓ Rápido sistema de toma de decisiones
✓ Mejores resultados
✓ Manejo de información multipropósito
✓ Automatización 
✓ Aplicaciones con rango amplio de uso
✓ Transformación de datos a insights
Ventajas y 
desventajas de ML
Ventajas de Machine 
Learning
✓ Mayor conocimiento de las necesidades, gustos y 
hábitos de compra de los clientes.
✓ Innovación en productos y soluciones tecnológicas.
✓ Optimización de la producción y de la productividad.
✓ Capacidad  de  realizar  acciones  preventivas  y 
correctivas.
✓ Predicción de tendencias y necesidades.
Desventajas de 
Machine Learning
 ✓ Insuficiente cantidad de               ✓ Interpretación de resultados: 
    información: ML requiere conjuntos       capacidad de interpretar con precisión 
    de datos masivos que deben ser           los resultados generados por los 
    inclusivos/imparciales y de buena        algoritmos. 
    calidad                               ✓ Alta susceptibilidad al error: si se 
 ✓ Tiempo y recursos: ML necesita            entrenan modelos con datos 
    tiempo suficiente y recursos masivos     insuficientes o con parámetros no 
    para permitir que los algoritmos         apropiados se puede tener alto error.
    aprendan y se desarrollen bien. 
¿Quien utiliza 
Machine Learning?
¿Quién utiliza?
 Servicios Financieros                   Salud                     Petróleo y Gas
Utilizan  la  tecnología  del  Tendencia      en     rápido  Principalmente para 
aprendizaje    basado     en  crecimiento en la industria de  encontrar nuevas fuentes 
máquina  para  dos  fines  atención a la salud, gracias a  de energía,  hacer análisis 
principales:     identificar  la aparición de dispositivos y  de minerales del suelo, 
insights  importantes  en  sensores  que  pueden  usar  predicción de fallos de 
los  datos  y  prevenir  el  datos  para  evaluar  la  sensores de refinerías y 
fraude.                        salud  de  un  paciente  en  optimizar la distribución 
                              tiempo real.                  de petróleo para hacerla más 
                                                            eficiente.
¿Quién utiliza?
             Gobierno                         Marketing y Ventas                            Transporte
 Dependencias                   como  Los          sitios      Web        que  Analizar datos para identificar 
 seguridad       pública      y    los  recomiendan  artículos  que  patrones  y  tendencias,  es 
 servicios  públicos  tienen  una  podrían llegar a gustarnos con  clave  para  la  industria  del 
 necesidad  particular  del  ML  base  en  nuestras  compras  transporte, que se sustenta en 
 porque        tienen      múltiples  anteriores,  utilizan  ML  para  hacer las rutas más eficientes 
 fuentes de datos de las que se  analizar  nuestro  historial  de  y                    anticipar        problemas 
 pueden  extraer  insights.  Por  compras  y  de  tal  forma  potenciales  para  incrementar 
 ejemplo,  el  análisis  de  datos  promocionar  otros  artículos  la rentabilidad.
 en las Smart Cities.                    que       podrían       llegar      a 
                                         resultarnos de interés. 
¿Quién utiliza?
   Ciberseguridad               Desarrollo Software                    Trading
Los  sistemas  normales  no  ML puede transformar todo el  Luego  de  crisis  financiera  de 
pueden mantenerse al día con  ciclo de vida del software, con  2008,    se   pudieron   crear 
los  malware  y  virus  que  se  el  fin  de  modelar  nuevas  algoritmos   que    identifican 
crean  continuamente.  ML  se  aplicaciones    con   diversas  factores    relevantes   para 
puede  utilizar  para  detectar  arquitecturas  y  experiencias  predecir   las    anomalías 
amenazas.     Los    sistemas  de  usuario  con  base  en  el  financieras.   Por    ejemplo 
asistidos  pueden  encontrar  valor  comercial  y  el  impacto  GreenKey    Technologies   AI 
estos virus y malware, e.g la  para  la  organización,  e.g  utiliza     procesamiento    de 
plataforma Tessian              plataforma Neoteric.           lenguaje  natural  (NLP)  con 
                                                            este propósito
¿Qué se requiere para 
crear buenos sistemas de 
machine learning?
✓ Recursos de preparación de datos.
✓ Algoritmos – básicos y avanzados.
✓ Automatización y procesos iterativos.
✓ Escalabilidad del Modelo en producción.
Para pensar
¿Si quisiéramos resolver un problema donde 
tenemos información georeferenciada de clientes 
cómo podríamos utilizar el ML para incrementar 
las ventas de un producto? 
Contesta mediante el chat de Zoom 
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Modelo analítico en 
Ciencia de datos y 
sus etapas
 Modelo analítico en Ciencia 
 de datos
  Los modelos analíticos más usados son:
   ✓ Clasificación: predecir categorías de interés
   ✓ Clustering:  crear  grupos  con  base  en 
       atributos en común
   ✓ Modelos  de  pronóstico:  con  el  fin  de 
       anticiparse a eventos
   ✓ Detección        de    atípicos:    identificar 
       anomalías
   ✓ Series de tiempo: para predecir tendencias 
       y comportamientos esperados
Etapas de modelo 
analítico
Etapas de modelo analítico
  Fase I:               Fase II:                Fase III:              Fase IV: 
Entendimiento          Extracción de            Limpieza de            Exploratory 
del problema              datos                    datos              Data Analysis
Fase VIII:              Fase VII:               Fase VI:               Fase V: 
Despliegue del         Validación del          Modelamiento            Selección de 
 modelo                 modelo                                        variables
                                        1 Entendimient
                                            o de 
                                            problema
                                           ✓ Esta consiste en la etapa fundamental donde 
                                              se debe comprender bien el problema que 
                                              se quiere resolver
                                           ✓ Se  debe  descomponer  el  problema  en  un 
                                              flujo de procedimientos que involucran una 
                                              perspectiva     holística    del    contexto 
                                              empresarial 
                                                  2
      Extracción de 
      datos
  ✓ Las  diferentes  fuentes  de  información 
      pueden ser First, Second, Third Party, datos 
      estructurados,  semi  estructurados  o  no 
      estructurados, los cuales deben organizarse 
      para tener un esquema lógico eficiente y de 
      fácil manipulación
  ✓ Los datos son el resultado de algún proceso 
      de   recopilación   con   ciertos   diseños 
      muestrales estadísticos
           3 Limpieza de 
              datos
               Eliminación  de  inconsistencias  dentro  de  los 
               datos, se debe lidiar con:
                ✓ Elementos duplicados que reducen variedad 
                ✓ Se debe elegir una estrategia para el manejo 
                 de  datos  nulos  (Imputación,  llenado  o 
                 eliminación)
                ✓ Se  debe  contar  con  herramientas  para  el 
                 manejo de atípicos (outliers) que afectan la 
                 varianza de los estimadores en los modelos
Exploratory                                                     4
Data Analysis
✓ Se        deben      utilizar     herramientas        de 
 visualización        para       poder       entender 
 comportamientos y patrones en los datos
✓ Se deben realizar transformaciones para el 
 análisis comparativo de variables 
✓ Un  buen  desarrollo  de  esta  etapa  puede 
 ayudar a comprender mejor las causas que 
 explican el fenómeno de estudio generando 
 los primeros insights 
                                       5      Selección de 
                                              variables
                                             ✓ También  conocido  como  Feature 
                                                Selection,  consiste  en  el  proceso  de 
                                                seleccionar      las      características 
                                                (variables) con más importancia en la 
                                                variable a predecir.
                                             ✓ Las          variables         altamente 
                                                correlacionadas      pueden      generar 
                                                inestabilidad  en  los  modelos  por 
                                                colinealidad
                                             ✓ Existen diversos métodos y algoritmos 
                                                para  realizar  este  proceso  (Wrapper, 
                                                Embebed, Filter)
                       6
Modelamiento
✓ Este es uno de los procesos más cruciales en 
el modelado de ciencia de datos, ya que el 
algoritmo de aprendizaje automático ayuda 
a crear un modelo de datos utilizable.
✓ Hay muchos algoritmos para elegir, el 
modelo se selecciona en función del 
problema.
✓ Tres tipos de aprendizaje: Supervisado 
(Regresión y Clasificación), No supervisado y 
Aprendizaje por refuerzo 
              7 Validación del 
                 modelo
               ✓ Es fundamental comprobar que nuestros 
                 esfuerzos de modelado cumplan con las 
                 expectativas. 
               ✓ El modelamiento se aplica a los datos de 
                 prueba para verificar precisión y si contiene 
                 todas las características deseables.
               ✓ Si no se logra la precisión requerida, puede 
                 volver a la Fase 5, elegir un modelo de datos 
                 alternativo y luego probar el modelo 
                 nuevamente. 
                          8
Despliegue del 
modelo
✓ El modelo que proporciona los mejores 
 resultados basado en los hallazgos de las 
 pruebas se completa y se implementa en el 
 entorno de producción siempre que se logre 
 el resultado deseado a través de las pruebas 
 adecuadas según las necesidades 
 comerciales. Con esto concluye el proceso 
 de desarrollo del modelo analítico.
Para pensar
¿Cuál de las 8 etapas de modelos analíticos 
consideran que son las más relevantes en la 
resolución de un problema?
Contesta mediante el chat de Zoom 
Conceptos 
complementarios
Conceptos 
complementarios
✓ En el aprendizaje basado en máquina ������ un destino se 
conoce como etiqueta o target.
✓ En estadística ������  un destino se conoce como variable 
dependiente.
✓ Una variable en estadística ������ se conoce como característica 
en el Machine Learning o Feature.
✓ Una transformación, en estadística ������ se conoce como 
creación de característica en el ML. 
7 Conceptos claves
✓ Features (Variables): atributos que         ✓ Aprendizaje y entrenamiento: 
  describen las instancias en el dataset       entendimiento de patrones involucrados 
✓ Feature selection: proceso de               ✓ Tuning: optimización de parámetros de 
  seleccionar las variables óptimas            un algoritmo hasta encontrar la mejor 
  para incluir en la fase de                   combinación 
  entrenamiento                             ✓ Modelos: Estructuras matemáticas que 
✓ Dataset: información de insumo,                se utilizan para obtener insights y 
  material vital para el desarrollo de         predicciones
  modelos                                   ✓ Validación: proceso para validar 
                                               desempeño de modelos
Flujo de trabajo
A continuación se ilustra el flujo 
típico de trabajo para el 
Aprendizaje supervisado. Es clave 
a nivel empresarial contar con 
sistemas de retroalimentación que 
permitan mejorar continuamente los 
modelos
Fuente:  Nguyen D. et al. (2016). Joint Network Coding and Machine Learning for Error-prone Wireless 
Broadcast
Aprendizaje no 
supervisado
A continuación se ilustra el flujo típico 
de trabajo para el Aprendizaje No 
supervisado. Algunas veces este 
tipo de aprendizaje se aplica como 
etapa previa antes del uso del 
Aprendizaje Supervisado
Investigación sobre 
         ML
¡Llevemos lo visto hasta el momento a la acción!
     Duración: 30 minutos
ACTIVIDAD EN CLASE
Investigación 
ML
Descripción de la actividad. 
1. Ingresar a la siguiente página web: 
https://trends.google.com/trends/?geo=US
2. Seleccionar su país correspondiente.
3.   Identificar que temas relacionados se observan y 
evolución temporal reciente.
4. 4.    Explorar tres temas de interés y analizar las 
tendencias del momento
        Herramienta sugerida: Padlet 
CLASE N°13
Glosario
Modelo analítico: proceso que permite              Ventajas del Machine Learning: mayor 
combinar fuentes de diferente origen para          conocimiento de sistemas, innovación, 
obtener resultados sobre algún fenómeno            optimización y predicción
en particular                                      Desventajas del Machine Learning: 
Inteligencia Artificial: capacidad de              cantidad de data a veces insuficiente, 
una máquina para realizar tareas o                 tiempo y recurso, interpretación de 
procesos más eficientemente que un                 resultados, susceptibilidad al error
humano                                             Etapas de modelo analítico: conjunto 
Machine Learning: rama de la                       de pasos (i) entendimiento del problema, 
Inteligencia Artificial que consiste en la         ii) extracción, iii) limpieza de datos, iv) 
automatización de modelos analíticos               EDA, v)selección de variables, vi) 
con sistemas que aprenden de los datos             modelamiento, vii) validación y 
para generar pronósticos y ser capaces             despliegue ) que permiten el desarrollo de 
de tomar decisiones                                un modelo analítico
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
 MATERIAL AMPLIADO
Recursos multimedia
Título
✓ ¿Qué es Machine Learning? | Computerhoy.com 
✓ Top10 - Aplicaciones del Machine Learning | SQDMCompany 
✓ ¿Cómo se hace una aplicación de Machine Learning? | 
 MindMachineTV 
Disponible en nuestro repositorio.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Modelos Analíticos en Ciencia de Datos.
      ✓ ML, Ventajas, Aplicaciones.
      ✓ Etapas y Tipos de Aprendizajes.
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 03. DATA SCIENCE
  Introducción a la 
 programación con 
       Python II
Temario
            02                 03                  04
      Introducción a      Introducción a      Introducción a 
            la                  la           librería científica 
      programación        programación         con Python: 
                                              Pandas (Parte I)
       con Python          con Python 
                           ✓ Estructura de   ✓ Estructura de 
         (Parte I)          (Parte II)
                             control            datos en Pandas 
       ✓ Definición de 
          programa         ✓ Funciones       ✓ Manipulación de 
       ✓ Instalación       ✓ Tipo de datos      Datos en Pandas
       ✓ Nociones          ✓ IPython         ✓ Lectura de 
          básicas                               archivos con 
                           ✓ Instalación        Python
Objetivos de la clase
         Conocer las distintas formas de desarrollo con 
         Python
         Comprender las nociones básicas de la 
         programación estructurada.
MAPA DE CONCEPTOS
                                Basics: Variables, 
                                asignación, 
                                operaciones
                                Estructuras
          Nociones básicas 
          de Programación
                                Funciones
                                Tipos de datos
¿Repasamos?
Introducción a la 
programación Python
✓ Definición de Programa (Lenguaje compilado 
vs interpretado)
✓ Introducción a Python
✓ Nociones básicas: variable asignación, 
expresiones
✓ Objetos y punteros
✓ Operadores: aritméticos, de asignaciones, de 
comparación y de identidad y pertenencia.
¡Vamos a Kahoot!
Estructuras de 
control
Estructuras de 
control:
FOR, WHILE, IF
¿Qué son y para qué 
sirven?
  ✓ Las estructuras de control sirven           ✓ Las estructuras de control más 
     para dar claridad y orden al                  comunes son: 
     código. 
                                                   ������ For
  ✓ Si hay que hacer operaciones 
     repetitivas, estas estructuras nos            ������ While
     ayudan a organizarlas.                        ������ If
                                                   ������ Switch (Otros lenguajes e.g C)
Estructuras                                Estructuras 
de selección                               de ciclos 
para                                       para 
generación                                 ejecuciones 
de                                         repetitivas 
condiciones
Estructura 
condicional FOR
  Estructura FOR
    ✓ Repite un comando una cantidad fija de veces
for i in range(1,10):
   print(i)                   # muestra los números del 1 al 9
for i in [1,4,6,2]:
   print(i)                   # muestra los números de la lista
Estructura 
condicional WHILE
Estructura WHILE
✓ Repite una secuencia de comandos “mientras” una condición se cumpla. 
    Cuando la condición no se cumple más, termina la repetición.
                                   i = 1
                                   while i < 10:      # el código luego de los dos puntos se ejecuta
                                         print(i)     # mientras i es menor a 10.
                                         i += 1             # cuando i llega a 10 termina la ejecución
Estructura 
condicional IF
   Estructura condicional 
   (IF)
     ✓ Si se cumple una condición, se ejecuta una secuencia de comandos. En 
          otro caso, se ejecuta otra. 
     ✓ Pueden manejarse más de dos opciones.
          x = 1
          if x < 10:                                  # Pregunto si x es menor a 10
              print(x, "es menor a 10")  # Si es así muestro mensaje
          elif x > 10:                          # Si no es así, pregunto si x es mayor
              print(x, "es mayor a 10")  # a 10 y si es así muestro mensaje
          else:                                       # Si nada de lo anterior se
              print(x, "es 10")                 # cumple, ejecuto esto
 Estructura condicional 
 (IF)
      Diagrama de flujo condicionales                  Estructura if-elif-else Python
Actividad colaborativa
Probando estructuras en Python
Deberán resolver en grupo dos problemas 
reales, utilizando las estructuras 
aprendidas de programación en Python en 
una notebook.
Duración: 15 minutos
     ACTIVIDAD COLABORATIVA
Acuerdos
Presencia                                       Apertura al aprendizaje
✓ Participar y “estar” en la clase, que          ✓ Siempre, pero siempre puedes 
    tu alrededor no te distraiga                    seguir aprendiendo. Compartir el 
                                                    conocimiento es válido, la 
Escucha activa                                       construcción colaborativa es la 
                                                    propuesta.
✓ Escuchar más allá de lo que la 
    persona está expresando 
    directamente                               Todas las voces
                                                 ✓ Escuchar a todos, todos podemos 
                                                    reflexionar. Dejar el espacio para 
                                                    que todos podamos participar.
    ACTIVIDAD COLABORATIVA
Optimizando el stock 
para una PYME
Consigna: Se tiene una lista con                ✔ Sugerencia: Crear otra lista   
Valores= [200, 225, 232, 221, 243, 256,       Dias=['Lunes','Martes','Miercoles','Jueves','V
255] que representan los precios de una       iernes','Sabado','Domingo']
acción de la compañía X la semana                   para hacer la iteración y utilice un 
pasada (cada dato representa el promedio            ciclo con la siguiente estructura 
diario)                                             for x,y in zip(Dias,Valores): hacer uso 
Escribir el código para calcular los días de 
la semana donde hubo un retroceso                   de la función np.diff y de 
respecto al día anterior en el valor de la          condicionales
acción de la compañía X.
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
  probando estructuras en 
  python
  for - if - while
                     Dias= 
 Ejercicio 1:        ['Lunes','Martes','Miercoles','Jueves','Viernes','Sabado','Domingo']
                     Valores= [200, 225, 232, 221, 243, 256, 255]
 Solución            import numpy as np
                     Dif= np.diff(Valores, n=1)
                     for x,y in zip(Dias[1:],Dif):
                      if (x!= 'Lunes') & (y<0):
                        print(x,y)
                      
Funciones, 
argumentos y 
retorno
Funciones
                Funcione
                s
               ✓ Para trabajar profesionalmente en programación, 
                 el código que se usa en forma repetitiva se 
                 organiza en funciones. 
               ✓ Puede hacerse una analogía con una función 
                 matemática y = f(x): la función f recibe un 
                 argumento x, ejecuta una serie de comandos y 
                 devuelve un valor y.
Argumentos y 
retorno
Argumentos y 
retorno
Las funciones tienen al menos 3 elementos:
������ El nombre de la función
������ Cero o más argumentos (variables de entrada)
������ Un valor de retorno (salida de la función) 
Sintaxis de una 
función
 print(x)          # función nativa de Python que muestra el valor de x
 print(x,y)        # print puede mostrar los valores de más de una variable
 def suma(x,y): # aquí definimos una función propia con argumentos x e y
     z = x + y     # la función suma los valores x e y, y asigna resultado a z
     return z      # el valor de retorno es z
 res = suma(2,3)         # aplicamos la función definida a los números 2 y 3
                         # y guardamos el resultado en res
 print (res)       # mostramos el resultado: 2 + 3 = 5
Ejemplo
def suma(x,y): # Aquí definimos una función “suma”. 
   z = x + y # Esto es lo que pide el ejercicio
   return z
res = suma(2,3)    # Aquí probamos la función suma con dos números concretos
                  # Esta es la prueba para verificar que el código funciona
print (res)
Ejemplo en vivo
¿Cómo el uso de funciones permite resolver 
un problema real?
 Ejemplo aplicado de 
 funciones
   Un inversor financiero está                       También sabe que la probabilidad 
   interesado en invertir en la                      de obtener ganancias 
   compañía. Se propone comprar                      (aproximadamente 15% en cada 
   durante cada día de la semana                     inversión) es de 0.56 y la 
   20 acciones.                                      probabilidad de perder el 18% 
                                                     es 0.44 (Solo hay esas dos 
                                                     opciones).
Ejemplo aplicado de 
funciones
 ✓ ¿Cuál sería el valor esperado al        El precio de la acción cada día entre 
    final  de  la  semana  para  el        Lunes-Domingo es: Valores= [200, 
    inversor?                              225, 232, 221, 243, 256, 255]
 ✓ Crear una función llamada 
    retorno_semanal que calcule el         Pueden utilizar la siguiente fórmula:
    valor esperado con la cantidad 
    de acciones compradas cada 
    día, probabilidad de ganancia 
    y no ganancia
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Tipos de datos
Tipos de datos en 
Python
Tipo de dato
✓ Define qué tipos de operaciones se puede 
hacer con él. Por ejemplo, un número se              REEMPLAZAR 
puede sumar, pero un texto no.                       POR IMAGEN
✓ Python define dos grandes grupos de tipos 
de datos: simples y estructurados.
✓ Podemos saber el tipo de un dato x con la 
función type(x)
Fuente: Mixtrategy.com
Datos Simples
✓ Los tipos de datos simples están formados por un solo objeto de un solo tipo
       Tipo     Ejemplo            Definición
        int     x = 1              Enteros
       float    x = 1.0       Punto flotante (decimales)
      complex  x = 1 + 2j   Complejos (parte real e imaginaria)
       bool    x = True    Booleanos o lógicos: verdadero / falso
        str    x = 'abc'            Texto
      NoneType x = None   Tipo especial para indicar valores nulos
  Datos 
  ✓ Los tipos de datos estructurados están formados por más de un objeto.
  Estructurados
  ✓ El más utilizado es list, pero no es la única forma de trabajar con este tipo 
      de datos.
           Tipo         Ejemplo                        Definición
            list        [1, 2, 3]                    Lista ordenada
           tuple        (1, 2, 3)                Lista ordenada inmutable
           dict        {'a':1, 'b':2,     Diccionario: conjunto de pares clave:valor
                         'c':3}
            set         {1, 2, 3}      Conjunto, a la manera de un conjunto matemático
Mutabilidad
  Mutabilidad
   ✓ La estructura list es mutable 
       porque permite que sus                    ✓ La estructura dict, por su 
       elementos sufran modificaciones               parte, es mutable en sus 
       una vez definida.                             valores. Sin embargo, es 
   ✓ Por otro lado, las estructuras                  inmutable en sus claves.
       inmutables como las tuplas 
       (tuple) no admiten esta 
       reasignación de elementos en 
       tiempo de ejecución del 
       programa.
IPython, trabajo con 
Notebooks
Creando un jupyter 
notebook
  IPython y 
  notebooks
  La clase pasada hablamos un poco             Esto imprimirá cierta información 
  sobre los notebooks. Veamos un poco          sobre el notebook server en su 
  más a detalle el tema������                      terminal, incluida la URL de la 
  Para iniciar el cuaderno jupyter, se         aplicación web (de forma 
  debe escribir el siguiente comando en        predeterminada, http: // localhost: 
  la terminal:                                 8888) y luego abrirá su navegador 
                                               web predeterminado a esta URL.
       jupyter notebook
IPython y 
Una vez que se abre, verán un 
notebooks
panel, que mostrará una lista de 
archivos y subdirectorios en el 
directorio donde se inició el 
servidor de la libreta. 
La mayoría de las veces, desearía 
iniciar un servidor de notebooks 
en el directorio de nivel más alto 
que contenga cuadernos. A 
menudo, este será su directorio 
de inicio.
IPython y 
notebooks
Para crear un nuevo cuaderno, 
haga clic en el botón nuevo (New) 
en la esquina superior derecha. 
Haga clic en él para abrir una lista 
desplegable y luego, si hace clic 
en Python3, se abrirá un nuevo 
cuaderno.
IPython y 
notebooks
Celdas en jupyter 
notebook
Celdas
Las celdas pueden considerarse 
como el cuerpo del Jupyter. 
Existen tres tipos de celdas:
1. Código
2. Markdown
3. Raw NBConverter
1.Código
Es donde se escribe el código y, 
cuando se ejecuta, el código 
mostrará la salida debajo de la 
celda. En este ejemplo se crea un 
código simple de la serie 
Fibonacci.
2. 
Markdown
Markdown es un lenguaje de 
marcado popular que es el 
superconjunto del HTML. Jupyter 
Notebook también admite rebajas. 
El tipo de celda se puede cambiar.
2. 
Markdown
El encabezado se puede 
agregar anteponiendo 
cualquier línea con un 
'#' único o múltiple 
seguido de un espacio.
2. 
Markdown
Se puede agregar Orden 
de Jerarquía usando el 
signo '*'. 
2. 
Markdown
Permite añadir Ecuaciones en formato Latex y tablas
3. Raw 
Se proporcionan celdas sin 
NBConverter
procesar para escribir la salida 
directamente. Esta celda no es 
evaluada por el cuaderno de 
Jupyter. Después de pasar por 
nbconvert, las celdas sin formato 
llegan a la carpeta de destino sin 
ninguna modificación.
Kernel
Kernel
Un kernel se ejecuta detrás de cada 
notebook. Siempre que se ejecuta 
una celda, el código dentro de la 
celda se ejecuta dentro del kernel y 
la salida se devuelve a la celda para 
que se muestre.
Entonces, si se importa un módulo en 
una celda, ese módulo estará 
disponible para todo el documento, 
por ejemplo:
Kernel
Jupyter Notebook ofrece varias                    3. Reiniciar y ejecutar todo: Esto 
opciones para kernels. Esto puede ser                hará lo mismo que “Reiniciar y 
útil si se desea restablecer cosas. Las              borrar el output”, pero 
opciones son:                                        también ejecutará todas las 
 1. Reiniciar: Esto reiniciará los                  celdas en el orden de arriba 
    núcleos, es decir, borrará todas                hacia abajo.
    las variables que se definieron,             4. Interrumpir: Esta opción 
    borrará los módulos que se                       interrumpirá la ejecución del 
    importaron, etc.                                 kernel. Puede ser útil en el 
 2. Reiniciar y borrar el output: Hará               caso de que los programas 
    lo mismo que “Reiniciar”, pero                   continúen para su ejecución 
    también borrará toda la salida                   o si el kernel se atasca en 
    que se mostró debajo de la celda.                algún cálculo.
Untitled.ipy
Cuando se crea el cuaderno, 
nb
Jupyter Notebook nombra el 
cuaderno como Untitled.ipynb de 
forma predeterminada.
Para cambiar el nombre del 
cuaderno, simplemente haga clic 
en la palabra Untitled.ipynb. Esto 
abrirá un cuadro de diálogo 
titulado “Cambiar nombre del 
cuaderno”. Ingrese el nombre 
válido para su cuaderno en la barra 
de texto, luego haga clic en 
Aceptar.
Instalación de 
Jupyter notebooks, 
uso de Google Colab
Jupyter Notebooks
 Jupyter Notebook es una aplicación 
 cliente-servidor lanzada en 2015 por la        Se ejecuta desde la aplicación web cliente 
 organización sin ánimo de lucro                que funciona en cualquier navegador 
 Proyecto Jupyter. Permite crear y              estándar. 
 compartir documentos web en formato            El requisito previo es instalar y ejecutar en 
 JSON.                                          el sistema el servidor Jupyter Notebook 
 Tiene celdas que permiten almacenar            por medio de Anaconda (ver 
 código, texto (en formato Markdown),           Instalacion Anaconda). 
 fórmulas matemáticas y ecuaciones, o           Los documentos creados en Jupyter 
 también contenido multimedia (Rich             pueden exportarse, entre otros formatos, 
 Media).                                        a HTML, PDF, Markdown o Python (-py 
                                                o .ipynb) y también pueden compartirse 
                                                con otros usuarios por correo electrónico, 
                                                utilizando Dropbox o GitHub 
Google Colab
Para utilizarlo basta con acceder a 
nuestra cuenta de Google y, o bien 
entrar directamente al enlace de 
Google Colab. 
Otra opción es acceder a Google Drive, 
pulsar el botón de «Nuevo» y 
desplegar el menú de «Más», luego 
seleccionar «Colaboratory» y crear un 
nuevo cuaderno (notebook).
Google Colab
Cuando creamos un nuevo cuaderno,         La máquina en un inicio cuenta con 
este es «estático», es decir, vemos su    12 GB de RAM y 50 GB de 
contenido, pero no estamos conectados     almacenamiento en disco disponibles 
a ningún entorno de ejecución.            para el uso.
Nuestro cuaderno se conecta a una VM 
de Google Compute Engine (la 
infraestructura de máquinas virtuales 
de Google en la nube) cuando 
ejecutamos una celda o pulsamos sobre 
el botón de «Conectar».
Actividad colaborativa
Cálculo de estadística descriptiva básica
Aplicando conceptos de programación 
estructurada para obtener resúmenes 
numéricos
Duración: 20 minutos
    ACTIVIDAD COLABORATIVA
Optimizando el stock 
para una PYME
Consigna: Utilizaremos la información de      ������ Crear una función que itere sobre cada 
la Clase 2 asociada con las acciones de       columna de las acciones e identifique 
diversas compañías para resolver las          valor maximo y minimo 
siguientes consignas:                         Hint: Pueden usar las 
������ Por medio de un ciclo (For o While)        funciones .mean(), .std(), .var(), .min(), .m
obtener el promedio, desviación estándar      ax() de Pandas
y varianza de cada una de las acciones en 
cada columna                                  Se recomienda trabajar en grupos de 2 o 
                                             3 estudiantes. 
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
CLASE N°3
Glosario                                           If: estructura de selección que permite extraer 
                                                   condiciones de interés
Estructura de control: nos permiten hacer          Funciones: estructuras de programación con 
operaciones repetitivas y nos dan orden y          nombre propio que reciben argumentos y arrojan 
claridad en el código, se dividen en selección y   un resultado
cíclicas, mientras que las más comunes son (for, 
while, if)                                         Tipos de datos: estructuras básicas que 
                                                   permiten realizar operaciones (e.g. números, 
Estructuras de selección: son aquellas que         bool, diccionarios, tuplas, listas, strings)
nos permiten generar condiciones para una 
operación                                          Mutabilidad: cualquier objeto que sea 
                                                   susceptible de modificar sus valores luego de ser 
Estructuras cíclicas: nos permiten realizar        creado
operaciones repetitivas para una operación
                                                   Tipos de celdad Jupyter notebook: pueden 
For: estructura cíclica que repite varias veces    ser de tres tipos: 1) código 2) Markdown (para 
una tarea (hacer hasta)                            texto y opciones HTML) y 3) RawNBConverter 
While: estructura de control cíclica que permite   (sin procesar que no tienen efecto en el código) 
hacer hasta que se cumpla una condición de 
pare (hacer mientras)
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
 MATERIAL AMPLIADO
Recursos multimedia
✓ Guia de instalación Anaconda
Disponible en nuestro repositorio.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Nociones básicas: Estructuras de Control, 
        Operadores y Funciones.
      ✓ Tipos de datos 
      ✓ Ipython trabajo con notebooks
      ✓ Instalación de Jupyter notebooks, uso de Google 
        Colab
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 14. DATA SCIENCE 
 Modelos Analíticos 
      para DS II
Temario
               13                      14                      15
            Modelos                 Modelos                Modelos 
        analíticos para         analíticos para         analíticos para 
              DS I                    DS II                  DS III
        ✓ Modelo analítico        ✓ Recapitulación
                                                          ✓ Modelo analítico 
        ✓ Machine                 ✓ Aprendizaje 
           Learning                  supervisado          ✓ Reglas de 
                                  ✓ Clasificación            asociación
        ✓ Ciencia de                                      ✓
           datos: etapas          ✓ KNN                      Reducción de 
                                                             dimensionalidad
        ✓ Conceptos               ✓ Regresión
           complementario
           s
Objetivos de la clase
         Profundizar en el tipo de Aprendizaje 
         Supervisado.
         Identificar algoritmos principales de 
         Clasificación y Regresión.
      MAPA DE CONCEPTOS
                                                                                             Árbol de 
                                                                Clasificación                decisión
                                                                                             K-Nearest-
                                                                                             Neighbor
     Modelos                                                                                 Regresión 
     Analíticos para                                                                         logística
                                  Aprendizaje 
     Ciencia de Datos             Supervisado
     II
                                                                Regresión
Momento de repaso
Recapitulación de 
la clase anterior
Para pensar
¿Qué es Machine Learning?
¿Qué es un Modelo Analítico? 
Contesta mediante el chat de Zoom 
Repaso…
En la clase de Modelos Analíticos para      Entonces son diferentes algoritmos que 
DS I, comentamos que Machine Learning,      nosotros podemos desarrollar, en función 
es un método de análisis de datos que       de la problemática del negocio como así 
automatiza la construcción de “Modelos      también, del tipo de aprendizaje que 
Analíticos”.                                queramos aplicar.
¿Y ahora?
En esta sesión, vamos a profundizar en el      Así que primero, recordemos:
Tipo de Aprendizaje Supervisado, sus            ������ ¿Cuál es el objetivo principal del 
características, particulares y los            Aprendizaje Supervisado? 
principales algoritmos que podemos 
encontrar.                                     Predecir las respuestas que habrá en 
                                              el futuro, gracias al entrenamiento 
                                              del algoritmo con datos conocidos 
                                              del pasado (datos históricos).
Aprendizaje
supervisado
Aprendizaje 
supervisado
✔ Es  una  subcategoría   del  aprendizaje 
  automático y la IA. 
✔ Se define por el uso de conjuntos de datos 
  etiquetados  para  entrenar  algoritmos  que 
  clasifiquen  datos  o  predigan  resultados  con 
  precisión. 
✔ El  aprendizaje  supervisado  ayuda  a  las 
  organizaciones a resolver una variedad de 
  problemas del mundo real a gran escala, 
  por ejemplo clasificar el correo no deseado o 
  detectar fraude.
Recapitulación: Tipos 
problemas aprendizaje 
supervisado 
En la clase 12 vimos que existían dos           A la hora de decidir cuál tipo de algoritmo 
tipos de problemas comunes en el                a usar debemos tener claro lo siguiente…
Aprendizaje Supervisado.
Recapitulación: Tipos 
problemas aprendizaje 
supervisado 
              Regresión                                  Clasificación
✔ Requiere la predicción de una              ✔ Requiere variable objetivo con dos o 
    variable continua.                           más clases.
✔ Puede tener como entrada valores           ✔ Puede tener variables de entrada 
    continuos o discretos.                       discretas o continuas.
✔ Un problema con múltiples variables        ✔ Un problema con dos clases se 
    de entrada a menudo se denomina              denomina problema de clasificación 
    problema de regresión multivariante.         binaria y con más de dos 
                                                 clasificación multiclase.
Clasificación
Problemas de 
clasificación
Reconocen entidades específicas dentro del                           REEPLAZAR 
conjunto de datos e intenta obtener conclusiones                     POR IMAGEN
sobre cómo esas entidades deben etiquetarse o 
definirse. 
Los algoritmos de clasificación comunes son 
clasificadores lineales, máquinas de vectores de 
soporte (SVM), árboles de decisión, k-nearest 
Neighbor y Random Forest
Tipos de 
Problemas en 
clasificación                                                               REEMPLAZAR 
                                                                            POR IMAGEN
Tenemos dos grandes tipos: problemas de 
clasificación binaria y multiclase
 ✔ Clasificación binaria: Clasifica los datos en dos 
     clases, como Si/No, bueno/malo, alto/bajo, 
     padece una enfermedad en particular o no, 
     etc.
 ✔ Clasificación multiclase: Clasifica datos en tres 
     o más clases; por ej. clasificación de 
     documentos, categorización de productos, 
     clasificación de malware
Nota: No es conveniente tener muchas categorías en los problemas de clasificación
Ejemplos 
Problemas de 
clasificación
✔ Predicción comportamiento de clientes
✔ Clasificación de documentos
✔ Clasificación de imágenes
✔ Clasificación de texto web
✔ Predicción de la tasa de clics de los anuncios
✔ Categorización de productos
✔ Clasificación de Malware
✔ Detección de fraude
✔ Análisis de sentimientos de imágenes
Otros 
ejemplos…
✔ Evaluación para ofertas promocionales
✔ Problemas de detección de anomalías
✔ Fraude en tarjetas de crédito
✔ Validación de deducciones
✔ Evaluación de solvencia crediticia
✔ Recomendaciones para liberación de órdenes
✔ Análisis de sentimiento
✔ Predicción de abandono de clientes
Tipos de algoritmos 
de clasificación
                 Algoritmos          Abreviación    Muestras 
                                                   ponderadas?
              AdaBoostClassifier         ABC            Si
             Gaussian Naive Bayes       GNB             Si
                  Classifier
                  LightGBM               LGB            Si
          Gradient Boosting Classifier   GBC            Si
             K-nearest Neighbours        KNN            No
                  Classifier
          Linear Discriminant Analysis   LDA            No
                Decision Trees           DT             No
Fuente: Adaptado de Stenhouse K et al. (2021).
En Morado algoritmos más populares 
                       Algoritmos                Abreviació         Muestras 
                                                       n          ponderadas?
               Logistic Regression Classifier         LRC               Si
             Multi-layer Perceptron Classifier       MLPC               No
                Nearest Centroid Classifier          NCC                No
               Nu-Support Vector Classifier         nuSVC               Si
             Quadratic Discriminant Analysis         QDA                No
                 Random forest Classifier             RFC               Si
                         XGBOOST                     XGB                Si
Fuente: Adaptado de Stenhouse K et al. (2021).
En Morado algoritmos más populares 
Aprendizaje Supervisado | 
Clasificación
Como vimos anteriormente existen 
múltiples algoritmos de clasificación, a 
continuación compartiremos aquellos más 
populares. 
      Árbol de decisión             K-Nearest-Neighbor             Regresión logística
Árboles de decisión
Definición
 ✔ Son estructuras matemáticas (diagramas de 
    flujo) que utilizan criterios de teoría de la 
    información como la impureza (Gini, entropía)                    REEMPLAZAR 
    para hacer segmentaciones                                        POR IMAGEN
 ✔ El aprendizaje basado en árboles de decisión 
    está ampliamente extendido en la actualidad, 
    y múltiples modelos hacen diferentes 
    implementaciones de los mismos. 
 ✔ Las primeras versiones de estos modelos 
    fueron implementados por Leo Breiman. 
 ✔ Se utilizan para problemas de Clasificación y 
    Regresión.
               Definición
               ✔ Aprenden de los datos generando reglas de tipo if-
                 else. 
               ✔ Separan los datos en grupos cada vez más 
                 pequeños de subsets de un dataset original. 
               ✔ A cada división se la conoce con el nombre de 
                 nodo. Cuando un nodo no conduce a nuevas 
                 divisiones se le denomina hoja, para luego ser 
                 considerada como ramas del árbol. 
Árboles de Decisión
                REEMPLAZAR 
                POR IMAGEN
Partes de los árboles 
de decisión
               Partes de los 
               Árboles de 
               Decisión
               ✔ Nodo raíz: Representa a toda la población o 
                 muestra y esto se divide en dos o más conjuntos 
                 homogéneos.
               ✔ División: Es un proceso de división de un nodo en 
                 dos o más subnodos. 
               ✔ Nodo de decisión: Cuando un subnodo se divide 
                 en subnodos adicionales, se llama nodo de 
                 decisión.
               ✔ Nodo de hoja / terminal: Los nodos sin hijos (sin 
                 división adicional) se llaman Hoja o nodo terminal.
Partes de los 
Árboles de 
Decisión
✔ Poda (Pruning): Consiste en la reducción del 
tamaño de los árboles de decisión eliminando 
nodos.
✔ Rama / Subárbol: Una subsección del árbol de 
decisión se denomina rama o subárbol.
✔ Nodo padre e hijo: Un nodo, que se divide en 
subnodos se denomina nodo principal de 
subnodos, mientras que los subnodos son hijos de 
un nodo principal.
                REEMPLAZAR 
                POR IMAGEN
Ventajas y 
desventajas
Ventajas
✔ Caja blanca (conjunto de reglas con 
booleanos), sus resultados son fáciles de 
entender e interpretar.
✔ Relativamente robusto cuando la complejidad 
no es tan alta.
✔ Funcionan relativamente bien con grandes 
conjuntos de datos.
✔ Combinaciones de los mismos pueden dar 
resultados muy certeros sin perder 
explicabilidad, por ejemplo, Random Forest.
               Desventajas
               ✔ Tienden al sobreajuste u overfitting de los datos, 
                 por lo que el modelo al predecir nuevos casos no 
                 estima con el mismo índice de acierto.
               ✔ Se ven influenciadas por los outliers, creando 
                 árboles con ramas muy profundas que no 
                 predicen bien para nuevos casos. 
               ✔ Crear árboles demasiado complejos puede 
                 conllevar que no se adapten bien a los nuevos 
                 datos.
               ✔ Se pueden crear árboles sesgados si una de las 
                 clases es más numerosa que otra es decir, si hay 
                 desbalance de clases.
Ejemplos de 
aplicación
Árboles de decisión- 
Variables numéricas
Exploremos una nueva idea de como 
hacer una clasificación de la siguiente 
forma:
1.  Tomar un atributo, aplicar una 
    condición
2.  Seleccionar otro atributo y chequear 
    condición
3.  En las hojas tendremos la asignación 
    final
4.  Aplicar el método con cuántas 
    variables se desee
Árboles de decisión- 
Variables lógicas
Árboles de decisión- 
Variables categóricas
✔ Este árbol de decisión se 
fundamenta en decidir si se espera o 
no.
✔ Los nodos internos representan los 
atributos testeados
✔ Branching (creación de nivel) se 
realiza de acuerdo con los valores de 
los atributos
✔ Los leaf nodes representan los 
outputs (Asignación de clases)
Árboles de decisión- 
Aplicación
Ejemplo
Ejemplo
from matplotlib import pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
# Cargar los datos
iris = datasets.load_iris()
X = iris.data
y = iris.target
# ajustar arbol de decisión simple con 
hiperparametros (defecto)
clf = DecisionTreeClassifier(random_state=1234)
model = clf.fit(X, y)
# Graficando
fig = plt.figure(figsize=(18,10))
_ = 
tree.plot_tree(clf,feature_names=iris.feature_names, 
                class_names=iris.target_names,
                filled=True)
Ejemplo
from dtreeviz.trees import dtreeviz
# Una forma diferente de ver el arbol
viz = dtreeviz(clf, X, y,
             target_name="target",
             feature_names=iris.feature_names,
             class_names=list(iris.target_names))
viz.save("decision_tree.svg") # Guardar la imagen
viz
En este caso además de mostrarnos las 
divisiones nos proporciona la cantidad de 
individuos en cada categoría así como su 
ubicación en la distribución de la variable.
         ☕
       Break
       ¡10 minutos y 
        volvemos!
KNN: K-Nearest-
Neighbor (Vecinos 
cercanos)
KNN
Puede usarse para clasificar nuevas muestras (valores 
discretos) o para predecir (regresión, valores 
continuos). 
Sirve esencialmente para clasificar valores, buscando 
los puntos de datos “más similares” (por cercanía).
KNN
                                                           2
                                                            
Entonces, supongamos el siguiente escenario: Tenemos       e
                                                           r
un Dataset con 2 Features, en el cual cada instancia       u
                                                           t
puede pertenecer a una de dos clases: “Rojo” o “Azul”.     a
                                                           e
                                                           F
                                                                   Feature 1
KNN
                                                                                 ?
                                                          2
                                                           
                                                          e
Dada una nueva instancia, de la cual no sabemos cuál      r
                                                          u
es  su clase, vamos a recurrir a sus vecinos cercanos     t
para clasificarla. La pregunta sería entonces, ¿La        a
                                                          e
clasificamos como rojo o como azul?                       F
                                                                  Feature 1
KNN                                                                k = 
                                                                  1       ?
                                                   2
Si tomamos K=1, solo miraremos al vecino más        
                                                   e
cercano.                                           r
                                                   u
                                                   t
                                                   a
Aclaración: K es el nro de vecinos.                e
                                                   F
                   Azul
KNN                                                                        k = 
                                                                           3       ?
                                                           2
                                                            
                                                           e
Si elegimos otro valor de k, por ejemplo k > 1, nuestra    r
                                                           u
clasificación cambiará significativamente.                 t
                                                           a
                                                           e
Por ejemplo, con k = 3 tenemos dos vecinos Rojos y         F
uno Azul. Por lo tanto en base a este escenario, la 
clasificación será: Rojo.
                                                                    Feature 1
Para pensar
¿Qué ventajas y desventajas creen que 
puede tener esta metodología a la hora de 
hacer clasificaciones con muchos y pocos 
datos?.
Contesta mediante el chat de Zoom 
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import datasets, neighbors
from mlxtend.plotting import plot_decision_regions
def knn_comparison(data, k): # funcion de comparacion
x = data[['X','Y']].values # Extraccion de columns
y = data['class'].astype(int).values # Clase y como int
clf = neighbors.KNeighborsClassifier(n_neighbors=k) #algoritmo
clf.fit(x, y)# Graficar la region de decision
plot_decision_regions(x, y, clf=clf, legend=2)# Añadir 
anotaciones
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Knn with K='+ str(k))                       Datos tipo: Estructura de agrupamiento 
plt.show()
# Cargar y aplicar funcion                             en forma de u
data1 = pd.read_csv('ushape.csv')
for i in [1,5,20,30,40,80]: # Para diferentes valores de k (Knn)
knn_comparison(data1, i)
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import datasets, neighbors
from mlxtend.plotting import plot_decision_regions
# Data concentrica
data2 = pd.read_csv('concertriccir2.csv')
for i in [1,5,20,30,40,60]:
knn_comparison(data2, i)
                                                          Datos tipo: Estructura de 
                                                          agrupamiento concéntricas.
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import datasets, neighbors
from mlxtend.plotting import plot_decision_regions
# Data XOR
data3 = pd.read_csv('xor.csv')
for i in [1,5,20,30,40,60]:
knn_comparison(data3, i)
                                                     Datos tipo: Estructura de 
                                                     agrupamiento XOR con formas 
                                                     no lineales.
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import datasets, neighbors
from mlxtend.plotting import plot_decision_regions
# Linear separable
data4 = pd.read_csv('linearsep.csv')
for i in [1,5,20,30,40,60]:
knn_comparison(data4, i)
                                         Datos tipo: Estructura de 
                                         agrupamiento lineal separable.
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import datasets, neighbors
from mlxtend.plotting import plot_decision_regions
# Data outliers
data5 = pd.read_csv('outlier.csv')
for i in [1, 5,20,30,40,60]:
knn_comparison(data5, i)
                                         Datos tipo: Estructura de 
                                         agrupamiento con outliers.
Consideracione
s
1. En todos los casos vemos que si elegimos k=1 
se tiene un modelo con overfit.
2. Cuando el valor es muy grande de k (e.g 60) 
tenemos un modelo con underfit excepto 
cuando tenemos outliers y en formas como 
XOR no lineales
3. Cada dataset tiene su propio requerimiento 
para el valor de k 
4. Valores altos de k pueden llevar a alto costo 
computacional
5. Cuando k es pequeño tenemos bajo sesgo pero 
alta varianza. Valores altos de k generan 
menor varianza pero mayor sesgo
Regresión logística
Regresión logística
Técnica de aprendizaje automático que 
proviene del campo de la estadística. A 
pesar de su nombre no es un algoritmo, 
sino que es un método para problemas de 
clasificación, en los que se obtienen un 
valor binario entre 0 y 1.
Definición
 ✔ Es un modelo estadístico que se 
     utiliza para determinar si una               ✔ Depende de la definición de un 
     variable independiente tiene un                  umbral para distinguir las clases 
     efecto sobre una variable                        binarias (por ejemplo, <50% mal 
     dependiente binaria (Clasificación).             escrito = no es spam,> 50% mal 
 ✔ Usualmente solo hay dos resultados                 escrito = spam). 
     potenciales.
Un problema de clasificación es identificar 
si una operación dada es fraudulenta o 
no, asociándose una etiqueta “fraude” a 
unos registros y “no fraude” a otros. 
Entonces, la Regresión Logística describe 
y estima la relación entre una variable 
binaria dependiente y las variables 
independientes. 
                                                   Ejemplo
En general, este algoritmo se puede 
utilizar para varios problemas de 
clasificación, como la detección de spam, 
predicción de la diabetes, si un cliente 
determinado comprará un producto en 
particular o si se irá con la competencia, 
hay muchos más ejemplos en donde se 
puede aplicar este algoritmo. 
                                       Matemática involucrada
✔ Lleva el nombre de la función 
utilizada en el núcleo del método, la 
Función Logística es también 
llamada función Sigmoide. 
✔ Esta función es una curva en forma 
de S que puede tomar cualquier 
número de valor real y asignar a un 
valor entre 0 y 1. 
✔ La ecuación que define la función 
sigmoide es la siguiente:
                                       Matemática involucrada
✔ Si la curva va a infinito positivo la 
predicción se convertirá en 1, y si la 
curva pasa el infinito negativo, la 
predicción se convertirá en 0. 
✔ Si la salida de la función Sigmoide es 
mayor que 0.5, podemos clasificar el 
resultado como 1 o SI, y si es menor 
que 0.5 podemos clasificarlo como 0 
o NO. 
                                                   Matemática involucrada
Por  su  parte  si  el  resultado  es  0.75, 
podemos   decir  en   términos   de 
probabilidad  como,  hay  un  75%  de 
probabilidades  de  que  el  paciente  sufra 
cáncer.
                                                Cuando usar o no la Regresión Logística
        Regresión  logística  para    K=2  clases.          Regresión  logística  para    K=3  clases.  Se 
        Siempre  nos  dará  un  límite  de  decisión        introduce a la data train una tercera clase 
        lineal. Los puntos rojos y verdes representan       denotada por el color azul. A pesar de que 
        el training data de las diferentes clases y         ahora  hay  más  de  dos  clases  las 
        la intersección entre los campos rojo y verde       regiones  de  decisión  entre  cualquier 
        representan el decisión boundary obtenido           pareja de clases sigue siendo LINEAL.
        de la regresión logística aprendiendo desde 
        la data. 
Aprendizaje supervisado / 
Clasificación
¿Los Algoritmos de Clasificación como 
Árboles de Decisión, KNN y la Regresión 
Logística son los únicos que existen? ¡Por 
supuesto que no! 
Existen muchos más como ser por ejemplo: 
Support Vector Machines (SVM), Random Forest, entre 
otros. 
                                                                                                          Ejemplo
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
X, y = load_breast_cancer(return_X_y=True)
# Separacion train/tet
X_train, X_test, y_train, y_test = train_test_split(X, y)
model = LogisticRegression(max_iter=10000, n_jobs=-1)
# Ajustar modelo
model.fit(X_train, y_train)
#Predicciones
predicciones = model.predict(X_test)
from sklearn.metrics import confusion_matrix
cf_matrix = confusion_matrix(y_test, predicciones)
import seaborn as sns
ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')                  Obtenemos un accuracy de 97% lo 
ax.set_title('Matriz de confusion con labels\n\n');                    cual es bastante bueno en primera 
ax.set_xlabel('\nValores predichos')                                   medida para un modelo simple!
ax.set_ylabel('Valores reales ');
ax.xaxis.set_ticklabels(['False','True'])
ax.yaxis.set_ticklabels(['False','True']);plt.show()
Regresión
Problemas de regresión
✔ Reconocen  las  estructuras  matemáticas  y 
  relaciones  dentro  del  conjunto  de  datos  para 
  obtener conclusiones sobre la distribución de una 
  variable numérica. 
✔ Los  algoritmos  de  clasificación  comunes  son 
  clasificadores  lineales,  máquinas  de  vectores  de 
  soporte  (SVM),  árboles  de  decisión,  k-nearest 
  Neighbor y Random Forest
Tipos de Problemas en regresión
✔ Tenemos dos grandes tipos: problemas lineales y 
no lineales
✔ Problemas  lineales:  son  aquellos  donde  los 
coeficientes que acompañan a las variables del 
modelo son lineales
✔ Problemas no lineales: son todos aquellos en 
donde  no  se  cumple  el  supuesto  del  modelo 
lineal,  por  ejemplo  una  serie  de  Fourier  o  de 
crecimiento Weibull
Ejemplos Problemas de Regresión
 ✔ Predicción comportamiento de clientes
 ✔ Pronósticos de demanda
 ✔ Pronóstico de Revenue y Profit
 ✔ Cantidad de demanda 
 ✔ Eficiencia de operaciones
 ✔ Optimización de tiempos para procesos
 ✔ Soporte de decisiones
 ✔ Corrección de errores
 ✔ Análisis preventivo y correctivo
 ✔ Evaluación de riesgo
Tipos de algoritmos 
para regresión
Tipos de algoritmos para regresión
                Algoritmos        Abreviación   Muestras 
                                               ponderadas?
             Linear Regression       LR            No
             Ridge Regression        RR            Si
             Lasso Regression        LR            Si
           Support Vector Machine    SVM           Si
                AdaBoost             AB            Si
            Elastic Net Regression   ENR           Si
              Decision Trees         DT            No
Fuente: Adaptado de Woubishet Zewdu. (2020).
En Morado algoritmos más populares 
Tipos de algoritmos para regresión
                     Algoritmos            Abreviació       Muestras 
                                                n         ponderadas?
                Polynomial Regression          LRC             Si
                Multi-layer Perceptron        MLPR             No
                     Regression
                 Stochastic Gradient          SGD              Si
                     Descending
             Gaussian Process Regression      GPR              No
                      LightGBM                QDA              Si
              Random Forest Regression         RFC             Si
                      XGBOOST                 XGB              Si
Fuente: Adaptado de Woubishet Zewdu. (2020).
En Morado algoritmos más populares 
Aprendizaje supervisado / 
Regresión
Los algoritmos de Regresión, intentan 
predecir una variable de tipo numérica o 
cuantitativa como por ejemplo: 
¿Cuánto crees que vale esta casa?
    Regresión Simple              Regresión 
                                   Múltiple
Aprendizaje supervisado / 
Regresión
           ¿Cuánto crees que vale esta casa?
$ 70,000           ❓             $ 160,000
Aprendizaje supervisado / 
Regresión
El precio de una casa:
Aprendizaje supervisado / 
Regresión
El precio de una casa:
Aprendizaje supervisado / 
Regresión
El precio de una casa:
Actividad colaborativa
¡Llevemos lo visto hasta el momento a la 
acción!
Les proponemos que puedan realizar la 
siguiente actividad.
Duración: 15 minutos
     ACTIVIDAD COLABORATIVA
Acuerdos
Presencia                                       Apertura al aprendizaje
✓ Participar y “estar” en la clase, que          ✓ Siempre, pero siempre puedes 
    tu alrededor no te distraiga                    seguir aprendiendo. Compartir el 
                                                    conocimiento es válido, la 
Escucha activa                                       construcción colaborativa es la 
                                                    propuesta.
✓ Escuchar más allá de lo que la 
    persona está expresando                    Todas las voces
    directamente
                                                 ✓ Escuchar a todos, todos podemos 
                                                    reflexionar. Dejar el espacio para 
                                                    que todos podamos participar.
    ACTIVIDAD COLABORATIVA
Modelo de regresión en 
acciones
Consigna: Utilizaremos información de       Realizaremos la actividad en grupos de 3-
precios de acciones y las medidas de        4 personas 
volatilidad y retorno para crear un modelo 
de regresión                                Tiempo: 15 minutos
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
ACTIVIDAD COLABORATIVA
Modelo de regresión en 
acciones
✔ Cargar por medio de un ciclo for en un solo dataframe los precios de las siguientes 
acciones  en  la  carpeta  de  la  clase:  Dominion  Energy  Inc.  (D),  Exelon  Corp.  (EXC). 
NextEra Energy Inc. (NEE), Southern Co. (SO), Duke Energy Corp. (DUK)
✔ Calcular volatilidad relativa (High-Low)/Open y el índice de retorno por medio de la 
fórmula: (Close/Open)-1 
✔ Crear  un  modelo  regresión  usando  como  variable  dependiente  (Volatilidad 
relativa)  e  independientes  (Open,  High,  Low,  Close,  Volume_Millions  y 
Symbol)
✔ Crear un modelo regresión usando como variable dependiente (Índice de retorno) 
e independientes (Open, High, Low, Close, Volume_Millions y Symbol)
✔ Interpretar resultado.
CLASE N°14
Glosario
Aprendizaje Supervisado: subcategoría del             Árboles de decisión: estructuras 
aprendizaje automático y la inteligencia artificial   matemáticas tipo if-else que se construyen con 
que cuenta con datos etiquetados (históricos) para    criterios de impureza (gini, entropía) y que 
aprender de comportamiento de una variable            permiten entender el comportamiento de 
particular.                                           variables categóricas o numéricas. Son 
                                                      sencillos de interpretar pero puede llegar a 
Problemas de clasificación: son aquellos donde        tener mucho overfitting.
la variable respuesta es una categoría (e.g 
predicción de fraude), puede ser binario o            KNN (K nearest neighbor): técnica de 
multiclase.                                           clasificación que se fundamenta en distancias 
                                                      para encontrar pertenencia a categorías 
Problema de regresión: son aquellos donde la          determinadas.
variable respuesta es una variable continua (e.g 
predicción de ventas).                                Regresión logística: técnica de clasificación 
                                                      que utiliza como fundamento matemático la 
                                                      función sigmoide para encontrar probabilidad 
                                                      de poseer una característica determinada.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Aprendizaje Supervisado.
      ✓ Algoritmos de Clasificación.
      ✓ Algoritmos de Regresión.
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 48. DATA SCIENCE
Mejora de modelos de 
 Machine Learning I
Temario
               47                      48                     49
          Validación de        Mejora de modelos       Mejora de modelos 
       modelos- Métricas      de Machine Learning     de Machine Learning 
                                        I                      II
        ✓ Análisis de           ✓ Bias vs. Variance     ✓ Repaso validación 
           Clustering              Tradeoff                de modelos
        ✓ Métricas de           ✓ Validación de         ✓ Hypertuning 
           calidad para            modelos                 parameter
           Clustering
Objetivos de la clase
         Identificar conceptos asociados a la mejora 
         de modelos de Machine Learning
        MAPA DE CONCEPTOS                                                             Bias vs Variance 
                                                                                          Tradeoff
                                                                                                                Clase 
                                                                                      Validación simple         48
                                                                                          y LOOCV
                                                                                      K fold y Stratified 
                                                                                           K-Fold
                    Mejora de                                                        Hyperparamete
                   modelos de                                                             r Tuning
               Machine Learning
                                                                                       GridSearchCV
                                                                                     RandomSearchC
                                                                                              V
    Repaso
             Les proponemos tomarse unos minutos 
             para realizar un repaso de los conceptos 
              aprendidos en Kahoot, ¿están listos?
                  Profe, puedes compartir el 
                   PIN o link de acceso al 
                        juego
Bias vs Variance 
Tradeoff
Definición
Bias
Es una forma de cuantificar el error de un 
modelo  y  se  define  como  la  diferencia 
entre el valor esperado del estimador 
(es  decir,  la  predicción  media  del 
modelo) y el valor real. 
Bias
Cuando  se  dice  que  un  modelo  tiene  un 
bias muy alto quiere decir que el modelo 
es muy simple y no se ha ajustado a 
los  datos  de  entrenamiento (suele 
ser underfitting), por lo que produce un 
error    alto   en    todas     las   muestras: 
entrenamiento, validación y test.
Varianza
Es una medida de dispersión pero se utiliza 
como  estimador  de  cuánto  varía  la 
predicción  según  los  datos  que 
utilicemos para el entrenamiento.
Varianza
La mayoría de los algoritmos de Machine 
Learning  aprenden  de  los  datos  de 
entrenamiento  de  manera  aleatoria.  Así 
que  es  normal  que  todos  los  modelos 
tengan    cierta  varianza.  Aunque    si 
creamos  un  modelo  robusto,  este 
debería aprender las relaciones entre 
las variables y el target.
Varianza
✔ Un modelo con varianza baja indica que 
cambiar los datos de entrenamiento se 
producen cambios pequeños en la 
estimación.
✔ Al contrario, un modelo con varianza 
alta quiere decir que pequeños cambios 
en el dataset conlleva a grandes cambios 
en el output (generalmente 
overfitting).
   PARA RECORDAR
Bias-Variance Tradeoff
Los algoritmos que suelen tener un error de Bias 
(Sesgo) alto suelen tener una varianza baja. 
Por el contrario modelos con Bias (Sesgo) alto 
suelen tener varianza alta
      Bias Alto   Varianza                   Varianza         Bias Bajo
                  Baja                       Alto
Bias-Variance 
Tradeoff
Bias-Variance Tradeoff
Es una consideración de diseño al entrenar 
modelos de aprendizaje automático. Se 
puede ver como un paradigma a la hora de 
aplicar modelos de Machine Learning
Lo óptimo que queremos tener, es un 
modelo que tenga poco bias y poca 
varianza. Esto es lo que podríamos llamar 
un modelo robusto.
Bias-Variance Tradeoff
El problema justamente lo tenemos, cuando no contamos con 
un modelo robusto y queremos mejorar uno de estos 
errores. Aquí se encuentra el famoso “tradeoff”.
Lo óptimo que queremos tener, es un modelo que tenga poco bias y 
poca varianza. Esto es lo que podríamos llamar un modelo robusto.
Bias-Variance Tradeoff
El “tradeoff” se explica porque:
● Disminuir la varianza implica aumentar el bias.
● Disminuir el bias hace que la varianza crezca.
Esto se explica porque un modelo con bias es un modelo muy simple y un modelo con 
varianza es un modelo muy complejo.
Bias-Variance Tradeoff
Lo que debemos hacer es encontrar un 
modelo predictivo que tenga un buen 
balance entre bias y varianza.
Esto implica que el modelo no debe 
ser muy complejo ni muy simple con el 
fin de minimizar el error en las 
predicciones:
Error= Bias^2+ Varianza + Error 
Irreducible
Bias-Variance Tradeoff
Por tanto a el punto óptimo será aquel donde 
se minimice el error total, esto implica que el 
error total se minimice y la complejidad del 
modelo sea la suficiente para no permitir ya 
sea underfitting u overfitting. Con esto 
garantizamos que el modelo sea lo más 
generalista posible y no particular
Causas de overfitting
Causas del overfitting
Las causas del overfitting pueden ser diversas complicadas. Sin embargo se 
tienen tres principales:
      1                         2                         3
Ruido en el conjunto de     Mala gestión del Bias-       Falta de Datos
                         Variance Tradeoff
entrenamiento
1)Ruido en el conjunto de 
  entrenamiento 
Cuando el conjunto de entrenamiento es 
demasiado pequeño o tiene menos datos 
representativos  o  demasiado  ruido.  Esta 
situación hace que los “ruidos” tengan 
grandes    posibilidades   de    ser 
aprendidos, y luego actuar como base 
de  predicciones.  Por  lo  tanto,  un 
algoritmo  que  funcione  bien,  debería 
poder distinguir los datos representativos 
de los ruidos.
2) Mala gestión del Bias-Variance 
Tradeoff
Esto se debe a lo visto anteriormente, 
cuando    no   se   logra  gestionar 
efectivamente el equilibrio entre sesgo 
y  varianza  nuestro  modelo  incluso, 
puede    verse   afectado   por   el 
underfitting.
El caso opuesto es cuando tenemos un 
modelo que solo funciona en los datos 
de entrenamiento (overfitting)
3) Falta de datos
  Sin  dudas,  cuando  tenemos  pocos 
  registros  en  nuestro  dataset  es  un 
  problema  a  la  hora  de  lograr  una 
  generalización de nuestro modelo.
  Los datos faltantes no permiten que se 
  pueden        realizar       operaciones 
  matriciales    lo   que    dificulta  las 
  operaciones matemáticas desarrolladas 
  por los modelos
¿Cómo evitar el 
overfitting?
1)Cross Validation
   Es  una  técnica  potente  para  evitar  el 
   overfitting. Se utiliza el dataset de train 
   original    para     generar      múltiples 
   datasets  y  en  cada  uno  ajustamos  al 
   modelo.
   Existen diversos métodos para aplicar 
   está técnica como por ejemplo K-Fold y 
   LOOCV  donde  se  pueden  ajustar  los 
   hiperparametros del modelo
2) Entrenar con más datos
  Esto   permite  que  los  algoritmos 
  detectan la señal mejor. De igual forma 
  con   más  datos  se  previene  el 
  overfitting   porque     aumenta      la 
  capacidad  de  generalización  de  los 
  modelos. 
  Sin  embargo  esta  solución  puede  ser 
  costosa y en algunos casos si la data 
  que  se  agrega  es  ruido  genera 
  problemas de interpretación
3) Data Augmentation
 Es una alternativa cuando no se tienen 
 suficientes  recursos  para  conseguir 
 más datos.
 Se puede hacer que los datos que se 
 tienen sea más diversos de tal manera 
 que  el  se  entrenen  algoritmos  con 
 datasets     ligeramente      diferentes 
 generando     mayor     capacidad    de 
 discernimiento al modelo.
4) Reducir la complejidad 
   Se  disminuye  la  complejidad  del 
   modelo  con  el  fin  de  prevenir  el 
   overfitting.   Esto    se   puede  hacer 
   generando  algoritmos  que  permitan 
   elegir  variables  apropiadas  y  que 
   permitan que el número de parámetros 
   no  sea  tan  alto,  de  esta  forma  se 
   tienen modelos menos pesados y que 
   se ejecutan más rápido.
5) Regularización
  Conjunto  de  técnicas  que  permiten 
  forzar al modelo a ser más simple. Esto 
  se puede hacer utilizando funciones de 
  costo   que   penalicen    por   mayor 
  cantidad de parámetros.
  La  regularización  también  permite 
  realizar hypertuning de parámetros con 
  el fin de tener modelos más robustos
6) Uso de métodos de Ensemble
Combinar predictores de múltiples modelos 
con el fin de disminuir el error total de los 
modelos.  Existen  dos  formas  distintas 
(Bagging y Boosting) 
Bagging  genera  el  entrenamiento  de  de 
modelos  en  paralelo  para  optimizar  las 
predicciones 
Boosting consisten en usar modelos base e 
ir incrementando la complejidad 
Para pensar
Si tenemos un modelo con alto sesgo, ¿estamos 
en un escenario de Underfitting ? ¿Por qué? 
¿Cómo podríamos alcanzar el equilibrio entre 
sesgo y varianza para este caso?
Contesta en el chat de Zoom 
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Validación de 
modelos
Validación simple
Validación simple
Antes de hablar de la Validación Cruzada, 
tenemos que entender el concepto de la 
Validación Simple. Este tipo de validación 
consiste en repartir aleatoriamente las 
observaciones disponibles en dos grupos, 
uno se emplea para entrenar al modelo y 
otro para evaluarlo. 
Ventajas de Validación simple
1. Permite una validación rápida de las 
predicciones de los modelos 
utilizados
2. No requiere de mucho costo 
computacional y de tiempo para su 
ejecución
3. Es sencillo de implementar en 
diversos lenguajes de programación
Desventajas de Validación simple
1. La estimación del error es 
altamente variable, dependiendo de 
qué observaciones se incluyan como 
conjunto de entrenamiento y cuáles 
como conjunto de validación 
(problema de varianza).
Desventajas de Validación simple
2. Al excluir parte de las 
observaciones disponibles como 
datos de entrenamiento 
(generalmente el 30%), se dispone 
de menos información con la que 
entrenar el modelo y, por lo tanto, 
se reduce su capacidad (problema 
de bias).
  LOOCV
Leave One Out Cross-Validation
Este método es de tipo iterativo y 
se inicia empleando como 
conjunto de entrenamiento todas 
las observaciones disponibles 
excepto una, que se excluye 
para emplearla como 
validación.  
Leave One Out Cross-Validation
Es un caso especial de validación 
cruzada donde el número de folds es 
igual al número de instancias en el 
conjunto de datos. 
Si se emplea una única observación 
para calcular el error, este varía 
mucho dependiendo de qué 
observación se haya seleccionado.  
Leave One Out Cross-Validation
Para  evitarlo,  el  proceso  se 
repite  tantas  veces  como 
observaciones    disponibles, 
excluyendo en cada iteración 
una   observación    distinta, 
ajustando el modelo con el resto y 
calculando  el  error  con  dicha 
observación. 
Ventajas de LOOCV
1. No hay aleatoriedad en el uso de 
algunas observaciones para el 
entrenamiento frente al conjunto de 
validación como en el método común
2. Menos sesgo en general para el 
modelo, ya que el conjunto de 
entrenamiento tiene un tamaño n-1.
Desventajas de LOOCV
1. Aunque el error de prueba no tiene 
mucho sesgo, tiene una alta 
variabilidad ya que solo se usó un 
conjunto de validación de 
observación para la predicción.
2. Computacionalmente costoso 
(tiempo y energía), especialmente si 
el conjunto de datos es grande, ya 
que requiere ajustar el modelo n 
veces
PARA RECORDAR
LOOCV es un método de validación muy 
extendido ya que puede aplicarse para 
evaluar cualquier tipo de modelo. Sin 
embargo, también suele aplicarse el método 
K-Fold Cross-Validation
Ejemplo en vivo
A continuación analizaremos un ejemplo 
donde pondremos en práctica los 
conceptos de validación simple y LOOCV 
para modelos de clasificación y Regresión 
(Carpeta de clase: Ejemplo 1)
Cross Validation algoritmos 
 de Clasificación I
Utilizaremos lo aprendido en clase sobre Cross 
Validation para evaluar algoritmos Clasificación
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Cross validation 
algoritmos de 
Trabajaremos con base en lo desarrollado en clases 
Clasificación I
previas con los datos de fuga en el enlace: 
Telco Customer Churn
1. Separar los datos en train (70%)/test(30%)
2. Entrenar un árbol con ‘max_depth=10’ y calcular 
las métricas recall, precisión y accuracy
3. Utilizar las metodologías de LOOCV y validación 
simple para evaluar el desempeño del modelo
4. Evaluar el performance en general del modelo 
(¿Tendrá problemas de overfitting o underfitting?)
K-Fold Cross 
Validation
K-Fold Cross Validation
El método K-Fold Cross-Validation es 
también   un   proceso  iterativo. 
Consiste en dividir los datos de 
forma aleatoria en k grupos de 
aproximadamente  el  mismo 
tamaño, k-1 grupos se emplean 
para entrenar el modelo y uno 
de los grupos se emplea como 
validación.
K-Fold Cross Validation
Este  proceso  se  repite k veces 
utilizando un grupo distinto como 
validación  en  cada  iteración.  El 
proceso       genera k estimaciones 
del    error   cuyo  promedio  se 
emplea como estimación final.
Ventajas de K-Fold Cross Validation
1. Conjunto de validación más grande 
que en LOOCV, ofrece menos 
variabilidad en el error de prueba 
2. Menos sesgo que en el método de 
validación simple ya que el conjunto 
de entrenamiento es más grande
3. Computacionalmente NO es costoso 
como LOOCV
Desventajas de K-Fold Cross 
Validation
1. Un sesgo algo más alto que LOOCV 
debido a un conjunto de 
entrenamiento más pequeño para 
cada iteración (pero un sesgo más 
pequeño que el método común de 
validación).
2. El algoritmo de entrenamiento debe 
volver a ejecutarse desde cero k 
veces, lo que significa que se 
necesitan k veces más cálculos
Stratified K-Fold
Stratified K-Fold
Es  una  variante  mejorada  de  K-fold,  que  cuando  hace  los  splits  (las 
divisiones) del conjunto de train, tiene en cuenta mantener equilibradas las 
clases, esto significa que cada conjunto contiene aproximadamente el 
mismo  porcentaje  de  muestras  de  cada  clase  objetivo  que  el 
conjunto completo. 
Stratified K-Fold
Esto es muy útil, en el siguiente caso por ejemplo: Tenemos que clasificar 
nuestro target en “SI/NO” entonces si una de las iteraciones del K-fold 
normal tuviera muestras con etiquetas sólo “SI” el modelo no podría 
aprender a generalizar y aprenderá para cualquier input a responder “SI”. 
Ventajas de K-Fold Cross Validation
1. 'Valida' el rendimiento de su modelo 
en múltiples folds de sus datos.
2. Puede equilibrar las clases si se 
trata de un conjunto de datos 
desequilibrado.
3. Brinda una respuesta más estable 
sobre cómo se desempeña su 
modelo en diversos conjuntos de 
datos
Desventajas de K-Fold Cross 
Validation
1. K-fold realmente no funciona bien 
con datos secuenciales (e.g una 
serie de tiempo de algún tipo).
2. Si solo confía en el puntaje agregado 
final de K-fold para su modelo se 
pierde mucha información sobre el 
rendimiento de los modelos que se 
puede considerar como un riesgo
Ejemplo en vivo
A continuación analizaremos un ejemplo 
donde pondremos en práctica las técnicas 
de Cross validation y Stratified K-Fold para 
modelos de clasificación y Regresión. 
(Carpeta de clase: Ejemplo 2 y 3)
Cross Validation algoritmos 
 de Clasificación II
Utilizaremos lo aprendido en clase sobre Cross 
Validation para evaluar algoritmos Clasificación
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Cross validation 
algoritmos de 
Trabajaremos con base en lo desarrollado en clases 
Clasificación II
previas con los datos de fuga en el enlace: 
Telco Customer Churn
1. Separar los datos en train (70%)/test(30%)
2. Entrenar un RandomForest ‘max_depth=6’ y 
calcular las métricas recall, precisión y accuracy
3. Utilizar la metodología de K-Fold Cross Validation 
para evaluar el desempeño del modelo
4. Evaluar el performance en general del modelo 
(¿Tendrá problemas de overfitting o underfitting?). 
Comparar resultados con el árbol de decisión 
creado previamente
CLASE N°48
Glosario
Bias: Es una forma de cuantificar el error       Overfitting: Ocurre cuando el modelo solo 
de un modelo y se define como la                 funciona correctamente en los datos de train 
                                                pero no es capaz de generalizar lo aprendido
diferencia entre el valor esperado del 
estimador                                        LOOCV: Este método es de tipo iterativo y 
                                                se inicia empleando como conjunto de 
Varianza: Es una medida de dispersión            entrenamiento todas las observaciones 
pero se utiliza como estimador de cuánto         disponibles excepto una, que se excluye 
varía la predicción según los datos que          para emplearla como validación.  
utilicemos para el entrenamiento.                Stratified K Fold: Es una variante 
                                                mejorada de K-fold, que cuando hace los 
Bias Variance Tradeoff: Consideración            splits (las divisiones) del conjunto de train, 
de diseño al entrenar modelos de                 tiene en cuenta mantener equilibradas las 
aprendizaje automático. Es un paradigma          clases
a la hora de aplicar modelos de Machine 
Learning
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
 MATERIAL AMPLIADO
Recursos multimedia
Algoritmos de regresión
✓ RandomiSearchCV | Scikit-Learn | Enlace
Disponible en nuestro repositorio.
¿Preguntas?
Opina y valora 
esta clase
                 Resumen 
           de la clase hoy
           ✓ Bias vs Variance Tradeoff
           ✓ Validación simple 
           ✓ LOOCV
           ✓ K-Fold Cross Validation
           ✓ Stratified K-Fold
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 41. DATA SCIENCE
     Algoritmos de 
      clasificación
Temario
                40                       41                       42
          Introducción            Algoritmos de              Algoritmos de 
            a Machine              clasificación             clasificación y 
             Learning                                          Regresión
        ✓ Marco CRISP-DM y la 
            fase de ML y                                    ✓ SVM
            modelado             ✓ KNN
        ✓ Scikit-Learn                                      ✓ Ejemplos de 
                                 ✓ Random Forest               clasificación errónea
        ✓ Técnicas de Encoding                              ✓ Regresión lineal 
                                 ✓ Regresión Logística         simple y múltiple
        ✓ Feature Engineering
                                                            ✓ Optimización de 
        ✓ Flujo de trabajo                                     hiperparametros
Objetivos de la clase
         Profundizar en el Aprendizaje Supervisado 
         Identificar el funcionamiento de los modelos 
         de clasificación
 MAPA DE CONCEPTOS
                            Regresión 
                            Logística
    KNN                 Algoritmos de               Random Forest
                        clasificación
    Repaso
             Les proponemos tomarse unos minutos 
             para realizar un repaso de los conceptos 
              aprendidos en Kahoot, ¿están listos?
                  Profe, puedes compartir el 
                   PIN o link de acceso al 
                        juego
K-nearest-neighbor:
  KNN
KNN (vecinos cercanos)
Puede usarse para clasificar nuevas 
muestras (valores discretos) o para 
predecir (regresión, valores 
  continuos). 
Sirve esencialmente para clasificar 
valores, buscando los puntos de datos 
“más similares” (por cercanía).
KNN (vecinos cercanos)
Entonces, supongamos el siguiente 
escenario: 
                                        2
Tenemos un Dataset con 2 Features, en      
                                        e
                                        r
                                        u
el cual cada instancia puede pertenecer   t
                                        a
                                        e
                                        F
a una de dos clases: “Rojo” o “Azul”.
                                              Feature 1
Para pensar
Dada una nueva instancia, de la cual no 
sabemos cuál es su clase, vamos a recurrir a sus 
vecinos cercanos para clasificar ¿La clasificamos 
como rojo o azul?
Contesta la encuesta de Zoom 
KNN (vecinos cercanos)
Si tomamos K=1, solo miraremos al 
vecino más cercano.                              k = 
                                               1    ?
                                    2
Aclaración: K es el nro de vecinos     
                                    e
                                    r
                                    u
                                    t
                                    a
                                    e
                                    F
          Azul
                                          Feature 1
KNN (vecinos cercanos)
Si elegimos otro valor de k, por ejemplo 
k > 1, nuestra clasificación cambiará                k = 
                                                   3     ?
significativamente.
                                       2
                                        
                                       e
                                       r
Por ejemplo, con k = 3 tenemos dos       u
                                       t
                                       a
vecinos Rojos y uno Azul.                e
                                       F
Por lo tanto en base a este escenario, 
la clasificación será: Rojo.                    Feature 1
Para pensar
¿Conoces el algoritmo de Random Forest?
¿Alguna vez lo han utilizado?
¿Cómo lo explicarías a una persona que no lo 
conoce?
Contesta en el chat de Zoom 
Random Forest
Definición
Random Forest 
Random Forest es un tipo de Ensamble en Machine Learning en 
donde combinaremos diversos árboles de decisión. Pero 
entonces, ¿Qué son los métodos de Ensamble en ML? 
¿Cómo funciona?
Métodos de Ensamble en ML
También llamados métodos combinados, intentan ayudar a 
mejorar el rendimiento de los modelos de Machine Learning. 
Este es un proceso mediante el cual se construyen 
estratégicamente varios modelos de ML para resolver un 
        problema particular.
PARA RECORDAR
Importancia de Random 
Forest
Por otro lado, resulta importante mencionar, 
que Random Forest, al igual que el Árbol de 
decisión, son modelos de aprendizaje 
supervisado comúnmente utilizados en 
problemas de clasificación (aunque también 
puede usarse para problemas de regresión)
¿Cómo surge?
Uno de los problemas que aparecía con la 
creación de un árbol de decisión, es que si le 
damos la profundidad suficiente, el árbol 
tiende a “memorizar” las soluciones en 
vez de generalizar el aprendizaje. Es decir, 
a padecer de overfitting. La solución para 
evitar esto es la de crear muchos árboles y 
que trabajen en conjunto. 
Miremos un ejemplo de cómo funciona…
Ejemplo aplicado
                            Paso 1: Creación de un boostrapped 
Tenemos estos datos          dataset (Muestra de los datos) con mismo 
                            tamaño del original
Ejemplo aplicado
              Paso 2: Creamos un árbol de decisión con el 
              Bootstrapped Dataset pero solo seleccionando 
              algunas columnas al azar en cada paso
              Asumiremos 2 variables (columnas) en cada paso
Ejemplo aplicado
Paso 3: Volvemos al paso 1 y repetimos el proceso creando nuevos bootstrapped 
datasets y creando nuevos árboles de decisión (se puede hacer 100 veces, en este 
caso solo puse 6)
Ejemplo aplicado
Imaginemos que tenemos un            Paso 4: Utilizar el modelo en los diferentes 
nuevo paciente y queremos            árboles de decisión
saber si padecera ataque 
cardiaco 
Ejemplo aplicado
Para el segundo árbol la decisión es Yes
Para el tercer árbol la decisión es Yes
Ejemplo aplicado
Para el cuarto árbol la decisión es Yes
Para el quinto árbol la decisión es Yes
Ejemplo aplicado
Tenemos como resultado
                               Al final se busca la mayor 
                               votación y a partir de esto se 
                               toma la decisión de la 
                               clasificación final
                               Bootstrapping los datos y 
                               agregarlos para tomar 
                               decisiones se conoce como 
                               Bagging
Para pensar
¿Conoces el algoritmo de Regresión Logística?
¿Alguna vez lo han utilizado?
Contesta en el chat de Zoom 
Regresión Logística
Definición
Regresión Logística
Como vimos anteriormente, se trata 
de una técnica de aprendizaje 
automático que proviene del campo 
de la estadística. A pesar de su 
nombre, no es un algoritmo, sino que 
es un método para problemas de 
clasificación, en los que se 
obtienen un valor binario entre 0 
    y 1.
Ejemplos
Regresión Logística: 
Un problema de clasificación es 
Fraudes
identificar si una operación dada es 
fraudulenta o no, asociándolo una 
etiqueta “fraude” a unos registros y “no 
  fraude” a otros. ������
Entonces, la Regresión Logística 
describe y estima la relación entre una 
variable binaria dependiente y las 
variables independientes.
Regresión Logística: 
Si la curva va a infinito positivo la 
Fraudes
predicción se convertirá en 1, y si la 
curva pasa el infinito negativo, la 
predicción se convertirá en 0.
Si la salida de la función Sigmoide es 
mayor que 0.5, podemos clasificar el 
resultado como 1 o SI, y si es menor 
que 0.5 podemos clasificarlo como 0 o 
     NO. 
Regresión Logística: 
Fraudes
Por su parte si el resultado es 0.75, 
podemos decir en términos de 
probabilidad como, hay un 75% de 
probabilidades de que nuestro producto, 
por ejemplo en este caso, tenga éxito en 
el mercado.
Regresión Logística: Fraude
Fraude
                     Fraude
No Fraude              No Fraude
En este caso una línea recta no es apropiada, por ende se habla de una 
función que se ajuste a los datos y podemos usar la función Logística
Regresión Logística: Fraude
Fraude                 Fraude
No Fraude               No Fraude
Las X en rojo representan nuevos individuos. Como se puede observar a 
izquierda al entrar a la curva se tiene una alta probabilidad de cometer 
fraude. Sin embargo como se observa a la izquierda podemos tener diferentes 
casos de análisis. 
         ☕
       Break
     ¡10 minutos y 
     volvemos!
          ¡Lanzamos la
          Bolsa de 
          Empleos!
         Un espacio para seguir potenciando tu carrera y 
         que tengas más oportunidades de inserción 
         laboral.
         Podrás encontrar la Bolsa de Empleos en el menú 
         izquierdo de la plataforma.
         Te invitamos a conocerla y ¡postularte a tu futuro 
         trabajo!
           Conócela
Ejemplo en vivo
Analizaremos el dataset de titanic con los 
datos train_titanic.csv y test_titanic.csv 
dentro de la carpeta de clase. Revisaremos 
cómo el uso del Feature Engineering puede 
ayudar a mejorar los resultados de un 
modelo Random Forest
Elaborando un algoritmo de 
    clasificación
Aplicaremos Feature Engineering y generaremos un 
    modelos de clasificación
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Elaborando un 
algoritmo de 
En esta oportunidad nos reuniremos en grupos de 
clasificación
máximo 4 personas.
1. Elegir 4 variables independientes que consideren 
útiles para predecir el “churn” o “fuga/baja” de 
clientes
2. Realizar el “encoding” de las variables 
independientes (una persona hace el código y 
comparte, los demás ayudan dando instrucciones, 
etc) para generar matriz para el modelo
3. Elegimos uno de los modelos aprendidos en clase 
(e.g KNN, Random Forest, Reg. Logística o árboles 
de decisión) y entrenan un modelo
4. Crear matriz de confusión para evaluar 
performance
¿Preguntas?
CLASE N°41
Glosario
KNN: algoritmo de aprendizaje               Bagging: Boostrapping aggregating por 
supervisado que permite resolver            sus siglas en inglés, es un mecanismo que 
problemas de clasificación basándose en     permite reducir el overfit en los modelos 
distancias o métricas                       basados en árboles de decisión y se 
                                           fundamenta en que la mayoría hace la 
Random Forest: algoritmo de                 fuerza 
aprendizaje supervisado que permite         Regresión Logística: algoritmo basado en 
resolver problemas de clasificación         la función sigmoide, meramente estadístico 
utilizando el mecanismo bagging por         que permite resolver problemas de 
medio del uso de muchos árboles de          clasificación en el contexto de aprendizaje 
decisión a la hora de discernir la          supervisado. Ideal para clasificación 
predicción final                            binaria.
Opina y valora 
esta clase
                Resumen 
           de la clase hoy
          ✓ KNN
          ✓ Random Forest
          ✓ Regresion Logistica
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 30. DATA SCIENCE
Workshop: Limpieza 
        de Data
Temario
               29                      30                     31
     Data Wrangling II            Workshop:           Exploratory Data 
                                 Limpieza de           Analysis (EDA)
                                     datos
        ✓ Etapas de Data        ✓ Repaso Data           ✓ Análisis 
           Wrangling               Wrangling + tips        estadístico
        ✓ Data Transformation      buenas prácticas
                                ✓ Recomendaciones       ✓ Correlaciones y 
        ✓ Opciones de remoción     para Data Wrangling     variables
           de duplicados
        ✓                       ✓ ¿Cómo hacer una       ✓ Identificación de 
           Índices Jerárquicos                             outliers
                                   revisión de pares y 
        ✓ Print, GroupBy, Apply    dar feedback?        ✓ Valores perdidos
Objetivos de la clase
         Identificar algunas buenas prácticas de Data 
         Wrangling
         Comprender qué herramientas y librerías 
         permiten realizar el proceso de Data Wrangling 
         de manera eficiente
MAPA DE CONCEPTOS
                       Repaso de Data 
                         Wrangling
Recomendacione         Workshop:                Tipos de 
 s para Data                                   buenas 
 Wrangling          Limpieza Datos             prácticas
                        Revisión de 
                          pares
Cuestionario de tarea
¿Te gustaría comprobar tus 
conocimientos de la clase anterior?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
Repaso Data Wrangling + 
tips y buenas prácticas 
Repaso de Data 
Wrangling
Para pensar
¿Cúal es el objetivo de Data Wrangling? 
¿Qué implica la transformación de datos? 
¿Qué funciones de manipulación de datos 
conoces? ¿Qué diferencias hay entre ellas?
Responder en el chat de Zoom 
              Repaso
               El objetivo de hacer data wrangling es a partir de 
               un set de datos crudo, comprender sus 
               características y manipularlo para luego poder 
               extraer la información que tiene.
               La transformación de Datos puede incluir:
                ✔ Fusiones de DataFrames
                ✔ Eliminación de duplicados
                ✔ Agrupamiento de las tablas
                ✔ Tratamientos de valores nulos
                ✔ Operaciones con los distintos tipos de datos
Tips de buenas 
prácticas
Aprender estructuras de 
datos
 ✔ Python ofrece una variedad de estructuras de 
    datos  sorprendentes  con  excelentes  métodos 
    integrados. 
 ✔ Los  diccionarios  actúan  como  mini  base  de 
    datos en memoria con pares clave-valor. 
 ✔ Explore    otras    bibliotecas  integradas 
    relacionadas con estas estructuras de datos. 
 ✔ Cree su propia versión  de  las  estructuras  de 
    datos esenciales, como stack, queues, heaps y 
    trees, utilizando clases y estructuras básicas, y 
    mantenerlas  para  una  recuperación  y  un 
    recorrido rápidos de los datos.
Aprender manejo de datos
✔ Es importante  saber  cómo  abrir  y  manipular 
archivos
✔ También  es  importante  manipular  y  navegar 
por la estructura de directorios
✔ El  manejo  de  datos  es  muy  importante  a  la 
hora de flexibilizar procesos 
Entender capacidades de 
Numpy y Pandas
✔ Es importante crear, acceder, ordenar y buscar 
dentro de arrays en Numpy.
✔ Tratar  de  reemplazar  ciclos  for  con  una 
operación  vectorizada.  Esto  aumentará  la 
velocidad de su operación de datos.
✔ Explore tipos de archivos especiales como .npy 
(almacenamiento  nativo  de  Numpy)  para 
acceder/leer  grandes  conjuntos  de  datos  con 
una  velocidad  mucho  mayor  que  la  lista 
habitual
Tener capacidad sólida en 
estadística
✔ Ejecutar algunas pruebas estadísticas estándar 
puede  darte  rápidamente  una  idea  sobre  la 
calidad  de  los  datos  con  los  que  necesitas 
lidiar.
✔ Graficar  datos  a  menudo,  incluso  si  son 
multidimensionales.  No  intente  crear  gráficos 
en 3D sofisticados. 
✔ Utilizar  boxplots  para  ver  la  dispersión  y  el 
rango  de  los  datos  para  detectar  valores 
atípicos.
Aprender más de un 
lenguaje de programación
✔ Deben  ser  capaces  de  manipular  un  gran 
almacenamiento en bases de datos.
✔ Siempre  es  una  buena  idea  conocer  los 
conceptos básicos de administración de bases 
de datos y álgebra relacional.
✔ Entender  el  mundo  del  Big  y  Massive  Data 
(tecnologías como Hadoop/Pig/Hive/Impala). El 
conocimiento  de  Data  Wrangling  es  muy 
importante para lidiar con estos escenarios.
Recomendaciones 
para hacer Data 
Wrangling
Recomendaciones
✔ Filtra tus datos para aligerar la carga
✔ Considera el resultado deseado a lo largo de la 
manipulación del dato
✔ Mantener siempre la capacidad de retroceder a 
una versión anterior de los datos.
✔ Entender dónde y cómo están guardados los 
datos
✔ Hacer un diccionario de datos
✔ Incluir un experto en la materia siempre que 
sea posible
Filtrar datos
✔ Filtrar los datos que necesites hará el proceso de 
data wrangling más simple y rápido
✔ Asegura no gastar recursos extra en tareas 
innecesarias.
✔ Incrementa la eficiencia de cálculos en fusión 
de datos 
Tener en cuenta el output 
deseado
✔ Es importante tener una idea clara de cuál es el 
resultado final deseado luego del proceso de 
data wrangling. Por ejemplo definir los niveles de 
granularidad.
✔ Por ejemplo cómo sería la forma con un conjunto 
específico de observaciones
✔ Esto ayuda a saber que tareas realizar para 
obtener dicho resultado mientras uno cura el set 
de datos
Tener una versión de los datos
✔ La idea detrás de esto es si uno toma una 
decisión equivocada poder volver atrás de 
forma sencilla 
✔ Por ejemplo, cuando uno trabaja con un Excel, si 
comienza a eliminar columnas y reformatear 
celdas, es posible querer la versión anterior y 
hacer diferentes elecciones de preparación de 
datos.
Saber cómo y dónde están los 
✔
datos
Tienen acceso a datos en vivo o es una captura 
diaria?
✔ Acceso directo al raw data o son datos ya 
previamente procesados?
✔ Siempre incluir un experto en la materia 
siempre que sea posible
✔ Esta información debería ser transversal a 
todos los miembros que utilicen el mismo set de 
datos
✔ Con un entendimiento claro y buena 
comunicación se puede tener una mayor confianza 
en los datos 
Hacer diccionario de datos
✔ Cuando se trabaja con datos poco familiares, uno 
se hace muchas preguntas (tipos, fuentes)
✔ Utilizar un diccionario para explicar que contiene 
cada fuente que se utiliza y que son cada variable 
es una buena práctica para orientarse uno y a 
otros acerca del set de datos
Trabajar con expertos
Trabajar con expertos en la materia no es solo una 
oportunidad para asegurarse de que está limpiando los 
datos de acuerdo con su significado. También es una 
oportunidad para mejorar los resultados. 
Escuchar el conocimiento experto de las personas que 
mejor  conocen  los  datos  le  brinda  la  oportunidad  de 
infundir  su  experiencia  técnica  con  contexto,  lo 
que lleva a mediciones más creativas de sus resultados 
clave, formas innovadoras de predecir sus resultados y 
una capacidad mejorada para comunicar resultados.
Para pensar
¿En qué situaciones han tenido que hacer 
manipulación de datos y con qué 
herramientas?
Responder en el Chat de Zoom
Actividad colaborativa
Exploración dataset GDP per capita
Formaremos grupos de 4-5 personas en el Break-out 
Rooms y utilizaremos el dataset en el siguiente enlace 
GPD y responderemos las siguientes preguntas 
1. Calcular el GPD promedio de su país durante el periodo 
2015-2021 y compartirlo con sus compañeros
2. Calcular el GPD promedio de Sudamerica y 
Norteamerica en el mismo periodo de tiempo 
Duración: 20-25 minutos
 Actividad colaborativa
Summary: Exploración dataset GDP per capita
En esta oportunidad realizamos las siguientes etapas de limpieza y 
transformación:
1. Carga y exploración inicial de datos
2. Verificación de nulos y duplicados
3. Comprensión de la estructura de datos 
4. Uso de filtros para extracción de información deseada
5. Recodificación y estandarización de valores (Data Transformation)
Cual de las fases anteriores consideran relevantes para su 
proyecto final ¿Existe algún proceso que no se encuentre en las 
fases anteriores? ¿Si es así aclararemos dudas?
Duración: 20-25 minutos
         ☕
       Break
     ¡10 minutos y 
     volvemos!
¿Cómo hacer una 
revisión de pares y 
dar feedback?
Definición
Proceso de revisión 
por pares
Es un proceso donde se indaga acerca de los 
posibles enfoques metodológicos para enmarcar 
y resolver el problema en cuestión, donde 
podemos formular nuestra recomendación sobre 
el mejor camino a seguir, complementando con 
una lista de las ventajas y riesgos inherentes a 
cada elección.
Proceso de revisión
por pares
Se realizan usualmente tres tipos de controles 
para validar un proyecto
✔ La verificación de la validez técnica, que a 
menudo se realiza junto con los ingenieros 
de datos/software que respaldan el 
proyecto
✔ Revisión de la investigación, donde un 
compañero ayuda al científico de datos a 
validar su proceso y recomendaciones. 
✔ Verificación de la validez del alcance y los 
KPI, realizada con la persona a cargo del 
producto
Puntos de control
Motivación
Se debe evitar que elijan el enfoque o la 
dirección equivocados en esta fase inicial del 
proyecto. Estos errores, denominados fallas de 
enfoque en el flujo de trabajo de DS, son muy 
costosos de cometer y obstaculizan el proyecto 
durante el resto de su ciclo de vida.
Si tenemos suerte, descubrimos la falla del 
enfoque después de varias iteraciones de 
desarrollo del modelo y perdemos preciosas 
semanas de trabajo como científico de datos.
Objetivos
Se deben detectar errores costosos desde el 
principio, analizando aspectos centrales del 
proceso.
Aquí se pueden establecer dos objetivos 
secundarios adicionales: primero, mejorar la 
capacidad del científico de datos para explicar y 
defender sus decisiones en el próximo proceso 
de revisión de productos/negocios. Segundo, 
prepararse mejor para presentar el resultado de 
la fase de investigación al resto del equipo, 
Estructura
✔ Se analiza el flujo del proyecto por medio 
de una presentación (no necesariamente 
diapositivas) del proceso de investigación 
por el que pasó.
✔ Se puede establecer una reunión larga (al 
menos 60 minutos) con el revisor.
✔ El revisor revisa la lista de verificación 
(antes de la reunión).
Checklist
Propiedades de datos
¿Cómo se generó? ¿Cómo se muestreo? ¿Estaba 
actualizado?
¿Qué ruido, sesgo de muestreo y datos faltantes 
introdujo esto?
¿Datos perdidos?
¿Puedes modelar explícitamente el ruido, 
independientemente de un enfoque?
Si el conjunto de datos está etiquetado, ¿cómo fue 
etiquetado?
¿Se puede medir?
¿Puede modificar o aumentar el proceso de 
etiquetado para compensar el sesgo existente?
Supuestos de 
aproximación
¿Qué suposiciones hace cada enfoque sobre los 
datos/el proceso de generación de datos/el 
fenómeno en estudio?
¿Es razonable hacer estas suposiciones sobre su 
problema? ¿Hasta qué grado?
¿Cómo espera que la aplicabilidad se relacione 
con la violación de estos supuestos?  
¿Se pueden validar estos supuestos de forma 
independiente?
¿Algún caso que viole estos supuestos?
Experiencia pasada
¿Qué experiencia tiene aplicando este enfoque, 
ya sea en general o para problemas similares?
¿Encontró alguna publicación de éxito/fracaso 
de la aplicación de este enfoque a problemas 
similares?
¿Te comunicaste con tus compañeros para 
conocer su experiencia?
¿Qué lecciones se pueden aprender de lo 
anterior?
Qué soluciones se usaron para resolver este 
problema hasta ahora? ¿Cuáles fueron sus 
ventajas y qué problemas sufrieron?
Alineación de objetivos
Aprendizaje supervisado: 
¿Qué funciones de pérdida se pueden utilizar al 
ajustar los parámetros del modelo? ¿Cómo se 
relacionan con los KPI del proyecto?
¿Qué métricas se pueden utilizar para la 
selección de modelos/optimización de 
hiperparámetros? ¿Cómo se relacionan con los 
KPI del proyecto? 
Aprendizaje no supervisado
¿Qué medida optimiza el método?
¿Cómo se relaciona con los KPI?
¿Cuáles son algunos casos extremos que 
satisfacen bien la métrica pero no el KPI?
Implementación
¿Existen implementaciones del enfoque en un 
lenguaje utilizado actualmente en su entorno de 
producción?
¿La implementación está actualizada? ¿Apoyado 
consistentemente? ¿En amplio uso?
¿Hay usos exitosos de esta implementación 
específica por parte de empresas/organizaciones 
similares a la suya? ¿En problemas similares al 
tuyo?
Escalamiento
¿Cómo se escala el tiempo de 
cálculo/entrenamiento con la cantidad de puntos 
de datos?
¿Con el número de características o variables?
¿Cómo se escala el almacenamiento con puntos 
de datos/características?
Requerimientos de 
información
¿En qué medida cada enfoque depende de la 
cantidad de información disponible? ¿Cuál es el 
impacto esperado en el rendimiento de 
pequeñas cantidades de información?
¿Esto se alinea bien con la cantidad de 
información actualmente disponible? ¿Con 
disponibilidad futura?
Actividad colaborativa
Revisión de Pares
Nos organizaremos en el break-out rooms en grupos de 4-
5 personas. 
Deberán mostrar el trabajo realizado en las fases 
entregables pasadas, cada estudiante dispondrá de 2 
minutos para contar brevemente sobre su código y recibir 
feedback
Se propone alternancia de roles (recibir vs. dar feedback) 
Duración: 10-15 minutos
  Data Wrangling
Aplicaremos los conocimientos aprendidos hasta el 
   momento de Data wrangling
     Duración: 15-20 mins
 ACTIVIDAD EN CLASE
Data Wrangling
Utilizaremos el archivo ‘nba_salary.sqlite’ y seguiremos los 
siguientes pasos.
✔ Leer los datos de la base nba_salary.sqlite. Extraer ambas 
  tablas.
✔ Ver cuántos datos nulos tiene  cada tabla. Analizar que 
  columnas y filas eliminaria
✔ De la tabla de Seasons_Stats, seleccionar solo el año 2017. 
  Analizar por qué hay varios jugadores que aparecen varias 
  veces en un año.  Lo mismo para la tabla From 
  NBA_season1718_salary.
✔ Hacer un inner join entre las dos tablas en base al jugador y 
  al equipo que juega con la tabla From 
  NBA_season1718_salary. 
✔ Agrupar por jugador y calcular el salario total y la cantidad 
  de puntos por año.
CLASE N°30
Glosario
Buenas prácticas Data Wrangling:                Revisión de pares: un proceso donde se 
conjunto de pasos que se deben seguir           indaga acerca de los posibles enfoques 
para permitir que el proyecto de ciencia        metodológicos para enmarcar y resolver el 
de datos funcione como debería ser              problema en cuestión, donde podemos 
                                               formular nuestra recomendación sobre el 
Recomendaciones Data Wrangling:                 mejor camino a seguir
filtrado, output deseado, versión de            Puntos de control revisión de pares: 
control, saber cómo y dónde están los           principalmente son 3 (Motivación, Objetivos 
datos, diccionario de datos y trabajo con       y Estructura). Esto nos permite validar los 
expertos                                        procesos realizados
¿Preguntas?
Opina y valora 
esta clase
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Repaso de Data Wrangling + tips de buenas 
        prácticas
      ✓ Recomendaciones para hacer Data Wrangling
      ✓ ¿Cómo hacer una revisión de pares y dar 
        feedback?
Esta clase va a ser
grabad
  a
      Clase 28. DATA SCIENCE
  Data Wrangling I
Temario
               27                      28                     29
       Data Acquisition        Data Wrangling I      Data Wrangling II
                II
         ✓ Intro Github          ✓ Data Wrangling en     ✓ Etapas de Data 
                                    proyectos DS           Wrangling
         ✓ Javascript Object 
            Notation             ✓ Etapas Data           ✓ Data Transformation
         ✓ Introducción APIs        Wrangling            ✓ Opciones de 
                                 ✓ Combinar y              remoción de 
         ✓ Conexión a modelos       fusionar objetos       duplicados
            relacionales usando     con Pandas. Merge    ✓ Índices jerárquicos
            Pandas                  y Concat
                                                         ✓ Print, GropBy, Apply
Objetivos de la clase
         Establecer la relación entre Data Wrangling y 
         un proyecto de Ciencia de datos
         Analizar cada una de las etapas de Data 
         Wrangling
         Fusionar y concatenar DataFrames
MAPA DE CONCEPTOS            Data Wrangling 
                           en proyecto de DS
                            Etapas de Data 
                Parte I      Wrangling
                            Merge y Concat
Data Wrangling                   Data 
                            Transformation
                            Remoción de 
                             duplicados
                Parte II      Índices 
                             jerárquicos
                            Print, GroupBy, 
                              Apply
Cuestionario de tarea
¿Te gustaría comprobar tus 
conocimientos de la clase anterior?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
Repaso de Data 
Acquisition
Para pensar
¿Cómo accedemos a bases de datos usando 
Pandas?
¿De qué depende el proceso de Data 
Acquisition en una empresa?
          Repaso Data 
           Acquisition
En clases pasadas hemos visto cómo 
acceder a diferentes fuentes de 
datos usando pandas
A partir de ejemplos logramos 
concluir que en algunas ocasiones el 
proceso de Data Acquisition no es 
simple, depende del grado de 
madurez y manejo de datos en las 
compañías
En esta clase nos enfocaremos en la 
preparación de datos o Data 
Wrangling. Empecemos ������
Data Wrangling en 
un proyecto de DS
Definición
Definición de Data Wrangling
La manipulación de datos es el proceso de limpieza 
y  unificación  de  conjuntos  de  datos  complejos  y 
desordenados  para  facilitar  el  acceso,    análisis  y 
modelado.  Este  proceso  generalmente  incluye 
convertir y mapear los datos crudos (raw data) y 
dejarlos  en  un  formato  más  adecuado  para  su 
uso. 
Ejemplos de Tareas Data 
Wrangling
✔ Fusión  de  múltiples  fuentes  de  datos  en  un  único 
conjunto de datos para el análisis
✔ Identificar lagunas en los datos (por ejemplo, celdas 
vacías  en  una  hoja  de  cálculo)  y  llenarlas  o 
eliminarlas
✔ Eliminar  datos  que  son  innecesarios  o  irrelevantes 
para el proyecto en el que están trabajando
✔ Identificar valores atípicos extremos en los datos y 
explicar las discrepancias o eliminarlas para que se 
pueda realizar el análisis
Proceso Data Wrangling
El Data Wrangling puede ser un proceso manual o 
automatizado.  En las organizaciones que emplean un 
equipo de datos completo, un científico de datos u otro 
miembro del equipo suele ser responsable de la disputa 
de datos. 
En organizaciones más pequeñas, los profesionales que 
no son de datos a menudo son responsables de limpiar 
sus datos antes de aprovecharlos.
Importancia del Data 
Wrangling
Importancia Data Wrangling
El Data Wrangling puede llevar mucho tiempo y 
agotar los recursos, especialmente cuando se 
realiza manualmente. Esta es la razón por la que 
muchas organizaciones instituyen políticas y 
mejores prácticas que ayudan a los empleados a 
optimizar el proceso de limpieza de datos.
Por esta razón, es vital comprender los pasos del 
proceso de Data Wrangling y los resultados 
negativos asociados con datos incorrectos o 
defectuosos. 
Importancia Data Wrangling
Cualquier análisis que realice una empresa estará 
limitado en última instancia por los datos que los 
informan. Si los datos son incompletos, poco 
confiables o defectuosos, los análisis también lo 
serán, disminuyendo el valor.
El Data Wrangling busca eliminar ese riesgo al 
garantizar que los datos estén en un estado 
confiable antes de que se analicen y aprovechen. 
Esto lo convierte en una parte crítica del proceso 
analítico.
Rol de Data 
Wrangling en Fases 
Proyecto DS
Fases iniciales de un 
proyecto DS
   1                2               3                 4                5
Definición de     Contexto         Problema            Data       Exploratory Data 
objetivo       Comercial       Comercial         Acquisition    Analysis (EDA)
Fases finales de un 
proyecto DS
  6               7               8                 9               10
Data        Selección del  Desarrollo del     Validación y    Conclusiones
Wrangling        algoritmo       algoritmo        despliegue
(Munging)        apropiado
                     ~60% del tiempo de un Data Scientist 
                     consiste en limpiar y manipular datos
Etapas del Data 
Wrangling
Etapas del Data Wrangling
Descubrimiento   Limpieza       Validación
1       2       3       4       5      6
  Estructuración Enriquecimiento   Publicación
        Exploremos un poco cada una de las etapas
Descubrimiento
Antes  de  empezar  cualquier  análisis,  es 
importante  comprender  los  datos,  la 
estructura, tipos y cantidad. También lo es 
conocer  por  qué  una  compañía  los  utiliza  y 
cómo.  Ésto  sirve  para  tomar  decisiones 
posteriores con un rumbo claro. 
Estructuración
La  idea  de  esta  etapa  es  estandarizar  el 
formato  de  los  datos.  Dependiendo  de  si 
hay  diversas  fuentes  u  orígenes,  los  datos 
estarán en diferentes formatos y estructuras. 
Limpieza
Debemos eliminar los datos que no brinden 
información  extra  como  los  duplicados, 
revisar datos faltantes, etc. Esta propiedad 
estandariza  el  formato  de  las  columnas 
(float, datatimes, etc).
Enriquecimiento
Esta  etapa  se  refiere  a  agregar  datos  extra 
(Fuentes  externas)  que  complementan  a  los 
que ya existen para agregar información extra 
al análisis.
En  algunos  casos  se  puede  crear 
variables resumen
Validación
Es   muy  importante  para  los  equipos, 
asegurarse que los datos son precisos  y que 
la información no se alteró durante el proceso. 
Esto   significa  asegurar  la  fiabilidad, 
credibilidad y calidad de los datos limpios 
debido  a  que  van  a  utilizarse  para  tomar 
decisiones. 
Publicación
Una  vez  que  los  datos  están  validados,  se 
pueden compartir para su uso, realizar análisis 
exploratorios,  entrenar  modelos  y  tomar 
decisiones.
Se entiende como un producto final que 
se entrega para ser usado
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Combinar y fusionar 
objetos con Pandas. 
Merge y Concat
Repaso de Pandas
Datos ausentes
✔ Estos valores podrían no ser adecuados para 
algunos algoritmos de Data Science. Por ello, 
deben ser manejados correctamente.
✔ Pandas nos provee de herramientas para trabajar 
con ellos.
✔ Podemos utilizar las funciones isnull(), fillna() y 
dropna() a la hora de lidiar con datos de este tipo
Datos ausentes
     Podemos marcarlos                   Numeros_nan.isnull()         ������
     Podemos 
     reemplazarlos                       Numeros_nan.fillna(0)         ������
     Podemos eliminarlos                 Numeros_nan.dropna()           ������
Datos ausentes
                         La librería missingno es una gran 
                         alternativa a la hora de explorar 
                         datos faltantes en un dataframe
                         Blanco: Valores del faltante
                         Negro: Valores con dato
Funciones de agregación
Agregaciones: sum, max, min, std, 
var etc..
describe() e info() para describir el 
dataset de forma general
Funciones con str y Datetimes para 
mejorar formato de fecha y strings
Merge
Merge
pandas.merge():  Conecta filas de dos o más DataFrames basado en una o mas keys. Es 
similar al join en una query de sql.
Merge: 
Fusión de DataFrames
En el ejemplo, vemos 
dos tablas con una   Result = pd.merge(left, right, on="key")
misma key (llave 
primaria). 
Podemos unirlas por 
medio de: 
Merge: 
Fusión de DataFrames
            Caso no tan simple: Más de una llave primaria
           result = pd.merge(left, right, on=["key1", "key2"])
Concatenate
Concatenate
pandas.concat(): Concatena o apila dos o más dataframes a lo largo de un 
eje Ejemplo: 
  Este caso es ordenado y simple 
  porque las tablas tienen la            ������
  mismas columnas
pd.concat(frame, axis=0) # 0 indica por filas, 1 por 
columna
Concatenate
 Ejemplo: 
 Axis = 0 es el  valor por default y significa que va a apilar las flas de los 
 Dataframes.
 La única columna que tienen en común es la D
Concatenate
Ejemplo: 
Axis = 1 significa que va a apilar las columnas de los DataFrames
Las filas que tiene que tienen índices en comun tienen valores en todas las columnas.
Ejemplo en vivo
Aprenderemos a utilizar la función 
concatenate y merge de Pandas con el fin 
de entender cómo concatenar Pandas 
Dataframes, utilizaremos los archivos 
llamados clase0.xslx y clase1.xslx dentro 
de la carpeta de clase.
Merge y Concat de 
    DataFrames
Revisaremos cómo construir consultas equivalentes a 
las que hemos realizado en las clases de SQL 
      utilizando Pandas
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Merge y Concat de 
Dataframes
Utilizaremos los archivos planos llamados 
employees.csv, orders.csv y customers.csv en la 
carpeta de clase
1. Obtener un dataframe con el OrderID, 
LastName y FirstName con base en las 
tablas orders y employees. (Sugerencia: 
utilizar la función merge)
2. Obtener un dataframe con el 
CustomerName y el OrderID utilizando las 
tablas Customers y Orders
CLASE N°28
Glosario
Data Wrangling: proceso de convertir y     Merge: función disponible en pandas que 
mapear los datos crudos (raw data) y       nos permite combinar Dataframes con base 
dejarlos en un formato más adecuado        a columnas que comparten de diversas 
para su uso. Usualmente toma bastante      maneras: full join, inner join, left join y right 
tiempo (60% en promedio del tiempo de      join
un DS).
                                          Concatenate: función disponible en 
Etapas de Data Wrangling: son 6 fases      pandas que nos permite apilar DataFrames 
que describen el proceso de                por columnas o filas con el fin de generar 
estructuración de un dataset:              una sola estructura general. La opción 
descubrimiento, estructuración, limpieza,  axis=0 permite concatenar por filas y 
enriquecimiento, validación y publicación. axis=1 por columnas. 
¿Preguntas?
¿Aún quieres conocer 
  más?
Te recomendamos el 
siguiente material
 MATERIAL AMPLIADO
Recursos multimedia
  Porque Data Cleaning es necesario?
   ✓ Why is “Data cleaning” necessary | Medium | 
     Why DATA CLEANING
  Missingno library
   ✓ Missing data visualization module for Python | Github | 
     Missingno
Disponible en nuestro repositorio.
Opina y valora 
esta clase
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Data Wrangling en un proyecto de DS
      ✓ Etapas de Data Wrangling
      ✓ Combinar y fusionar objetos con Pandas. Merge y 
        Concat
Esta clase va a ser
grabad
  a
      Clase 23. DATA SCIENCE
 Data Acquisition I
Temario
               22                      23                     24
          Retomando                  Data               Fundamentos 
            impulso              Acquisition I           de bases de 
                                                            datos
         ✓ Fases de un          ✓ Adquisición de       ✓ Bases de datos
            proyecto de DS          datos              ✓
                                                          Sistemas de gestión 
         ✓ Numpy y Pandas       ✓ Lectura de datos        de bases de datos
         ✓ Visualización            con Pandas         ✓ Tipos de sistemas de 
                                                          gestión
         ✓ Estadística          ✓ Hojas de cálculo
            Descriptiva                                ✓ Backup, Conexiones, 
                                ✓ Pyspark                 auditoría
Objetivos de la clase
         Identificar el ciclo de vida de un proyecto de 
         ciencia de datos
         Rastrear el origen de los datos y su uso
         Leer datos con Pandas
MAPA DE CONCEPTOS
                      Data Acquisition I
Comprensión     Lectura de 
del problema    fuentes de       Hojas de        PySpark
de negocio       datos con       cálculo
                 Pandas
Big Data Value   Lectura de      Manipulación    Big Data y 
  Chain        archivos planos  de fechas y      PySpark
                                booleanos
  Data          Lectura de     Otros formatos   PySpark vs 
Management      datos en APIs  estructurados      Spark
Maturity Model
Introducción a la 
Adquisición de datos
Comprensión del 
problema de negocio
Entendimiento del caso de 
negocio
Antes de buscar datos es de vital 
importancia tener identificado el problema 
de negocio y una noción elemental de cómo 
poder resolverlo
El problema de negocio está asociado con la 
pregunta problema que se quiere resolver, 
por ejemplo: ¿Existe diferencias significativas 
entre los salarios de hombres y mujeres en 
los últimos años para una compañía de IT 
especifica?
Importante: No todos los problemas en el mundo de 
Ciencia de Datos necesitan de un modelo
       ¿Cómo podemos 
            empezar?
Punto de partida
✓ ¿Qué problema debemos solucionar?
✓ ¿Qué tipo de datos se requieren para 
hacer el análisis?
✓ ¿Dónde podemos encontrar dichos 
datos?
✓ ¿Cómo puedo acceder a los datos?
✓ ¿Los datos que deseamos realmente 
existen?
Repositorios gratuitos
Casos de éxito: 
Amazon y 
Mercadolibre
Amazon y 
Mercadolibre
✔ Estas dos empresas utilizan modelos 
predictivos con el fin de detectar 
futuras necesidades del cliente para 
poder diseñar una mejor estrategia de 
venta.
Amazon y 
Mercadolibre
✔ Para los casos particulares de estas dos 
empresas generan enriquecimiento de 
sus bases de datos por medio de redes 
sociales, logs de navegación, 
análisis de textos, sensores, 
información exógena para tener una 
perspectiva más completa a la hora de 
analizar y predecir
✔ Todo esto se puede corroborar cuando al 
usar los servicios de estas dos empresas 
podemos ver cómo se generan 
sugerencias de acuerdo al perfil del 
consumidor 
Para pensar
Basándonos en los casos mencionados y 
otros de la industria, ¿cómo piensas que se 
puede aplicar la estructura de Data 
Acquisition en la empresa donde laboras? 
¿Qué desafíos se pueden presentar?
Contesta en el chat de zoom
Big Data Value Chain
Big Data Value Chain
                              Definición: Serie de pasos que 
                              permiten describir el flujo de 
                              información dentro de un sistema de 
                              Big Data para generar valor e 
                              información útil a partir de los datos. 
                              La cadena de valor permite el análisis 
                              de tecnologías de big data para cada 
                              paso dentro de la cadena de 
                              producción de una empresa.
                              Consta de 5 etapas (Data Acquisition, 
                              Data Analysis, Data Curation, Data 
                              Storage y Data Usage) que 
                              describiremos a continuación:
Fuente: The Big Data Value Chain: Definitions, Concepts, and Theoretical Approaches
Big Data Value Chain
                             ✔ Data acquisition: es el proceso de 
                               recopilación, filtrado y limpieza de 
                               datos antes de colocarlos en un 
                               almacén de datos o cualquier otra 
                               solución de almacenamiento en la 
                               que se pueda realizar el análisis de 
                               datos.
                             ✔ Data analysis: hacer que los datos 
                               sin procesar adquiridos sean aptos 
                               para su uso en la toma de decisiones
                             ✔ Data Curation: gestión activa de los 
                               datos durante su ciclo de vida para 
                               garantizar que cumplan con los 
                               requisitos de calidad de datos 
                               necesarios para su uso efectivo
Fuente: The Big Data Value Chain: Definitions, Concepts, and Theoretical Approaches
Big Data Value Chain
                             ✔ Data Storage: es la persistencia y 
                               gestión de datos de forma escalable 
                               que satisface las necesidades de las 
                               aplicaciones que requieren un acceso 
                               rápido a los datos
                             ✔ Data Usage: actividades 
                               comerciales basadas en datos que 
                               necesitan acceso a los datos, su 
                               análisis y las herramientas necesarias 
                               para integrar el análisis de datos 
                               dentro de la actividad comercial.
Fuente: The Big Data Value Chain: Definitions, Concepts, and Theoretical Approaches
Adquisición de datos
¿Qué es la Adquisición de 
datos?
✔ Se refiere a la recuperación de 
datos, lo cual implica decidir acerca 
de qué datos se requieren, por que 
y como para luego poder utilizarlos
✔ No existe una única forma de 
adquirir los datos, ya que se tienen 
muchas fuentes disponibles
✔ Los datos se pueden adquirir por 
medio de la organización misma 
(First Party), ser buscados de forma 
externa (Second Party) o 
comprados (Third Party)
¿Qué hacen los 
Data Scientist?
                             Entrenan algoritmos
                  Otras cosas                Refinan algoritmos
                                               Minería de datos
            Limpian y organizan 
            datos
                                                Recolectan Datasets
Data Management 
Maturity Model
Data Management 
Maturity Model
Este modelo se fundamenta en 5 
componentes: 
✔ Estrategia de manejo de 
datos
✔ Calidad de datos
✔ Gobernanza de datos 
✔ Arquitecturas 
✔ Plataformas usadas junto 
con las operaciones 
asociadas a data
Data Management Maturity 
Model
       Nivel 1             Nivel 2             Nivel 3             Nivel 4             Nivel 5
   ✔Poca o ninguna    ✔Gobierno           ✔Data vista como    ✔Gobierno           ✔Procesos de Alta 
     gobernanza         emergente           habilitador         centralizado y      Predicción
   ✔Roles definidos   ✔Introducción         organizacional.     planificado       ✔Riesgo Reducido
     dentro de los      consistente de    ✔Procesos y         ✔Gestión de         ✔Métricas bien 
     silos              herramientas        herramientas        Riesgos asociado    establecidas y 
   ✔Problemas de      ✔Algunos roles y      escalables          a datos             desplegadas 
     calidad de         procesos          ✔Metas              ✔Métricas de          para medir la 
     datos no           definidos           establecidas        Performance de      calidad de los 
     abordados        ✔Creciente            considerando la     Iniciativas de      datos
                        conciencia del      calidad de los      Datos
                        impacto de los      datos             ✔Métricas de 
                        problemas de      ✔Automatización       mejora de 
                        calidad de datos    de procesos         Calidad de Datos
Las empresas se pueden clasificar en 5 niveles según su nivel de madurez 
Tipos de datos
Tipos de datos
✔ Estructurados: archivos que 
muestran configuración de filas y 
columnas con variables definidas. 
Pueden ser ordenados y procesados 
fácilmente con herramientas de 
minería de datos
✔ Semi-estructurados: Tienen 
características consistentes y definidas, 
no tienen una estructura rígida como 
en el esquema relacional
✔ No estructurados: no tienen 
estructura relacional y pueden venir 
con información dispersa y compleja de 
procesar. 
Fuente: Lawtomated
Para pensar
¿Conocen algunos ejemplos de datos 
estructurados, semi-estructurados y no 
estructurados? ¿Qué tan difícil creen que 
es procesar cada uno?
Contesta en el chat de zoom
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Lectura de fuentes 
de datos con Pandas
Timeline de Pandas
✔ 2008: Inicio de desarrollo de Pandas 
✔ 2009: Pandas se convirtió de libre acceso
✔ 2012: Primera edición de Python
✔ 2015: Se convierte en objeto del proyecto NUMFOCUS para una mejor 
 ciencia
Lectura de datos
con Pandas
Recordemos algunas de las características 
de pandas:
✔ Nos permite lidiar con archivos con 
codificaciones raras (parámetro 
encoding)
✔ Nos permite manipular encabezados y 
columnas de archivos 
✔ Permite manipulación y estructuración 
de datos en formato fecha
✔ Definir tipos de datos a priori en la 
lectura
✔ Identificar instancias inválidas
✔ Concatenar y manipular archivos
Lectura de archivos 
planos
Métodos de lectura de 
datos
       Método 1 (Github)
       url = 'copied_raw_GH_link'
       df1 = pd.read_csv(url)
       Método 2 (Local - csv)
       from google.colab import files
       uploaded = files.upload()
       import io
       df2 = pd.read_csv(io.BytesIO(uploaded['Dataset banco 
       ejemplo.csv']),sep=",")
       df2.head(10)
Métodos de lectura de 
   Método 3 (Local - Excel)
datos
   archivo = 'archivo_d.xlsx'
   df1 = pd.read_excel(url)
   Método 4 (Local - txt)
   ruta = 'C:/Downloads/David/shopping_list.txt'
   resultados= []
   with open(ruta) as f:
      linea= f.readline()
      with linea: 
        resultados.append(linea.strip().split(“ ”))
        linea= f.readline()
   f.close()
   data= pd.DataFrame(resultados,columns=['Col1',’Col2’])
Métodos de lectura de 
datos
Método 5 (Bases de datos SQLLite)
import pandas as pd
import sqlite3 as
sqlconn = sql.connect('/Users/David/Downloads/chinook.db')
df1 = pd.read_sql_query("SELECT * FROM invoice", conn)
cur = conn.cursor()
results = cur.execute("SELECT * FROM invoice LIMIT 
5").fetchall()
df2 = pd.DataFrame(results)
Métodos de lectura de 
datos
   Método 6 (Local - csv)
   host = "localhost";
   database= "suppliers"
   user = "postgres";
   password = "SecurePas$1"
   import psycopg2;
   import config
   conn = 
   psycopg2.connect( host=config.host,database=config.database,
   user=config.user,password=config.password)
   df1 = pd.read_sql_query("SELECT * FROM invoice", conn)
Métodos de lectura de 
datos
Método 7 (APIs - sin credenciales)
import requests
response = requests.get("http://api.open-notify.org/astros.json")
print(response.status_code)
print(response.json())
res = pd.DataFrame(response.json()["people"])res.head()
Métodos de lectura de 
Códigos posibles
datos
✔ 200: todo salió bien y se ha devuelto el resultado 
(si lo hay).
✔ 301: el servidor lo está redirigiendo a un punto 
final diferente. Esto puede suceder cuando una 
empresa cambia los nombres de dominio o se 
cambia el nombre de un punto final.
✔ 400: El servidor cree que hiciste una mala 
solicitud. Esto puede suceder cuando no envía los 
datos correctos, entre otras cosas.
✔ 403: El recurso al que intenta acceder está 
prohibido: no tiene los permisos adecuados para 
verlo.
✔ 404: El recurso al que intentó acceder no se 
encontró en el servidor.
✔ 503: El servidor no está listo para manejar la 
solicitud
Métodos de lectura de 
Método 8 (APIs - con credenciales)
datos
# Se necesita un archivo tipo config.py
personal_api_key = "intentasaberlo"
import config
import pandas as pd; import requests
parameters = {
  "personal_api_key": config.personal_api_key,
  "date": "2021-09-22"
}
response = requests.get(url, params = parameters)
print(response.status_code)
print(response.json())
res = pd.DataFrame(response.json()["people"])
res.head()
Métodos de lectura de 
Método 9 (Librerías- Pandas_datareader)
datos
from pandas_datareader import data
import datetime as dtzm = data.DataReader(
  "ZM",start='2019-1-1',end=dt.datetime.today(),
  data_source='yahoo').reset_index()zm.head()
Método 10 (Librerías- Pytrends Google 
trends)
import pandas as pdfrom pytrends.request
import TrendReqpytrends = TrendReq()
keywords = ["oat milk", "soy milk", "almond milk"]
pytrends.build_payload(keywords, cat=0, geo='', gprop='') # 
data de ultimos 5 años
top_queries = pytrends.interest_over_time()[keywords]
top_queries.head()
Headers, Booleanos 
y Fechas
Lectura de archivos planos 
(Headers)
Lectura de archivos planos (Fechas)
Parseo automático de fechas
Lectura de archivos planos (Fechas)
Parsing manual de fechas
Lectura de archivos planos (Booleanos)
  Read_csv puede detectar automáticamente booleanos si se le indica.
“asistió” se refiere a la asistencia de un alumno y “Tarea” si completo la tarea o no.
Ejemplo en vivo
Importemos los siguientes datos hosteados 
en el siguiente link Datos robos. 
Realizaremos una exploración básica de los 
datos y obtendremos conclusiones 
relevantes para el caso
Ejemplo en vivo
De los resultados obtenidos podemos concluir:
✔ Los robos de autos han ido aumentando en lo corrido de 
Enero de 2022
✔ Volskwagen, Renault y Chevrolet son las marcas más 
robadas
✔ Los hombres son más susceptibles de sufrir robos 
✔ La mayoría de los robos reportados se dan en Buenos 
Aires.
Hojas de cálculo
Hojas de cálculo
✔ Son datos almacenados de forma tabular en 
filas y columnas
✔ Cada fila se considera una instancia y cada 
columna una variable
✔ A diferencia de los archivos en formato 
plano, pueden tener fórmulas y formato
✔ Un solo archivo puede tener varias hojas de 
cálculo
✔ Tienen algunas limitaciones de cantidad de 
almacenamiento y velocidad de 
procesamiento
Hojas de cálculo
                         La lectura de hojas de cálculo permite muchas opciones 
                         una de las más importantes es la lectura de hojas 
                         específicas, eliminación de columnas indeseables e 
                         índices innecesarios
Hojas de cálculo
    Lectura de hojas y columnas
Hojas de cálculo
Podemos, en lugar de usar el método pd.read_excel, crear un objeto y 
         hacer un parsing las hojas del excel.
                  Atributo que enumera los 
                  nombres de las hojas. 
Otros formatos 
estructurados
Otros formatos 
estructurados (Pickle)
          Es un formato nativo de python que es popular para la serialización de objetos.
      ✔ Es mucho más rápido que .csv.                        ✔ Al ser nativo de python 
      ✔ Reduce el tamaño de los archivos en la mitad             solo puede leerse 
         usando técnicas de compresión.                          utilizando python.
      ✔ No hay necesidad de especificar columnas de 
         datos ni argumentos.
PySpark
PySpark para BigData
✔ Spark es framework y un conjunto de librerías para el procesamiento de datos 
en paralelo. Fue creado en 2014 para abordar muchas de las deficiencias de 
Apache Hadoop y es mucho más rápido que Hadoop para procesamiento analítico 
porque almacena datos en la memoria (RAM) en lugar de en el disco.
✔ PySpark es una biblioteca de Python que permite usar Spark
✔ Spark permite la construcción de una arquitectura que permite la gestión de datos 
en streaming, consultas, aprendizaje automático y acceso a tiempo real 
PySpark para BigData
 ✔ Trabaja estrechamente con el lenguaje           ✔ Pandas es una librería muy 
     SQL, es decir, datos estructurados.              potente que permite hacer 
     Permite consultar los datos en tiempo            infinidad de procesos. El 
     real.                                            problema viene cuando 
 ✔ Spark es la herramienta adecuada                   queremos adquirir volúmenes 
     gracias a su velocidad y sus API                 demasiado grandes para una 
     enriquecidas para trabajar en entornos           computadora normal.
     cloud.
Lectura de datos con 
       Pandas
Examinaremos cómo leer datos en formato sql, JSON y 
      cómo utilizar APIs
     Duración: 15-20 mins
      ACTIVIDAD EN CLASE
 Lectura de 
 datos con 
 Aprenderemos a manipular SQL, JSON y APIs. 
 Pandas
 Exploramos las librerías sqlite3, yfinance y la función read_json 
 con el fin de comprender cómo leer y procesar archivos en 
 diferentes formatos. 
   1.   Leer las tablas NBA_season1718_salary y Season_Stats 
        dentro del archivo nba_salary.sqlite
   2.   Leer el archivo JSON en la siguiente página Web: JSON file
   3.   Utilizar la funcion Ticker de yfinance para extraer 
        información relevante de la compañias PFE (Pfizer)
 Trabajaremos de forma individual. Se estiman 5 minutos 
 para cada ejercicio de lectura y 5 min para compartir las 
 conclusiones.
CLASE N°23
Glosario
Pregunta problema: pregunta que                 Data Management Maturity Model: 
resume el objetivo que se quiere resolver.      proporciona pautas para ayudar a las 
                                               organizaciones a construir, mejorar y medir 
Big Data Value Chain: proceso para              su capacidad de gestión de datos 
describir el flujo de información dentro de     empresariales.
un sistema de big data para generar valor       Tipos de datos: pueden ser 
e información útil a partir de los datos.       estructurados, semi-estructurados y no 
Adquisición de datos: proceso de carga          estructurados.
de datos para la resolución de un               APIs: un conjunto de funciones y 
problema de interés.                            procedimientos para la creación de 
                                               aplicaciones que acceden a las 
                                               características o datos de un sistema 
                                               operativo u aplicación.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Comprensión del problema de negocio
      ✓ Lectura de fuentes de datos con Pandas
      ✓ Hojas de cálculo
      ✓ PySpark
      ✓ Manipulación de datos API, JSON y SQL
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 32. DATA SCIENCE
  Introducción a la 
   visualización de 
         datos
 Temario
                          31                                 32                                 33
              Exploratory Data                    Introducción a la                    Visualizaciones 
                Analysis (EDA)                     visualización de                    efectivas y Data 
                                                           datos                          Storytelling
                 ✓ Análisis                       ✓ Historia de las                  ✓ Beneficios del 
                     estadístico                                                         storytelling y 
                                                      visualizaciones                    componentes
                 ✓ Correlaciones y                ✓ Principios generales del         ✓ Visualización de datos
                     variables                        diseño analítico
                                                                                     ✓ Elevator Pitch
                 ✓ Identificación de              ✓ Buenas prácticas
                     outliers                     ✓ Gestalt                          ✓ Interfaz de usuario
                 ✓ Valores perdidos               ✓ Visualizaciones                  ✓ Principios de 
                                                      engañosas                          usabilidad
Objetivos de la clase
         Reconocer la utilidad y beneficios de la 
         visualización de datos
         Comparar buenas prácticas de aplicación
MAPA DE CONCEPTOS
                          Historia de las 
                         visualizaciones
  Principios         Introducción a la 
 generales del        visualización de           Visualizaciones 
diseño analitico                                  engañosas
                           datos
                            Gestalt
Cuestionario de tarea
¿Te gustaría comprobar tus 
conocimientos de la clase anterior?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
Historia de las 
visualizaciones
        Repaso…
En módulos anteriores de nuestro curso, 
hemos realizado ya algunas 
visualizaciones de datos, sin embargo no 
hemos entrado en profundidad sobre 
cuáles son los verdaderos fundamentos de 
una correcta visualización
A lo largo de esta sesión, abordaremos 
estos conceptos de manera detallada…
Visualización de datos
Se refiere a la presentación de datos en 
un formato ilustrado o gráfico. 
Permite observar el análisis de datos, 
de manera visual, para captar 
conceptos difíciles o identificar 
patrones.
¿Para qué sirve?
Gracias a la visualización de datos, las 
personas encargadas de tomar 
decisiones por ejemplo dentro de una 
empresa, ven significativamente facilitada 
su labor, al contar con medios visuales que 
simplifican la comprensión del conjunto de 
datos. 
Para pensar
Ahora bien… respecto al tema de crear 
visualizaciones ¿Es una práctica actual o se 
remonta desde qué época? ¿Desde cuándo 
piensan que las visualizaciones han sido 
importantes para el ser humano?
Contesta en el chat de Zoom
Para pensar
Si respondieron que sí, ¡acertaron!
Efectivamente las visualizaciones de datos, datan 
de hace un largo tiempo atrás.
Revisemos un poco la historia…
Línea de tiempo
Línea de tiempo 
                             12000 BP- Datos de estrellas y el 
                             universo, cavernas
                             5500 BC- Representación de 
                             tokens como cantidades
                             1160 BC - Mapas e ilustraciones de 
                             recursos geológicos 
                             1786- Sr William Playfair concluyó 
                             que los gráficos representan mucho 
                             mejor las ideas que las tablas. El 
                             fue el inventor de los gráficos de 
                             líneas, barplots, piecharts
Fuente: TimeLine Visualization
Línea de tiempo 
                                    Finales siglo 19- Edad de Oro 
                                    Estadística
                                    1980’s- Sparklines información 
                                    temporal
                                    1990’s - Excel con sus herramientas de 
                                    visualización facilitó el manejo de datos
                                    Hoy en día- Visualización enfocada en 
                                    casos de uso
                                     1. BI (business Intelligence) 
                                        reporting   
                                     2. Custom Data Visualization
                                     3. Information Visualization
                                     4. EDA
Fuente: TimeLine Visualization
Línea de tiempo 
                     Hoy en día hemos llegado a este 
                     estado, donde tenemos un sin número 
                     de visualizaciones posibles
                     ¿Cuántos tipos de gráficos creen 
                     que existen en la actualidad?
Visualizaciones 
icónicas
Visualizaciones icónicas
 Sabemos la importancia de la visualización, pero… 
 ¿pensaron        alguna      vez      cómo       las 
 visualizaciones influyeron en la historia? 
 Los   invitamos  a  conocer  algunas  de  las 
 visualizaciones más icónicas y su historia.
 ¡Entremos
 !
Para pensar
Basándonos lo aprendido y teniendo en cuenta la 
historia de las visualizaciones…
¿Qué crees que hace a una buena visualización?
Contesta en el chat de Zoom
Principios generales 
del diseño analítico
Edward Tufte
Si hablamos de referentes del mundo de las 
Visualizaciones de Datos, además de Alberto 
Cairo, no podemos no mencionar a: Edward 
Tufte . Su libro más destacado ha sido: “The 
Visual     Display     of     Quantitative 
Información”,  también  conocido  por  su 
traducción    al    español    como:    “La 
Representación    Visual   de   Información 
Cuantitativa”. 
Principios generales de 
Tufte
Uno de los grandes aportes de Tufte fueron 
los Principios Generales del Diseño Analítico, 
los cuales hasta el día de hoy tienen vigencia:
✔Principio 1: Muestra comparaciones, 
contrastes, diferencias.
✔Principio 2: Muestra causalidad, 
mecanismo, explicación, estructura 
sistemática.
✔Principio 3: Muestra datos multivariados, 
es decir, más de una o dos variables.
Principios generales de 
Tufte
✔ Principio 4: Íntegra palabras, 
números, imágenes y diagramas.
✔ Principio 5: Describe la totalidad 
de la evidencia. Muestra fuentes 
usadas y problemas relevantes.
✔ Principio 6: Las presentaciones 
analíticas, se sostienen de la 
calidad, relevancia e integridad de 
su contenido.
Buenas prácticas
Uso apropiado de gráficas
Una buena gráfica debe proveer:
✔ Contenido 
✔ Debe ser la apropiada según lo que se 
quiere presentar
✔ Menos es más (No es necesario incluir 
etiquetas de más, ni colores en exceso)
✔ Es útil para el usuario que la usa
Fuente: Choice of Charts
¿Qué hace una buena 
visualización?
Sin dudas, esta pregunta es muy ambiciosa y no existe un 
claro consenso en que hace a una visualización de datos 
"buena",  pero  vamos  a  basarnos  en  algunos  conceptos 
que Alberto Cairo referente en el tema nos brinda en su 
libro  “The  Truthful  Art”.  4  puntos  (Veracidad, 
funcionalidad, perspicaz , atractiva)
¿Qué hace una buena 
visualización?
Según Cairo (y Enrico Bertini), una visualización es exitosa 
porque es: 
✔ Veraz:  No  debe  esconder  información  y  ser  el 
resultado de un análisis/investigación "honesto“.
✔ Funcional: Debe permitir a los usuarios entenderlo de 
la manera más simple posible.
¿Qué hace una buena 
visualización?
✔ Atractiva: Una visualización debe buscar atraer 
la  atención mediante un buen diseño estético y 
basado  en  la  Usabilidad  de  Usuario  (este 
concepto  lo  abordaremos  en  detalle  en  la 
próxima clase ������).
✔ Perspicaz: Muestra evidencia que sería difícil de 
ver  de  otra  manera  esclarecedora:  nos  da 
evidencia nueva sobre el tema que se analiza.
Ejemplos:
Buenas prácticas de 
visualización 
La visualización de datos, como ya sabemos, tiene un 
papel relevante dentro del mundo empresarial. Esto es 
así debido a que las personas procesan mejor la 
información, en el uso de esquemas o gráficos para 
observar grandes cantidades de datos complejos. 
Buenas prácticas de 
visualización 
Con las visualizaciones de datos podemos, por ejemplo:
✔ Identificar áreas de la empresa que necesitan de una 
mayor atención.
✔ Ayudar a comprender los factores que influencian el 
comportamiento de los clientes.
✔ Predecir cifras claves de negocio. 
Buenas prácticas de 
visualización 
✔ Las  visualizaciones  ayudan  a  los  usuarios  a  darse 
cuenta de realidades que no eran obvias de forma 
inicial. 
✔ Los  patrones  subyacentes  de  los  datos  se  pueden 
observar de forma rápida y sencilla. 
✔ Transmiten  la  información  de  una  manera  visual  y 
clara.  Este  concepto  se  asocia  a  lo  que  se  conoce 
como Data StoryTelling,  temática  que  abordaremos 
en la próxima sesión.
¿Cómo elegir el método 
adecuado?
Además de considerar los tipos  de  datos  asociados  a  las  variables  que 
queremos graficar,  debemos  de  tener  en  cuenta  que  todos  los  gráficos 
poseen: “Una posición, una forma, un tamaño, un color, un ancho de 
línea y un tipo de línea”.
Tipos de datos
Un pequeño juego
Ascombe Dataset
Ascombe Dataset
Asociar conceptos
1. Edad de tus amigos                              A. quantitative
2. Género de tus amigos                            B. categorical
3. Número de amigos que tienes                     C. categorical/ordinal
4. Los nombres de tus amigos                       D. categorical
5. Todas tus notificaciones                        E. relational
6. Links entre un grupo de amigos                  F. Spatial/categorical
7. Las ciudades de todos tus amigos
¿Qué es más sencillo de 
leer?
Percepción de 
variables visuales
Percepción de variables 
visuales
  1. DETECCIÓN:     ¿Podemos    (espontáneamente)    discernir/separar 
     elementos gráficos?
     -> relacionado con la propiedad selectiva
  2. ESTIMACIÓN:  ¿Podemos  hacer  comparaciones  de  magnitudes  de 
     elementos de datos a partir de los elementos gráficos?
     -> relacionado con la propiedad cuantitativa
Detección: Procesamiento de 
pre-atención
      -    Algunos  estímulos  pueden  ser  percibidos  sin  necesidad  de 
           atención focalizada
      -    Generalmente dentro de 200-250 ms
      -    Parece ser hecho en paralelo por el sistema de visión de bajo 
           nivel
    ¡Esto  tiene  un  gran  impacto  en  la  codificación  visual  que 
    debemos usar!
Detección
Pre-atención
Pre-atención
          Cuando diseñes visualizaciones, intenta 
           usar la pre-atención con el fin de 
            resaltar lo que más te interesa.
Conjunción
Conjunción
                      El color hue es tiene pre-atención
Conjunción
Conjunción
                      El shape es tiene pre-atención
Conjunción
Conjunción
                    El color y shape es tiene pre-atención
Pre-atención
        Evite las conjunciones que 
         inhiban la pre-atención.
        (Usualmente asociada sa 
            búsqueda)
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Gestalt
Generalidades
Implementación de 
Gestalt en DS
La implementación de Gestalt aplica más al área de BI. 
Sin embargo, es un concepto muy interesante a tener en cuenta como 
científicos de datos para entender a la visualización en su conjunto y 
contexto.
Leyes de percepción 
visual
Leyes de percepción visual 
Gestalt
La percepción visual, es la interpretación de la información transmitida por los estímulos 
lumínicos externos que, a través de un proceso cerebral activo, se transforma en una 
representación o copia de la realidad, la cual claramente es subjetiva y se encuentra 
atada a las vivencias, a la cultura y a nuestra experiencia personal.
Leyes de percepción visual 
Gestalt
Ahora que ya sabemos esto, ¿Qué son y para qué sirven las leyes de Gestalt? De 
cierta manera, nos ayudan a entender cómo captamos el mundo que nos rodea y nuestra 
tendencia  a  percibir  las  relaciones  entre  los  distintos  elementos  como  un  todo  y 
justamente se basan en la premisa, de que el “Todo es superior a la suma de sus 
partes”. 
Leyes de percepción visual 
Gestalt
Leyes de percepción visual 
Gestalt
1. Proximidad: Si unos elementos se encuentran próximos 
entre sí, se perciben como un grupo. En la figura, percibimos 
los elementos circulares alineados verticalmente agrupados 
como si fueran columnas debido a la proximidad entre ellos, y 
no agrupados en filas 
Leyes de percepción visual 
Gestalt
  En el siguiente gráfico, las ventas de las regiones "Norte",        Si intentamos comparar las ventas de la región "Norte" 
  "Sur", "Este" y "Oeste" forman 4 grupos distintos. Es bastante      (gráficos de barras azules) en los 4 trimestres. Difícil no?. La 
  fácil comparar las ventas de cada región dentro de un               ley de proximidad debe resaltar lo que se desea enfatizar
  trimestre.
Leyes de percepción visual 
Gestalt
2. Semejanza: Los elementos con atributos similares 
(color, forma, tamaño u orientación) se perciben como un 
grupo.
Asociamos los triángulos de color naranja bajo un mismo 
grupo pese a estar distanciados entre sí y más próximos a 
los elementos circulares de color coral.
Leyes de percepción visual 
Gestalt
Darles el mismo color a las barras 
en este caso particular elimina la 
carga cognitiva adicional 
impuesta por el uso disruptivo de 
diferentes colores en el gráfico 
de la izquierda y nos facilita 
mucho comparar y encontrar 
puntos de interés.
Se recomienda utilizar 
características y atributos 
similares (color, tamaño, forma, 
etc.) para establecer relaciones 
entre objetos y reforzar 
agrupaciones.
Leyes de percepción visual 
Gestalt
3. Cierre: Los elementos que parecen tener 
un contorno o borde alrededor de ellos, 
forman parte de un grupo. Aunque estén 
cerca y tengan semejanza con otros 
elementos fuera del borde, el hecho de 
existir un borde alrededor de varios de ellos 
hace que los percibamos como un grupo, aún 
sin ser semejantes.
Leyes de percepción visual 
Gestalt
Debido al bajo contraste entre la figura y el fondo en el gráfico de la 
izquierda, existe una carga cognitiva adicional. Aumentar el contraste 
(como en el gráfico de la derecha) mejora la legibilidad. 
Por tanto es importante asegúrese de que haya suficiente contraste entre 
sus figuras y el fondo.
Leyes de percepción visual 
Gestalt
4. Compleción: Nuestra percepción tiende a completar 
las partes que faltan rellenandolas con los elementos 
que faltan, siempre que sea razonable. La figura no 
está completa, le faltan partes, y sin embargo vemos 
un círculo.
Leyes de percepción visual 
Gestalt
 En la figura de la izquierda vemos un lineplot en línea continua. A la derecha observamos un 
 lineplot pero con líneas discontinuas. No es necesario generar estructuras continuas ya que 
 el cerebro humano es capaz de crear las conexiones necesarias entre los puntos evitando 
 consumir espacio adicional y en muchos casos facilitando las comparaciones
Leyes de percepción visual 
Gestalt
5. Continuidad: Los elementos que están uno a 
continuación de otro, se perciben como elementos continuos 
aunque haya un espacio entre sí. 
Pese a la semejanza en forma, color y tamaño, percibimos 
una línea recta que se cruza con una línea curva debido a la 
continuidad de las mismas. 
Leyes de percepción visual 
Gestalt
En el siguiente gráfico de la izquierda, hay cambios bruscos de dirección.
En el gráfico de la derecha, se sigue un camino continuo; esto hace que todo el gráfico sea más legible debido a 
la dirección descendente continua. Se sugiere alinear los elementos linealmente para facilitar la comparación de 
diferentes elementos que se encuentran en una agrupación relacionada.
Leyes de percepción visual 
Gestalt
6. Conectividad: Los elementos conectados 
físicamente entre sí se perciben como un grupo de 
manera más fuerte que el hecho de tener una 
semejanza (color, forma, tamaño, etc) similar.
Leyes de percepción visual 
Gestalt
 En la gráfica superior izquierda debido a que no hay conexiones entre los elementos el 
 cerebro tiene a establecer asociaciones por el color
 Sin embargo en las otras gráficas se puede observar que al existir conexiones entre los 
 elementos es mucho más sencillo comprender patrones en los datos
Visualizaciones 
engañosas
Visualizaciones engañosas
No todo en el mundo de las visualizaciones de 
datos es color de rosas, tenemos que tener 
mucho cuidado con las visualizaciones 
engañosas es decir, gráficos creados a 
propósito con el objetivo de confundir y/o 
engañar a una audiencia.
Visualizaciones engañosas
✔  Los ejes suelen estar cortados.
✔  La escala no es homogénea durante todo el 
gráfico.
✔ Visualizaciones muy cargadas de 
información de manera innecesaria.
✔ Pocos datos o no se detalla la fuente.
Ejemplo en vivo
Exploremos en el notebook dentro de la 
carpeta de clase con el fin de examinar las 
características ideales para un gráfico. 
Examinaremos ventajas y desventajas 
de las librerías Matplotlib y Seaborn.
CLASE N°32
Glosario
Visualización de datos: presentación             Buenas visualizaciones: según Cairo y 
                                                Enrico Berteni debe cumplir con 4 
de datos en un formato ilustrado o               características (veraz, funcional, atractiva y 
gráfico. Permite observar el análisis            perspicaz)
de datos, de manera visual, para 
captar conceptos difíciles o                     Leyes de percepción visual de Gestalt: 
identificar patrones.                            es la interpretación de la información 
                                                transmitida por los estímulos lumínicos 
Principios generales de diseño                   externos que, a través de un proceso 
analitico: establecidos por Tufte                cerebral activo, se transforma en una 
(comparaciones, causalidad,                      representación o copia de la 
multivariados, integración de números,           realidad (Proximidad, Semejanza, 
palabras e imágenes, descripción de la           Cierre , Compleción, Continuidad y 
evidencia y calidad) permiten definir la         conectividad)
forma y estructura de gráficos.
¿Preguntas?
¿Aún quieres conocer 
  más?
Te recomendamos el 
siguiente material
 MATERIAL AMPLIADO
Recursos multimedia
 The mortality rates and the space-time pattern
  ✓ The Mortality rates and the space-time pattern of John Snow’s 
     Colera epidemic map | International Journal of Health 
     Geographics | John Snows map
 From Data to Viz
  ✓ From Data to Viz | Yan Holtz & Conor Healy | Datatoviz
 Catálogo de visualizaciones
  ✓ Tipos de visualizaciones | Dataviz | Código DataViz
Disponible en nuestro repositorio.
Opina y valora 
esta clase
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Análisis estadístico
      ✓ Correlaciones y variables
      ✓ Identificación de outliers
      ✓ Valores perdidos
Esta clase va a ser
grabad
  a
      Clase 49. DATA SCIENCE
Mejora de modelos de 
 Machine Learning II
Temario
               48                      49                     50
       Mejora de modelos       Mejora de modelos          Modelos de 
           de Machine         de Machine Learning     Ensamble y Boosting 
           Learning I                  II                   Models
        ✓ Bias vs. Variance     ✓ Repaso validación 
           Tradeoff                de modelos           ✓ Métodos de 
                                                           ensamble
        ✓ Validación de         ✓ Hypertuning           ✓ Metodologías de 
           modelos                 parameter               ensamble
Objetivos de la clase
         Aplicar en Python modelos de optimización e 
         Hypertuning de parámetros 
         Diferenciar los conceptos de parámetro e 
         hiperparametros
   MAPA DE CONCEPTOS                                   Bias vs Variance 
                                                          Tradeoff
                                                       Validación simple 
                                                         y LOOCV
                                                       K fold y Stratified 
                                                          K-Fold
           Mejora de                                  Hyperparamete
          modelos de                                     r Tuning       Clase 
        Machine Learning                                                49
                                                       GridSearchCV
                                                      RandomSearchC
                                                            V
    Repaso
             Les proponemos tomarse unos minutos 
             para realizar un repaso de los conceptos 
              aprendidos en Kahoot, ¿están listos?
                  Profe, puedes compartir el 
                   PIN o link de acceso al 
                        juego
Para pensar
¿Cómo podemos cuantificar el desempeño de un 
modelo de Machine Learning? ¿Para evaluar el 
desempeño era indiferente si teníamos un 
modelo de clasificación o regresión?
Contesta en el chat de Zoom 
Repaso validación de 
modelos
Repaso validación de modelos
En la clase pasada estuvimos hablando de 
diferentes estrategias para realizar la 
validación de modelos de Machine Learning, 
vimos diferentes metodologías como:
1. Validación simple
2. LOOCV
3. K-fold Cross Validation
4. Stratified K-Fold
Repasemos brevemente cada una
Validación simple
Validación simple
Antes de hablar de la Validación Cruzada, 
tenemos que entender el concepto de la 
Validación Simple. Este tipo de validación 
consiste en repartir aleatoriamente las 
observaciones disponibles en dos grupos, 
uno se emplea para entrenar al modelo y 
otro para evaluarlo. 
  LOOCV
Leave One Out Cross-Validation
Este método es de tipo iterativo y 
se inicia empleando como 
conjunto de entrenamiento todas 
las observaciones disponibles 
excepto una, que se excluye 
para emplearla como 
validación.  
K-Fold Cross 
Validation
K-Fold Cross 
Validation
El método K-Fold Cross-Validation es también un 
proceso iterativo. Consiste en dividir los 
datos de forma aleatoria en k grupos de 
aproximadamente el mismo tamaño, k-
1 grupos se emplean para entrenar el 
modelo y uno de los grupos se emplea 
como validación.
Stratified K-Fold
Stratified K-Fold
Es una variante mejorada de K-fold, que cuando hace los splits (las divisiones) 
del conjunto de train, tiene en cuenta mantener equilibradas las clases, esto 
significa que cada conjunto contiene aproximadamente el mismo 
porcentaje de muestras de cada clase objetivo que el conjunto 
completo. 
Resumen
Resumen 
  Método                    Ventajas                                Desventajas
 Validació   -   Permite una validación rápida        -   Estimación de error altamente 
 n Simple    -   No requiere de mucho costo               variable
                 computacional. Sencillo de           -   Tiene problemas de sesgo (bias) en 
                 implementar                              la estimación del error
 LOOCV       -   No hay aleatoriedad en el uso de     -   Tiene alta variabilidad 
                 algunas observaciones                -   Computacionalmente costoso 
             -   Menos sesgo por mayor tamaño             (tiempo y energía)
                 de entrenamiento
Resumen 
  Método                      Ventajas                                   Desventajas
 K-Fold       -  Conjunto de validación más grande  -        Sesgo más alto que LOOCV por 
 Cross           que LOOCV                                   conjunto de entrenamiento más 
 Validatio    -  Menos sesgo que esquema                     pequeño 
 n               validación simple                        -  El algoritmo de entrenamiento debe 
              -  No es computacionalmente                    volver a ejecutarse desde cero k 
                 costoso                                     veces
 Stratified   -  Valida el rendimiento de su modelo  -       No funciona bien con datos 
 K-Fold          en múltiples folds                          secuenciales (e.g series de tiempo)
              -  Puede equilibrar las clases              -  Si solo se confía en el puntaje final 
              -  Brinda una respuesta más estable            agregado se pierde mucha 
                                                             información
Para pensar
Al ajustar un modelo de regresión observan que a medida que 
aumenta la cantidad de datos de entrenamiento, el error de prueba 
disminuye y el error de entrenamiento aumenta. El error de train es 
bastante bajo, mientras que el error de prueba es mucho mayor. 
¿Cuál crees que es la razón principal detrás de este 
comportamiento? 
A. Alta varianza
B. Alto sesgo en el modelo
C. Alta estimación de sesgo
D. Ninguna de las anteriores
Contesta en el chat de Zoom 
Para pensar
Al ajustar un modelo de regresión observan que a medida que 
aumenta la cantidad de datos de entrenamiento, el error de prueba 
disminuye y el error de entrenamiento aumenta. El error de train es 
bastante bajo, mientras que el error de prueba es mucho mayor. 
¿Cuál crees que es la razón principal detrás de este 
comportamiento? 
A. Alta varianza
R// Las variables analizados presentan una alta varianza que 
hace que el comportamiento en test siempre presente una 
alta tasa de error a pesar de haber agregado más datos 
Contesta en el chat de Zoom 
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Hypertuning 
Parameter
Definición
Hypertuning Parameter
La  optimización  de  hiper  parámetros 
en el aprendizaje automático, tiene por 
objeto     encontrar      los     hiper 
parámetros  de  un  determinado 
algoritmo        de        aprendizaje 
automático que ofrezcan el mejor 
rendimiento       medido      en     un 
conjunto de validación. 
Parámetros vs 
Hiperparametros
Parámetros vs 
Hiperparametros
Parámetros: son todos aquellos que se ajustan con 
los datos a partir del proceso de entrenamiento y 
permiten que el algoritmo de ML se puede 
desarrollar
Hiperparametros: son los ajustes del modelo para 
que pueda resolver de manera óptima el problema 
de aprendizaje automático 
Ejemplos Parámetros e 
Hiperparametros
Parámetros: pesos en las redes neuronales, 
vectores de soporte en SVM, coeficientes de 
regresión Lineal y Logística
Hiperparametros: tasa de aprendizaje para el 
entrenamiento (controla el overfitting), valor elegido 
de K para el KNN, alpha, C, gamma, max_depth, 
random_state, penalty, cv, kernel, metric, test_size
              Parámetros                       Hiperparametros
    Estimados durante el proceso de    Estimados a priori y no deben ser 
       entrenamiento con datos             entrenados con los datos
               históricos 
     Son parte integral del modelo        Externos al modelo elegido
       Los valores estimados son          No hacen parte del modelo 
        guardados en el modelo          entrenado y sus valores no son 
               entrenado                          guardados 
     Dependientes de los datos que        Independientes de los datos 
     entrenan al algoritmo elegido     elegidos para entrenar el modelo
Hypertuning Parameter
La optimización de hiper parámetros 
tiene como objetivo entonces 
encontrar una combinación que 
devuelve un modelo óptimo, 
reduciendo una función de pérdida 
predefinida y a su vez aumentando 
la performance del mismo.
Importancia
Hypertuning Parameter
Los hiper parámetros pueden tener un 
impacto directo en el entrenamiento 
de los algoritmos de aprendizaje 
automático. Por lo tanto, para lograr el 
máximo rendimiento, es importante 
entender cómo optimizarlos.
A continuación explicaremos las técnicas 
más utilizadas para optimizar 
hiperparametros
Métodos Hypertuning Parameter
Los métodos más comunes son
1. Ajuste Manual
2. Grid Search CV
3. Randomized Search
4. Halving Grid Search
5. Halving Randomized Search
6. HyperOpt-Sklearn
7. Bayes Grid Search
Ajuste manual
Ajuste manual 
Tradicionalmente, los hiper parámetros 
se ajustaban manualmente por ensayo 
y error. Esto todavía se hace 
comúnmente, y los ingenieros 
experimentados pueden "adivinar" 
los valores de los parámetros que 
ofrecerán una muy alta precisión 
para los modelos de aprendizaje 
automático. 
PARA RECORDAR
Sin embargo, existen métodos más 
avanzados que realizan una búsqueda 
continua que pueden ser más rápidos y 
automáticos para optimizar los hiper 
parámetros, a continuación explicaremos 
algunos de ellos
GridSearchCV
GridSearchCV
Es  esencialmente  un  algoritmo  de 
optimización    que   nos   permite 
seleccionar  los  mejores  parámetros 
para un problema de optimización de 
una  lista  de  opciones  de  parámetros 
que  nosotros  le  proporcionaremos  al 
método,     con    esto    logramos 
automatizar  la  aplicación  de  tipo 
‘prueba y error’. 
GridSearchCV
Aunque  se  puede  aplicar  a  muchos 
problemas  de  optimización,  es  más 
conocido  por  su  uso  en  el  Machine 
Learning para obtener los parámetros 
en  los  que  el  modelo  ofrece  la 
mejor precisión.
GridSearchCV
Entonces podemos decir que el método de 
Grid Search, es un método de ajuste de 
parámetros que realiza una búsqueda 
exhaustiva entre todas las selecciones de 
parámetros candidatos, a través de 
bucles y probando todas las posibilidades 
existentes, hasta encontrar la mejor 
combinación de hiper parámetros existente. 
Randomized Search
Randomized Search 
Muchas  veces  algunos  de  los  hiper 
parámetros  importan  más  que  otros. 
Realizar  una  búsqueda  aleatoria  en 
lugar    de    una     búsqueda      por 
cuadrículas,         permite          un 
descubrimiento         mucho        más 
preciso de los buenos valores para 
los       hiperparámetros           más 
importantes.
Randomized Search 
Establece  una  cuadrícula  de  valores 
hiper    paramétricos      y    selecciona 
combinaciones         aleatorias       para 
entrenar  y  puntuar  el  modelo.  El 
número de iteraciones  de  búsqueda  se 
establece  en  función  del  tiempo  o  los 
recursos,  Scikit  Learn  ofrece  la  función 
RandomizedSearchCV            para      este 
proceso.
Halving Grid Search 
Halving Grid Search
Halving Grid Search es una versión 
optimizada  de  la  optimización  de 
hiperparámetros  Grid  SearchCV.  En 
este caso busca en una lista específica 
de  hiperparámetros  utilizando  un 
enfoque  de  reducción  a  la  mitad 
sucesiva         de        posibles 
combinaciones.
Halving Grid Search
La  estrategia  de  búsqueda  comienza 
evaluando todos los candidatos en una 
pequeña  muestra  de  los  datos  y 
selecciona iterativamente los mejores 
candidatos  utilizando  muestras  cada 
vez más grandes.
Halving Randomized 
Search 
Halving Randomized Search
Halving Randomized Search utiliza 
el  mismo  enfoque  de  reducción  a  la 
mitad  sucesiva  que  el  Halving  Grid 
Search y puede llegar a tener mejores 
resultados.
No    se   entrena   en   todas   las 
combinaciones  de  hiper  parámetros, 
sino  que  elige  aleatoriamente  un 
conjunto de combinaciones 
Hyperopt Sklearn
Hyperopt-Sklearn
Hyperopt  es  una  librería  de  Python 
con    código    abierto    para    la 
optimización bayesiana, diseñada para 
optimizaciones  a  gran  escala  de 
modelos  con  cientos  de  parámetros. 
Permite  escalar  la  optimización  de 
hiperparámetros en varios núcleos de 
la CPU
Hyperopt-Sklearn
Hyperopt-Sklearn  es  una  extensión 
de la biblioteca Hyperopt, que permite 
la búsqueda automática de algoritmos 
de aprendizaje automático y modelar 
hiperparámetros   para   tareas   de 
clasificación      y        regresión 
principalmente.
Bayes Grid Search
Bayes Grid Search
Bayes Grid Search utiliza la técnica 
de   optimización    bayesiana    para 
modelar el espacio de búsqueda con el 
fin   de    llegar   a   valores    de 
hiperparámetros     optimizados     de 
manera rápida. Utiliza la estructura del 
espacio de búsqueda para optimizar el 
tiempo.
Bayes Grid Search
El  enfoque  de  búsqueda  de  Bayes 
utiliza  los  resultados  de  evaluaciones 
anteriores    para     probar    nuevos 
candidatos      que      tienen     más 
probabilidades     de    dar    mejores 
resultados.
Métodos Hypertuning Parameter
Los métodos más comunes son
1. Ajuste Manual
2. Grid Search CV
3. Randomized Search
4. Halving Grid Search
5. Halving Randomized Search
6. HyperOpt-Sklearn
7. Bayes Grid Search
Resumen 
      Método                     Ventajas                              Desventajas
   Validación      Permite ahorrar tiempo y costo        Al estimar las mejores combinaciones 
   simple          computacional                         manualmente podemos inducir error
   GridSearch      Búsqueda exhaustiva que busca en  Puede consumir bastante tiempo y 
   CV              case a los hiperparametros que se     recurso si no se delimita bien. Además 
                   consideran relevantes                 puede generar overfitting
   Randomized  Mucho más rápido que                      Si se eligen muchas iteraciones se tiene 
   SearchCV        GridSearchCV. Menor chance de         menor velocidad. Tiene un alto potencial 
                   Overfitting                           de generar varianza por ser aleatorio
   Halving Grid  Puede encontrar combinaciones de        También puede generar varianza en el 
   Search          hiperparametros con igual             proceso de estimación si no se ejecuta 
                   precisión que GridSearchCV en         correctamente
                   menor tiempo
Resumen 
       Método                     Ventajas                                Desventajas
    Halving         Alta precisión, menor costo            Al igual que el Randomized Search tiene 
    Randomized  computacional y menor                      un alto potencial de generar mucha 
    Search          probabilidad de cometer overfitting    varianza en la búsqueda
    HyperOpt        Permite utilizar modelos               Si el espacio de búsqueda es muy grande 
    Sklearn         secuenciales (SMBO) para utilizar      puede llegar a ser ineficiente y consumir 
                    información a priori y posteriori      mucho tiempo
    Bayes Grid      Limita el número de veces que un       Las distribuciones a priori deben elegirse 
    Search          modelo necesita ser entrenado          con cuidado porque de esto depende el 
                    para la validación                     correcto funcionamiento de la estrategia 
Ejemplo en vivo
A continuación analizaremos un ejemplo 
donde pondremos en práctica el proceso de 
Optimización de Hiperparametros con las 
técnicas aprendidas y revisaremos la 
metodología para aplicar Optimización 
Bayesiana.
Hypertuning de parámetros 
Utilizaremos lo aprendido en clase sobre Cross Validation 
e Hypertuning de Parámetros con el fin de optimizar 
       modelos de ML
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Hypertuning de 
Parámetros
Trabajaremos con base en lo desarrollado en clases 
previas con los datos de fuga en el enlace: 
Telco Customer Churn
1. Separar los datos en train (70%)/test(30%)
2. Utilizar uno de los métodos de validación cruzada 
aprendidos en clase y calcular las métricas 
(recall/precision/accuracy)
3. Evaluar el performance del modelos
4. Realizar optimización de hiper parámetros con 
alguno de los métodos aprendidos 
     Desafío Complementario: 
   CrossValidation y mejora de 
            modelos de ML 
   Continuaremos hablando sobre lo trabajado en el desafío “Ingeniería de atributos 
   y  selección de variables”. Crearás un notebook donde se utilizará uno de los 
   modelos de ML utilizados con las mismas variables que en el desafío anterior pero 
   aplicando métodos de validación cruzada. Con esto, se podrán identificar razones 
   por las cuales hay o no mejoras en el desempeño del modelo de ML. 
    DESAFÍO COMPLEMENTARIO
CrossValidation y mejora de modelos 
de ML                                         Formato
Consiga                                        ✓ Entregar un Jupyter notebook con el 
Deberán trabajar sobre el notebook creado:        nombre 
 ✓ Entrenar uno de los modelos elegidos con 
     las mismas variables pero aplicando          “Desafio_CrossValidation_+Nombre
     alguno de los métodos aprendidos de          _ +Apellido.ipynb”.
     validación cruzada                      Sugerencias
 ✓ Describir si hay cambios en el 
     performance del modelo y explicar con     ✓ Se recomienda utilizar entre 5-10 folds 
     razones el porqué                            para el proceso de CrossValidation con 
Aspectos a incluir                                el fin de optimizar el tiempo de 
 ✓ Notebook donde se detallen todos los           ejecución de la técnica. 
     pasos seguidos
Ejemplo
 ✓ K Fold CrossValidation
¿Preguntas?
CLASE N°49
Glosario
Validación de modelos:                   Hiperparametros: son los ajustes 
metodologías utilizadas para evaluar     del modelo para que pueda resolver 
el desempeño de modelos de ML            de manera óptima el problema de 
                                       aprendizaje automático 
con diferentes esquemas posibles
                                       Hypertuning Parameter: La 
Parámetros: son todos aquellos           optimización de hiper parámetros en 
que se ajustan con los datos a partir    el aprendizaje automático, tiene por 
del proceso de entrenamiento y           objeto encontrar los hiper parámetros 
permiten que el algoritmo de ML se       de un determinado algoritmo de 
puede desarrollar                        aprendizaje automático que ofrezcan 
                                       el mejor rendimiento medido en un 
                                       conjunto de validación. 
Opina y valora 
esta clase
           Resumen 
       de la clase hoy
      ✓ Repaso de validación de modelos
      ✓ Parámetros vs Hiperparametros
      ✓ Optimización de hiperparametros
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 52. DATA SCIENCE
 Introducción a Deep 
        Learning
Temario
                     51                            52                            53
              Despliegue de             Introducción a Deep               Introducción al 
             Modelos MLOps                     Learning                 Procesamiento de 
                                                                       Lenguaje Natural I
            ✓ Fundamentos de Cloud        ✓ Introducción a Deep          ✓   Procesamiento del 
               Computing                      Learning                       lenguaje natural
            ✓ DevOps vs. DevSecOps        ✓ Perceptrón y                 ✓   Expresiones regulares
            ✓ Continuous Deployment           perceptrón multi-          ✓   Bag of words
                                              capa
            ✓ Data team                                                  ✓   NLTK
                                          ✓ CNN
            ✓ MLOps                                                      ✓   Corpus, Document team, 
                                          ✓ RNN                              Matrix y Term Document 
                                                                             Matrix
Objetivos de la clase
         Conocer las bases del Deep Learning
         Conocer las principales arquitecturas usadas 
         en las áreas de Visión por Computadora y NLP
         Entender el impacto que Deep Learning tiene 
         en la industria
  MAPA DE CONCEPTOS
                                                    Deep Learning
                                                   Perceptrón y MLP
                                                       CNN
        Introducción a 
        Deep Learning                                  RNN
                                                     Industrias
                                                    Ejemplos de 
                                                     aplicación
Introducción a Deep 
Learning
Definición
Deep Learning
Deep learning es un subconjunto de 
machine learning (que a su vez es 
parte de la inteligencia artificial) donde 
las redes neuronales, algoritmos 
inspirados en cómo funciona el cerebro 
humano, aprenden de grandes 
cantidades de datos y son el fundamento 
de esta disciplina.
Deep Learning
Los algoritmos de aprendizaje profundo 
realizan una tarea repetitiva que ayuda a 
mejorar de manera gradual el resultado a 
través de las “deep layers” lo que 
permite el aprendizaje progresivo. Este 
proceso forma parte de una familia más 
amplia de métodos de aprendizaje 
automático basados en redes neuronales.
Origen
Origen
          Ingresa a este link para ver y explorar la línea de 
            tiempo sobre el Origen del Deep Learning
Fuente: Origen Deep Learning
Origen
Los primeros en trabajar en Deep Learning fueron Walter Pitts y Warren McCulloch con 
el desarrollo de ‘thresholded logic unit’ diseñada para emular el funcionamiento de las 
neuronas
Fuente: Origen Deep Learning
Origen
Frank Rosenblatt creó en 1957 el algoritmo perceptrón que sería el primer precursor 
de las redes neuronales modernas
Fuente: Origen Deep Learning
Origen
Minsky en colaboración con Seymor Papert crearon el libro Perceptrons que cambió 
la perspectiva de lo que era una red neuronal con la demostración del problema XOR
Fuente: Origen Deep Learning
Origen
Cerca de 1986 se desarrolló la idea del algoritmo backpropagation por medio de Geoff 
Hinton, David Rumelhart y Ronald Williams , resolviendo problemas clásicos de 
Perceptrón 
Fuente: Origen Deep Learning
Origen
Sin embargo esto no fue suficiente ya que con el desarrollo del algoritmo SVM en 1995 
quedaron en el congelador por su inconveniente de no ser escalables a problemas 
grandes 
Fuente: Origen Deep Learning
Origen
Cerca de 2006 Hinton resolvió el problema de la complejidad computacional de las 
redes neuronales generando un nuevo paradigma en las redes neuronales 
Fuente: Origen Deep Learning
Origen
Luego de 2012 comenzó un estallido en el uso de métodos asociados a redes neuronales para la 
resolución de problemas no lineales a gran escala lo cual se demostró en IMAGENET
Fuente: Origen Deep Learning
Características de 
Deep Learning
Características de Deep Learning
Algunas de las características más 
importantes de Deep Learning son:
1. Supervisado, Semi Supervisado o No 
supervisado
2. Grandes cantidades de recursos 
3. Grandes cantidades de capas en 
modelos
4. Sensibilidad a hiperparametros
5. Funciones de costo importantes
Supervisado, Semi Supervisado o No 
Supervisado
Deep learning usa aprendizaje supervisado 
en situaciones como por ejemplo en 
clasificación de imágenes (etiquetas 
conocidas)
De igual forma puede ser usado como 
aprendizaje no supervisado utilizando 
similitudes 
Adicionalmente los auto encoders se 
consideran aprendizaje semi supervisado 
ya que pueden comprimir y reconstruir 
imágenes 
Grandes cantidades de recursos
Requiere de Graphical Processing Units (GPUs) 
para procesar trabajos pesados. Una gran 
cantidad de datos estructurada, no 
estructurada y semiestructurada  requiere ser 
procesada.
Los problemas en la industria IT actualmente 
cuentan con grandes cantidades de 
información muy variada y a la que se le 
puede sacar provecho
Grandes cantidades de capas en modelos
Para poder comprender las relaciones no 
lineales es necesario contar con grandes 
cantidades de datos y bastantes capas de 
entrada, ocultas y de salida. 
Cada una de estas capas procesa la 
información con el fin de lograr un 
entendimiento de la situación problema. Para 
ellos es importante elegir funciones de 
activación apropiadas.
Sensibilidad a hiperparametros 
Hiper parámetros como: Número de épocas, 
Batch Size, Número de capas, Tasa de 
aprendizaje necesitan ser ajustados de 
manera apropiada para lograr buenas 
métricas de performance.
Las redes neuronales son sensibles a estas 
condiciones, por ende cometer Overfitting y 
underfitting puede ser algo común si se eligen 
mal estos parámetros 
Funciones de costo importantes
En las redes neuronales se utilizan las 
funciones de costo para determinar si el 
modelo tiene buen o mal desempeño. 
El objetivo es minimizar el costo cuando se 
compara con iteraciones previas. Existen 
diversos tipos de estas funciones como: Mean 
Absolute Error, Mean Squared Error, 
Hinge Loss, Cross Entropy entre otras son 
ejemplos
Ventajas de Deep 
Learning
Ventajas de Deep Learning
1. Resuelven problemas complejos como 
procesamiento de audio, texto y 
reconocimiento de imágenes entre otros
2. Pueden utilizar las bondades de la 
computación en paralelo
3. Los modelos se pueden entrenar con 
grandes cantidades de datos y se hacen 
mejores
4. Predicciones de alta calidad y escalabilidad
5. Funcionamiento apropiado con data no 
estructurada
Desventajas de Deep 
Learning
Desventajas de Deep Learning
1. Se requieren grandes cantidades de datos 
comparado con otras técnicas
2. Puede llegar a ser costoso debido a la 
complejidad de los modelos
3. No existe una teoría estándar que pueda 
guiar en la selección de herramientas y 
requiere de fuertes conocimientos en 
matemáticas y topología
4. Los sistemas aprenden por sí mismo y son 
difíciles de comprender su evolución
Para pensar
Ahora bien, ¿Han escuchado previamente de 
alguna técnica de Deep Learning?, ¿Por qué Deep 
Learning hace parte de Machine Learning?
Contesta en el chat de Zoom 
Perceptrón y 
Perceptrón multi-
capa
Perceptrón
Perceptrón
Frank     Rosenblatt,     un      psicólogo 
estadounidense,  propuso  el  modelo  de 
perceptrón  clásico  en  1958.  Minsky  y 
Papert  (1969)  lo  refinaron  y  analizaron 
cuidadosamente;  su  modelo  se  conoce 
como modelo de perceptrón.
El  modelo  de  perceptrón,  propuesto  por     Fuente: Perceptron
Minsky-Papert,      es      un      modelo 
computacional más general que la neurona 
de McCulloch-Pitts.
Perceptrón
En este caso el output deseado se obtiene 
como una suma ponderada de las variables 
input.
A  diferencia  del  algoritmo  de  neurona 
propuesto por McCulloch y Pitts los inputs 
ya  no  necesitan  ser  variables  binarias  lo 
cual lo hace un algoritmo generalizable y 
bastante útil en diversos contextos             Fuente: Perceptron
Ejemplo
Ejemplo
Consideremos la tarea de predecir si una persona verá un juego aleatorio o no. 
Utilizaremos 3 variables independientes binarias para simplificar el análisis 
Fuente: Perceptron
Ejemplo
w1,w2 y w3 son los pesos por otro lado w0 se le conoce como el sesgo. Todos 
estos  parámetros  dependen  de  los  datos.  Mientras  más  grande  (>θ)  sea  la 
suma ponderada mayor probabilidad tendremos de que una persona vea el 
partido
Fuente: Perceptron
Ventajas y 
Desventajas
Ventajas y desventajas
                 Ventajas                             Desventajas
         Tiene buenos fundamentos           No se puede entender fácilmente la 
                matemáticos                  representación del conocimiento
        Si la solución existe puede ser          Es un clasificador binario 
                encontrada                           principalmente
      Funciona bien con problemas bien      No funciona bien con datos que no 
                 definidos                sean linealmente separables (problema 
                                                          XOR)
      Funciona bastante bien a pesar de   No se puede actualizar el conocimiento 
         presentar ruido en los datos           del algoritmo si no es con 
                                                     entrenamiento
Perceptrón 
multicapa (MLP)
Perceptrón multicapa (MLP)
Un perceptrón multicapa (MLP) es una red 
neuronal artificial que genera un conjunto 
de  salidas  a  partir  de  un  conjunto  de 
entradas. 
Se caracteriza por varias capas de nodos 
de  entrada  conectados  como  un  grafo 
dirigido entre las capas de entrada y salida.     Fuente: Perceptron
Utiliza  backpropagation  para  entrenar  la 
red ajustando los pesos correspondientes
Perceptrón multicapa (MLP)
Cada nodo excepto los de entrada, tiene 
una función de activación no lineal.
El  MLP  es  un  algoritmo  que  se  usa 
ampliamente para resolver problemas que 
requieren  aprendizaje  supervisado.  Tiene 
diversas    aplicaciones    que     incluyen 
reconocimiento de voz, reconocimiento de         Fuente: Perceptron
imágenes y traducción automática.
Ejemplo
Ejemplo
Consideremos  la  tarea  de  predecir 
el salario neto de una persona con 
base en dos variables (x1= cantidad 
de  horas  trabajadas  y  x2=  horas 
extra)
En    la  figura  de    la  derecha 
observamos     la   aplicación   del 
algoritmo  back-propagation  con  el 
fin    de    obtener    los   pesos 
correspondientes
Ejemplo
Pasos  involucrados  en  el  algoritmo 
Back propagation:
1. Propagación  Forward:  en 
   donde    se   propagan    las 
   funciones  de  activación  desde 
   el input hacia el output
2. Propagación       Backward: 
   cuando  el  error  entre  valores 
   reales  y  los  predichos  para 
   ajustar pesos y sesgo
Ventajas y 
Desventajas
Ventajas y desventajas
                  Ventajas                               Desventajas
        Puede ser aplicado a problemas        No se conoce hasta qué grado cada 
             complejos no lineales           variable independiente es afectada por 
                                                        la dependiente
     Funciona bien con grandes cantidades     Los cálculos son difíciles de realizar y 
                   de datos                       consumen bastante tiempo
     Provee predicciones rápidas luego de     El funcionamiento propio del modelo 
                entrenamiento                      depende de la calidad del 
                                                        entrenamiento
      Mismo grado de precisión se puede      No se comprende del todo cómo opera 
     alcanzar con menores cantidades de       el mecanismo de entendimiento del 
                    datos                                  problema
Para pensar
¿Si aumentamos la cantidad de capas 
ocultas en una red neuronal de 
Perceptrón multicapa, el error de 
clasificación de los datos de prueba 
siempre disminuye?
Verdadero/Falso 
Contesta en el chat de Zoom 
 Para pensar
Falso, no siempre se disminuye el error 
en el entrenamiento al aumentar las 
capas de una red neuronal, muchas 
veces se puede generar overfitting al 
aumentar las capas
Contesta en el chat de Zoom 
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Redes neuronales 
Convolucionales 
(CNN)
Definición
Redes neuronales convolucionales 
(CNN)
Una red neuronal convolucional (CNN por sus siglas en inglés) es 
un tipo de red neuronal artificial (ANN) utilizada en el 
reconocimiento y procesamiento de imágenes que está diseñada 
específicamente para procesar datos en píxeles.
Redes neuronales convolucionales 
(CNN)
Las CNN son una herramienta potente 
para el procesamiento de imágenes que 
utilizan el aprendizaje profundo para 
realizar tareas tanto generativas como 
descriptivas (computer vision) que 
incluyen por ejemplo el reconocimiento 
de imágenes y videos, junto con sistemas 
de recomendación y procesamiento de 
lenguaje natural (NLP).
Origen
Origen
Las CNN en realidad se originaron con el diseño de LeNet por Yann 
LeCun entre 1989 y 1998 para la tarea de reconocimiento de 
dígitos escritos a mano.
Origen
El mérito de las arquitecturas más 
nuevas de las CNN se debe al desafío 
sobre ImageNet denominado "Imagenet 
Large scale visual recognition challenge 
(ILSVRC)". Se inició en 2010, lo que 
condujo a un esfuerzo significativo 
entre los investigadores para comparar 
sus modelos de aprendizaje automático 
y computer vision, en particular para la 
clasificación de imágenes
Origen
Después de esto se desarrollaron diversos 
modelos con el paso del tiempo que 
obtuvieron muy buenos resultados en la 
clasificación de Imagenet:
1. AlexNet (2012) Alex Krizhevsky y 
Geoffrey Hinton
2. ZFNet (2013) Zeiler y Fergus
3. VGGNet (2014) Visual Geometry 
Group de Oxford
4. GoogLeNet (2014)        Fuente: CNN history
5. ResNet (2015) Kaiming He et al. 
Arquitectura
Arquitectura
Una CNN básica normalmente tiene tres capas: una capa 
convolucional, una capa de agrupación (Pooling), capa totalmente 
conectada (fully connected) y capas no lineales.
1) Convolución
La capa de convolución es el componente 
de construcción central de la CNN. Lleva 
la parte principal de la carga 
computacional de la red.
Esta capa realiza un producto escalar 
entre dos matrices, donde una matriz es 
el conjunto de parámetros que se pueden 
aprender, también conocido como 
kernel, y la otra matriz es la parte 
restringida del campo receptivo. 
Fuente: Operación Convolución
1) Convolución
El kernel es espacialmente más pequeño 
(2x2 o 3x3) que la imagen pero tiene 
mayor profundidad. Esto significa que, si 
la imagen se compone de tres canales 
(RGB), la altura y el ancho del kernel 
serán espacialmente pequeños, pero la 
profundidad se extenderá hasta los tres 
canales de la imagen.
Fuente: Operación Convolución
1) Convolución
Durante la fase forward, el kernel 
se mueve a lo largo de la altura y el 
ancho de la imagen, lo que produce 
la representación de la imagen por 
regiones. Esto produce una 
representación bidimensional de la 
imagen conocida como activation 
map, que da la respuesta del 
kernel en cada posición espacial de 
la imagen. El tamaño deslizante del 
kernel se llama stride.
1) Convolución
Detrás de la convolución hay tres 
ideas principales:
1) Interacción escasa: esto se 
logra haciendo que el kernel sea 
más pequeño que la imagen 
original donde se logran detectar 
patrones en regiones pequeñas 
esto genera que se usen menos 
parámetros lo cual reduce memoria 
ocupada y aumenta la eficiencia
1) Convolución
2) Parámetros compartidos: a 
diferencia de las redes neuronales 
convencionales (ANN) donde se 
establecen pesos para la 
calibración de parámetros en las 
CNN en principio se da el mismo 
peso a todos los píxeles por ende 
los pesos aplicados a una entrada 
son los mismos que los aplicados a 
otros.
1) Convolución
3) Equivalencia a translación: 
Debido al intercambio de 
parámetros, las capas de las redes 
neuronales convolucionales tendrán 
una propiedad de equivalencia a la 
translación. Por ende, si cambiamos 
la entrada de alguna manera, la 
salida también cambiará de la 
misma manera.
2) Pooling
Reemplaza la salida de la red en ciertas ubicaciones al derivar una 
estadística de resumen de las salidas cercanas. Esto ayuda a reducir el 
tamaño espacial de la representación, lo que disminuye la cantidad 
requerida de cálculo y pesos. 
2) Pooling
Existen varias funciones pooling, como el promedio, la norma L2, 
promedio ponderado basado en la distancia desde el píxel central. Sin 
embargo, la función más utilizada es la denominada max pooling, 
que arroja como salida el valor máximo del vecindario analizado.
3) Fully connected layers
Las neuronas de las capa tienen 
conectividad total con todas las 
neuronas de la capa anterior y 
posterior. Esta es la razón por la que 
se pueden calcular las salidas como de 
costumbre mediante una 
multiplicación de matrices adicionando 
un efecto de sesgo.
La capa FC ayuda a mapear la 
representación entre la entrada y la 
salida.
3) Fully connected layers
Las capas FC en una red neuronal son 
aquellas donde todas las entradas de 
una capa están conectadas a cada 
unidad de activación de la siguiente 
capa. Usualmente las últimas capas 
son de tipo FC para compilar los datos 
extraídos en capas anteriores con el 
fin de generar el resultado final. Es la 
segunda capa que consume más 
tiempo después de la capa de 
convolución..
4) Capas no lineales
Dado que la convolución es una operación 
lineal y las imágenes están lejos de ser 
lineales, las capas de no linealidad a 
menudo se colocan directamente después 
de la capa convolucional para introducir la 
no linealidad en el mapa de activación. 
Hay varios tipos de operaciones no 
lineales, siendo las más populares:
1. Tanh
2. Sigmoide
3. ReLU (más usada, alta 
convergencia)
¿Dónde se usan las 
CNN?
Aplicaciones de CNN
Algunas de las aplicaciones más 
comunes son:
1. Reconocimiento facial
2. Análisis de documentos
3. Entendimiento del clima
4. Reconocimiento de objetos
5. Sistemas de recomendación
6. Detección de enfermedades
7. Encontrar ciertos tipos de sustancias 
Redes neuronales 
recurrentes (RNN)
Definición
Redes neuronales recurrentes (RNN)
Son un tipo de red neuronal artificial que 
utiliza datos secuenciales o datos de 
series temporales. Se usan comúnmente 
para problemas como traducción de 
idiomas, procesamiento de lenguaje 
natural (nlp), reconocimiento de voz y 
subtítulos de imágenes; se incorporan a 
aplicaciones populares como Siri y Google 
Translate por ejemplo
Redes neuronales recurrentes (RNN)
Así como las CNN, las RNN utilizan datos 
de entrenamiento para aprender. Se 
distinguen por su "memoria" ya que 
toman información de entradas anteriores 
para influir en la entrada y salida actual. 
Mientras que las redes neuronales 
profundas tradicionales asumen que las 
entradas y salidas son independientes, la 
salida de las RNN depende de los 
elementos previos dentro de la secuencia. 
Redes neuronales recurrentes (RNN)
Otra característica distintiva de las RNN es 
que comparten parámetros en cada 
capa de la red. Mientras que las redes 
feedforward tienen diferentes pesos en 
cada nodo, las RNN comparten el mismo 
parámetro de peso dentro de cada capa 
de la red. Los pesos se ajustan a través de 
los procesos de retropropagación y 
descenso de gradiente para facilitar el 
aprendizaje por refuerzo.
Origen
Origen
El concepto de RNN se planteó en 1986. Y la famosa arquitectura LSTM se 
inventó en 1997. La cantidad de arquitecturas conocidas de RNN es mucho 
menor que la de CNN. Existen 3 fases importantes: Vanilla RNN, LSTM y 
Encoder-Decoder
Vanilla RNN
Desarrollada a principios de 1986 donde x, o y h son la entrada, la salida y 
el estado oculto, con U, W y V son los parámetros que se les aplican 
respectivamente. Dentro de la célula RNN de color verde, podría haber una 
o más capas de neuronas normales u otros tipos de RNN.
LSTM
LSTM es la arquitectura RNN más 
popular, incluso después de más de 20 
años de su nacimiento. Pero, ¿por qué 
LSTM? La principal razón es que Vanilla 
RNN no puede recordar muy bien el 
pasado. El único conjunto de 
parámetros en Vanilla RNN tiene que 
procesar y recordar demasiada 
información y se sobrecarga 
fácilmente.
LSTM
Para información secuencial como 
conversaciones y textos. LSTM 
presenta tres tipos de memorias 
seleccionadas (gates) que son puertas 
de entrada, salida y Forget Gates 
respectivamente, así como la función 
sigmoide para representar el 
porcentaje de información para el 
procesamiento.
Encoder-Decoder
Es una arquitectura genérica que no pertenece a la RNN. Se aplica 
ampliamente en casos de uso de aprendizaje profundo de Secuencia a 
Secuencia. Los modelos de NLP previamente entrenados GPT-2 y GPT-3 de 
OpenAI son ejemplos.
Encoder-Decoder
Imita el proceso de cognición del cerebro humano. Por ejemplo, cuando 
vemos algo y lo “codificamos” como algún formato (Encoder Vector) en 
nuestra “red neuronal” (cerebro). Cuando queremos describirlo utilizamos el 
encoder vector y lo decodificamos
Arquitectura
Arquitectura
Existen diferentes tipos de RNN 
con diferentes aplicaciones:
1. One-to-one 
2. One-to-many
3. Many-to-one
4. Many-to-many
One-to-one
Es el ejemplo típico de Vanilla Neural 
Network. Se usa para problemas 
generales de aprendizaje automático, 
que tiene una sola entrada y una sola 
salida.
Ejemplo: Redes neuronales 
tradicionales 
One-to-many
Este tipo de red neuronal tiene una 
sola entrada y múltiples salidas. 
Funciona bien cuando los datos de 
entrada tienen un paso de tiempo y la 
salida contiene un vector de múltiples 
valores.
Ejemplo: Generación de música, 
subtitulos en peliculas 
Many-to-one
Este RNN toma una secuencia de 
entradas y genera una sola salida. 
Los datos de entrada tienen un paso 
de tiempo y la salida contiene un 
vector de múltiples valores.
Ejemplo: Clasificación de 
sentimientos
Many-to-one
Este RNN toma una secuencia de 
entradas y genera una secuencia de 
salidas. La traducción automática es 
uno de los ejemplos.
Ejemplo: Reconocimiento de 
entidades y problemas de traducción
¿Dónde se usan las 
  RNN?
Aplicaciones de RNN
Algunas de las aplicaciones más 
comunes son:
1. Problemas de predicción
2. Modelamiento de lenguaje
3. Traducción
4. Reconocimiento de voz
5. Descripciones de imágenes
6. Video Tagging
7. Análisis en call centers
Casos de uso
Ejemplos en 
industrias
Ejemplos en industrias
1) Aeroespacial: detectores y simulaciones 
de fallas de componentes de aeronaves, 
sistemas de control de aeronaves, pilotaje 
automático 
2) Automotriz: sistemas de guía mejorados, 
sensores virtuales, virtualizadores y 
analizadores  de garantía
Ejemplos en industrias
3) Electrónica: análisis de fallas de chips, 
diseños de chips de circuitos, visión artificial, 
predicción de la secuencias de código, control de 
procesos y síntesis de voz.
4) Manufactura: análisis de diseño, modelado 
dinámico, control de procesos, diagnóstico de 
máquinas, diseño y predicción de la calidad, 
sistemas de inspección de calidad visual y 
análisis de calidad de soldadura
Ejemplos en industrias
5) Mecánica: Monitoreo de condición, modelado 
de sistemas y control, sistemas de prevención de 
fallas, monitoreo de máquina, mantenimiento 
preventivo
6) Robótica: robots montacargas, controladores 
de manipuladores, control de trayectoria y 
sistemas de visión automática, máquinas 
multipropósito
Ejemplos en industrias
7) Telecomunicaciones: control de red de 
cajeros automáticos, servicios de información 
automatizados, procesamiento de pagos de 
clientes, compresión de datos, ecualizadores, 
gestión de fallas, reconocimiento de escritura a 
mano, diseño, gestión, enrutamiento y control de 
redes, monitoreo de redes, traducción en tiempo 
real del lenguaje hablado y reconocimiento de 
patrones 
Ejemplos de 
aplicación
Asistentes virtuales
Los asistentes de voz en la vida cotidiana 
funcionan en las redes neuronales 
capacitadas para ayudarlo y brindarle los 
resultados que solicitó.
Para ello hacen uso de redes neuronales 
recurrentes y modelos de lenguaje 
natural. Ejemplos: Siri, Alexa, Google 
Assistant
Reconocimiento facial
Deep Face es un sistema de 
reconocimiento facial de aprendizaje 
profundo creado por un grupo de 
investigación en Facebook. Identifica 
rostros humanos en imágenes digitales. 
El programa emplea una red neuronal de 
nueve capas con más de 120 millones de 
pesos de conexión y fue entrenado en 
cuatro millones de imágenes cargadas 
por usuarios de Facebook.
Identificación de sentimientos
Instagram identifica algorítmicamente los 
sentimientos detrás de los emojis y con 
esto crea y sugiere automáticamente 
emojis y hashtags relacionados con el 
contexto. Esto puede parecer una 
aplicación menor de la IA pero ser capaz 
de interpretar y analizar esta traducción 
de emoji a texto a mayor escala 
establece la base para un análisis más 
profundo de cómo se usa la aplicación
Computer vision
Se les enseña a las computadoras a "ver" 
como un ser humano, para identificar 
automáticamente objetos en imágenes (o 
"pins", como lo llaman) y luego 
recomendar pines visualmente similares. 
Otras aplicaciones de las redes 
neuronales en Pinterest incluyen 
prevención de spam, búsqueda y 
descubrimiento, rendimiento y 
monetización de anuncios 
Sistemas de recomendación
Amazon le muestra recomendaciones 
usando "clientes que vieron artículos 
similares", "clientes que compraron este 
artículo", y también a través de 
recomendaciones seleccionadas en su 
página de inicio, en la parte inferior de 
las páginas de artículos y a través de 
correos electrónicos. Esto permite 
desarrollar recomendaciones 
personalizadas
Detección de Fraude
FICO, la empresa que crea calificaciones 
crediticias que se utilizan para 
determinar la solvencia, utiliza redes 
neuronales para potenciar su inteligencia 
artificial para predecir transacciones 
fraudulentas. Los factores que afectan el 
resultado final de la red neuronal artificial 
incluyen la frecuencia y el tamaño de la 
transacción y el tipo de minorista 
involucrado.
Ejemplo en vivo
A continuación utilizaremos el siguiente enlace 
para entender el funcionamiento de las redes 
neuronales.
Graficamos diferentes objetos y por medio de 
redes neuronales aprovecharemos el uso de 
sistemas de detección de objetos
Detección de objetos
En esta oportunidad utilizaremos lo aprendido en clase 
para poner en práctica los conceptos de Deep Learning
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Detección de objetos
Nos reuniremos en breakout rooms y 
formaremos grupos, cada estudiante deberá 
tomar turnos como dibujante y espectador 
en Quick Drive:
1. Quienes estén observando deberán 
adivinar lo que el usuario está dibujando 
antes que la aplicación
2. Al final de la ronda intercambiaremos 
ideas sobre el uso de esta tecnología: Si 
puede identificar dibujos ¿qué más 
podría hacer?
¿Preguntas?
CLASE N°52
Glosario
Deep Learning: un subconjunto de              Redes neuronales convolucionales: s 
machine learning donde las redes              un tipo de red neuronal artificial (ANN) 
neuronales, algoritmos inspirados en cómo     utilizada en el reconocimiento y 
funciona el cerebro humano, aprenden de       procesamiento de imágenes que está 
                                             diseñada específicamente para procesar 
grandes cantidades de datos y son el          datos en píxeles.
fundamento de esta disciplina.
Perceptrón multicapa (MLP): Un                Redes neuronales recurrentes: Son un 
perceptrón multicapa (MLP) es una red         tipo de red neuronal artificial que utiliza 
neuronal artificial que genera un conjunto    datos secuenciales o datos de series 
                                             temporales. Se usan comúnmente para 
de salidas a partir de un conjunto de         problemas como traducción de idiomas, 
entradas.                                     procesamiento de lenguaje natural (nlp), 
                                             reconocimiento de voz y subtítulos de 
                                             imágenes
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Introducción a Deep Learning
      ✓ Perceptrón y Perceptrón multicapa (MLP)
      ✓ Redes neuronales convolucionales (CNN)
      ✓ Redes neuronales recurrentes (RNN)
      ✓ Casos de uso: Industrias y ejemplos de aplicación
Opina y valora 
esta clase
Esta clase va a ser
grabad
  a
      Clase 47. DATA SCIENCE
Validación de modelos - 
        Métricas 
Temario
                46                       47                      48
           Selección de             Validación de        Mejora de modelos 
           Algoritmo y               modelos -          de Machine Learning 
        entrenamiento del             métricas                     I
             Modelo II
        ✓ Métricas y               ✓ Análisis de 
           evaluación de              Clustering           ✓ Bias vs. Variance 
           modelos                                            Tradeoff
        ✓ RMSE                     ✓ Métricas de           ✓ Validación de 
                                      calidad para            modelos
        ✓ MAE                         Clustering
        ✓ R2
Objetivos de la clase
         Calcular métricas para evaluar modelos de 
         Clustering
         Identificar diferentes alternativas para 
         evaluar modelos de Clustering
  MAPA DE CONCEPTOS
                                                      Silhouette
                                                      Índice Rand
                                                     Rand ajustado
                           Métricas para              Información 
  Validación de            algoritmos de                mutua
modelos - métricas          Clustering
                                                       Calinski-
                                                       Harabasz
                                                    Davies-Bouldin
    Repaso
             Les proponemos tomarse unos minutos 
             para realizar un repaso de los conceptos 
              aprendidos en Kahoot, ¿están listos?
                  Profe, puedes compartir el 
                   PIN o link de acceso al 
                        juego
Análisis de algoritmos 
de Clustering
Para pensar
En clases pasadas hablamos de diferentes 
algoritmos para poder aplicar Clustering, 
¿Recuerdan alguno de estos algoritmos? 
¿Pertenecían al aprendizaje supervisado o no 
supervisado? ¿Cuál es su función principal?
Contesta en el chat de Zoom 
Definición
Algoritmos de Clustering
Los algoritmos de agrupamiento son métodos no 
supervisado, donde la entrada no está etiquetada y la 
resolución de problemas se basa en la experiencia que el 
algoritmo 
Estos algoritmos aprenden de los atributos disponibles en la 
matriz de diseño X con el fin de generar grupos de 
compartan características similares en los datos analizados
El campo de aplicación de estas técnicas puede ser: gráficos, 
reconocimiento de patrones, análisis de imágenes, 
recuperación de información, bioinformática y la compresión 
de datos.
Algoritmos de Clustering
Lo que buscan estos algoritmos es básicamente 
que:
•  Las  observaciones  dentro  de  un  cluster  sean 
  similares entre sí.
•  Las  observaciones  entre  clusters  sean  lo  más 
  distintas posibles.
Los algoritmos de Clustering siempre “buscan una 
forma natural de agrupar los datos”
Clasificación de 
métodos
Clasificación
Conectividad: como el agrupamiento jerárquico,
Centroides: como el agrupamiento de K-Means
Modelos de distribución: los clústeres se modelan mediante 
distribuciones estadísticas.
Modelos de densidad: DBSCAN u OPTICS, que definen el 
agrupamiento como una región densa conectada en el espacio 
Modelos de grupo: estos modelos no proporcionan resultados 
refinados. 
Modelos basados en grafos: subconjunto de nodos que miden 
adherencia y cohesión entre individuos
Redes neuronales: estas son una de las formas más antiguas 
para agrupar individuos
Para pensar
Si después de aplicar un método de Clustering 
obtienen los resultados respectivos ¿Cómo 
piensan que se debería hacer el proceso de 
caracterización de los clusters obtenidos?
Contesta en el chat de Zoom 
Cómo analizar 
resultados de 
Clustering
Opción 1: Describir                              Ejemplo
cada cluster 
individualmente
Identificar características relevantes para       Clúster 1 – Clientes nuevos que miran 
poder describir individualmente a cada            Netflix.
clúster                                             ● Antigüedad 4 años
Si tengo 3 clúster no habría demasiado              ●
problema, pero si tengo muchos clusters, el             Hacen 48 clicks
proceso se vuelve mucho más complejo.               ● Miran 5 videos
Opción 2: Obtener                            Ejemplo
características de 
resumen
Utilizar medidas de resumen numérico          Podemos utilizar la funciones de 
como media, mediana, varianza, desviación     agregación en Pandas (e.g. GroupBy, 
estándar entre otras variables para           Apply, Pivot) para obtener características 
caracterizar a cada clúster obtenido 
                                             relevantes en cada clúster
PARA RECORDAR
La caracterización de clusters en algunos 
casos puede ser un proceso subjetivo, 
complejo de realizar y que requiere de 
mucho cuidado, sobre todo cuando se 
tienen muchos clusters que analizar es 
difícil encontrar condiciones de 
diferenciación entre cada clúster. La mejor 
elección de grupos depende mucho de la 
pregunta que se quiera responder
          ¡Lanzamos la
          Bolsa de 
          Empleos!
         Un espacio para seguir potenciando tu carrera y 
         que tengas más oportunidades de inserción 
         laboral.
         Podrás encontrar la Bolsa de Empleos en el menú 
         izquierdo de la plataforma.
         Te invitamos a conocerla y ¡postularte a tu futuro 
         trabajo!
           Conócela
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Métricas de calidad 
para Clustering
Métricas de calidad para Clustering
Lo primero que tenemos que saber, es que al no ser 
un método supervisado no hay una métrica 
objetiva como el accuracy, curva ROC, 
precisión, sensibilidad, R2, R2 ajustado o MSE 
por ejemplo. 
Esto significa, que el data scientist es el principal 
“juez” de un algoritmo de Clustering y dicho juicio se 
va a encontrar claramente  condicionado por el 
problema a resolver. 
Métricas de calidad para Clustering
Las 6 métricas más populares para evaluar la 
calidad de los cluster obtenidos son:
1. Score de Silhouette
2. Índice de Rand
3. Índice de Rand Ajustado
4. Criterio de Información Mutua
5. Índice de Calinski-Harabasz
6. Índice de Davies-Bouldin
Analizaremos a continuación cada uno
Score de Silhouette
Score de Silhouette
El Score de Silhouette se usa para medir la distancia de 
separación entre clusters. Muestra la proximidad de 
cada punto respecto a sus clusters más cercanos. 
Tiene valores entre [-1,1] y es una gran herramienta 
para visualizar e inspeccionar similaridades dentro de 
cada cluster y diferencias entre clusters.
Esto significa que entre más cercano a 1 la 
observación ha sido bien asignada, en 
consecuencia más cercano a -1 la observación no ha 
sido bien asignada dentro del cluster. 
Score de Silhouette
Esta métrica nos ayuda a conocer si un registro está bien asignado a un cluster 
determinado o no, utilizando como fundamento las distancias entre puntos y centroides 
de clusters .
sklearn.metrics.silhouette_score(X, labels, *, metric='euclidean', sample_size=None, 
random_state=None, **kwds)
Ejemplo en vivo
A continuación observarán el resultado de 
aplicar el índice de Silhouette para 
diferentes números de clusters. Identificar 
para qué caso se obtiene la mejor 
partición. 
Recordar que mientras más grande sea el 
Score de Silhouette mejor segmentación se 
tiene 
Ejemplo 1: Score de Silhouette
Analicemos el siguiente caso donde tenemos que elegir la cantidad apropiada de grupos. A 
izquierda el índice de Silhouette para k=2 grupos y a la derecha las observaciones
Ejemplo 1: Score de Silhouette
Como podemos observar tanto para el caso K=2 y 3 no se obtiene la mejor partición ya que el 
score se Silhouette para cada grupo está por encima de la puntuación promedio. 
Ejemplo 1: Score de Silhouette
Para el caso de k=4 vamos observando que el valor para el índice de Silhouette se va haciendo 
más uniforme y obtenemos mejores particiones como se puede observar a la derecha  
Ejemplo 1: Score de Silhouette
Sin embargo si aumentamos el valor de K aún más como por ejemplo k=5 se observan algunos 
problemas en la forma de las particiones con grupos desequilibrados
Ejemplo 1: Score de Silhouette
Un caso similar ocurre si seguimos aumentando el número de particiones, en este caso K=6 se 
observan que se van generando subgrupos que pierden cohesión
Ejemplo 1: Score de Silhouette
Para el caso de K=7 la partición se hace muy fina y los valores de la puntuación de Silhouette 
tienden a disminuir considerablemente para cada grupo y en el promedio global
Ejemplo 2: Score de Silhouette
                             ¿Que valor elegimos?
                             ¿Cómo lo interpretamos?
Ejemplo para n=2, 3, 4 y 5 clusters
Ejemplo: Score de Silhouette
                           El valor de n_clusters =4 y 
                           5 no es muy óptimo por 
                           las siguientes razones:
                           1. Presencia de grupos 
                             con puntuaciones de 
                             silueta por debajo del 
                             promedio
                           2. Amplias fluctuaciones 
                             en el tamaño de las 
                             parcelas de silueta.
Ejemplo: Score de Silhouette
                           1. n entre 2 y 3 parece ser el 
                           óptimo. La puntuación de silueta 
                           para cada grupo está por 
                           encima de la puntuación de 
                           silueta promedio. 
                           2. La fluctuación de tamaño es 
                           similar. 
                           3. Para el caso n= 3, el grosor 
                           es más uniforme que el gráfico 
                           con n=2. Por lo tanto, se puede 
                           seleccionar el número óptimo 
                           de grupos como 3.
Índice de Rand
Índice de Rand
Otra métrica de uso común es el Índice Rand. Calcula 
una medida de similitud entre dos clusters 
considerando todos los pares de muestras y contando 
los pares que se asignan en el mismo o en diferentes 
conglomerados en las agrupaciones predichas y 
verdaderas.
sklearn.metrics.rand_score(labels_true, labels_pred)
Índice de Rand
El único inconveniente del Índice de Rand es que 
asume que podemos encontrar las etiquetas de los 
clústeres reales y usarlos para comparar el rendimiento 
de nuestro modelo, por lo que es mucho menos útil que 
el score de Silhouette para tareas de aprendizaje 
no supervisado. Sus valores están entre 0 y 1
sklearn.metrics.rand_score(labels_true, labels_pred)
Índice de Rand 
ajustado
Índice de Rand ajustado
Otra métrica de uso común es el Índice Rand. Calcula 
una medida de similitud entre dos clusters 
considerando todos los pares de muestras y contando 
los pares que se asignan en el mismo o en diferentes 
conglomerados en las agrupaciones predichas y 
verdaderas.
sklearn.metrics.adjusted_mutual_info_score(labels_true, 
labels_pred, *, average_method='arithmetic')
Índice de Rand ajustado
Es una normalización que se le hace al Índice de Rand 
para ajustarlo por sesgo pero sin embargo tiene los 
mismos problemas que el Índice de Rand. Sus valores 
están entre 0 y 1
sklearn.metrics.adjusted_mutual_info_score(labels_true, 
labels_pred, *, average_method='arithmetic')
Criterio de 
información mutua
Criterio de información mutua
Es una medida de la similitud entre dos etiquetas de los 
mismos datos. con |Ui| el número de muestras en el 
clúster Ui y |Vj| el clúster Vj, el valor para dos clústers 
U y V se da como:
sklearn.metrics.mutual_info_score(labels_true, labels_pred, 
*, contingency=None)
Criterio de información mutua
De manera similar al índice Rand, uno de los principales 
inconvenientes de esta métrica es que requiere conocer 
las etiquetas verdaderas. Algo que casi nunca es 
posible en los problemas reales de Clustering.
sklearn.metrics.mutual_info_score(labels_true, labels_pred, 
*, contingency=None)
Índice de Calinski-
Harabasz
Índice de Calinski-Harabasz
Índice para cuantificar desempeño de 
algoritmos de Clustering. Mientras más grande 
sea, indica que la segmentación es la correcta. 
Por lo que no existe un límite fijo para la 
elección del número de grupos
sklearn.metrics.calinski_harabasz_score(X, labels)
Índice de Davies-
Bouldin
Índice de Davies-Bouldin
Se define como la medida de similitud promedio de 
cada grupo con su grupo más similar. La similitud es la 
relación entre las distancias dentro de un grupo y las 
distancias entre grupos. De esta forma, los clusters que 
estén más alejados y menos dispersos darán una mejor 
puntuación.
sklearn.metrics.davies_bouldin_score(X, labels)
Índice de Davies-Bouldin
`Para este caso recordar que se debe elegir el valor 
más bajo del índice de Davies-Bouldin para la elección 
de la mejor segmentación para los datos.
sklearn.metrics.davies_bouldin_score(X, labels)
Ejemplo en vivo
Dentro de la carpeta de clase analizaremos 
cómo calcular diferentes métricas para 
evaluar la calidad al implementar 
algoritmos de Clustering.
Evaluando desempeño 
algoritmos de Clustering
Utilizaremos lo aprendido en clase para poder evaluar 
el desempeño de modelos de Clustering
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Evaluando 
desempeño 
algoritmos de 
Trabajaremos con base en lo desarrollado en clases 
previas con los datos de: Medical Insurance
clustering
1. Identificar variables para correr un algoritmo de 
clustering (no se puede elegir “charges””)
2. Elegir algún algoritmo de Clustering (e.g K-Means o 
clustering Jerárquico) y describir los clusters 
generados
3. Generar predicciones de clusters para cada 
observación en el dataset
4. Calcular métricas para evaluar los resultados 
obtenidos del(los) algoritmo(s) de Clustering 
                                1
                                2
Ingeniería de atributos y 
   selección de variables
     Deberás  entregar  el  duodécimo  avance  de  tu  proyecto  final.  Continuaremos 
     hablando  sobre  lo  trabajado  en  el  desafío  “Evaluando  modelo  de  Machine 
     Learning”.  Crearás  un  notebook  donde  se  terminará  el  proceso  de  Feature 
     Engineering del desafío anterior, se busca que se puedan crear nuevas variables 
     sintéticas que ayuden a mejorar el desempeño de los modelos de Machine Learning. 
     Finalmente, deberás realizar un PCA sobre todas las variables utilizadas con el fin de 
     determinar el peso relativo de cada variable en los modelos. 
                 Recordemos…
                                     Terminamos de realizar el 
                                        proceso de Feature 
                                           Engineering
                                     Ampliamos el número de 
                                       variables en el MVP
                                   Realizamos una segunda ronda 
                                     de entrenamiento con más 
                                             variables
        Clase 45
  Desafío entregable: 
  Evaluando modelos de               Evaluamos al algoritmo
    Machine Learning
     DESAFÍO 
     ENTREGABLE
Ingeniería de atributos y selección de 
variables
Consigna                                           Formato
 ✓ Crear variables sintéticas adicionales que       ✓ Se debe entregar un Jupyter notebook con 
     permitan mejorar el desempeño del                  el nombre: 
     modelo de ML en la entrega anterior.               “Desafío_FeatureSelection_+Nombre_ 
 ✓ Probar distintos modelos y elegir el mejor           +Apellido.ipynb”.
     teniendo en cuenta el Bias-Variance 
     tradeoff                                     Sugerencias
 ✓ Realizar PCA sobre las variables usadas y        ✓ Se recomienda realizar el PCA con el fin 
     explorar las cargas de los 2 primeros              de obtener las variables sintéticas y 
     componentes, identificar las variables             reducir el número de inputs con el fin de 
     más relevantes                                     mejorar el desempeño de los modelos de 
Aspectos a incluir                                       ML elegidos 
                                                    ✓ Dedicar un buen tiempo a la explicación 
 ✓ Notebook donde se detallen todos los                 de la metodología usada
     pasos seguidos
                                                  Explicación del desafío
Ejemplo                                              ✓
 ✓ Feature Selection (Filter Method),                   ¡Click aquí!
CLASE N°47
Glosario
Algoritmos de clustering: métodos no             Criterio de información mutua: Es una 
supervisado, donde la entrada no está            medida de la similitud entre dos etiquetas de 
etiquetada y la resolución de problemas se       los mismos datos. con |Ui| el número de 
                                                muestras en el clúster Ui y |Vj| el clúster V
basa en la experiencia que el algoritmo 
                                                Calinski Harabsz: Índice para 
Score de Silhouette: El Score de                 cuantificar desempeño de algoritmos 
Silhouette se usa para medir la distancia de     de Clustering. Mientras más grande 
separación entre clusters. Muestra la            sea, indica que la segmentación es la 
proximidad de cada punto respecto a sus          correcta.
clusters más cercanos. 
                                                Davies-Bouldin: Se define como la medida 
Índice de Rand: Calcula una medida de            de similitud promedio de cada grupo con su 
similitud entre dos clusters considerando        grupo más similar. 
todos los pares de muestras pero necesita 
etiquetas
¿Preguntas?
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Score de Silhouette
      ✓ Índice de Rand
      ✓ Índice de Rand ajustado
      ✓ Criterio de información mutua
      ✓ Score de Calinsksi-Harabasz
      ✓ Score de Davies-Bouldin
Opina y valora 
esta clase
Esta clase va a ser
grabad
  a
      Clase 45. DATA SCIENCE
Selección del algoritmo 
 y Entrenamiento del 
        Modelo I
Temario
               44                      45                     46
         Algoritmos de             Selección de           Selección de 
          Agrupación II            Algoritmo y            Algoritmo y 
                                entrenamiento del      Entrenamiento del 
                                    Modelo I               Modelo II
       ✓ PCA                    ✓ Matriz de confusión   ✓ Métricas y 
                                ✓ Accuracy                 evaluación de 
       ✓ Reducción de                                      modelos
           Dimensionalidad      ✓ Precisión 
                                                        ✓ RMSE
       ✓ Detección de           ✓ Sensibilidad 
           outliers                                     ✓ MAE
                                ✓ Especificidad 
                                ✓ F1 Score y curva ROC  ✓ R2
Objetivos de la clase
         Evaluar métricas de modelos de Clasificación
    MAPA DE CONCEPTOS
                                                               Accuracy
                                                               Precisión
                                                              Sensibilidad
   Selección del                 Matriz de 
    Algoritmo y               Confusión y sus                 Especificidad
 Entrenamiento del                métricas
     Modelo I
                                                                F1 Score
                                                               Curva ROC
    Repaso
             Les proponemos tomarse unos minutos 
             para realizar un repaso de los conceptos 
              aprendidos en Kahoot, ¿están listos?
                  Profe, puedes compartir el 
                   PIN o link de acceso al 
                        juego
Matriz de confusión y 
sus métricas
Para pensar
En clases pasadas hablamos de diferentes 
métricas para Algoritmos de clasificación…
✔  ¿Qué era una Matriz de Confusión? ¿Para que la 
utilizabamos? 
✔ ¿Recuerdan de alguna limitación o tipos de error 
que podíamos tener al aplicar algoritmos de 
Clasificación?
Contesta en el chat de Zoom 
Para qué servía la matriz de 
confusión
✔ Es una herramienta que nos permitía 
visualizar el desempeño de algoritmos de 
Aprendizaje Supervisado - Clasificación  
✔ Con ella podíamos cuantificar que tantas 
predicciones correctas/incorrectas 
resultaban luego de aplicar el algoritmo de 
Clasificación 
✔ Se puede aplicar para problemas de 
clasificación Binaria y multiclase 
Definición
Matriz de confusión
Es una técnica para resumir el desempeño de 
un algoritmo de clasificación.  El accuracy de la 
clasificación por sí sola puede ser engañosa si 
tiene un número desigual de observaciones en 
cada clase o si tiene más de dos clases 
Esta técnica puede dar una mejor idea de qué 
está haciendo bien el modelo de clasificación y 
qué tipos de errores está cometiendo.
Cómo calcular la 
matriz de confusión
Cómo calcular la matriz de 
confusión
1. Se necesita de un dataset de test o 
validación con los valores esperados 
2. Realizar l predicción para cada fila del 
dataset test
3. Del conteo de valores esperados y 
predicciones se encuentra:
a. El número de predicciones correctas
b. El número de predicciones incorrectas 
para cada clase
Cómo calcular la matriz de 
confusión
Estos números se organizan en una tabla o 
matriz de la siguiente manera:
a. Valores esperados: en cada fila de la 
matriz.
b. Predicciones: cada columna de la matriz.
En la diagonal principal de la matriz tenemos las 
predicciones correctas (TP=True Positives y 
TN=True Negatives) y por fuera quedan las 
predicciones incorrectas (FP= False Positive y 
FN= False Negatives)
Cómo calcular la matriz de 
confusión
Dentro de la matriz debemos definir las siguientes 
cantidades en el caso de un problema binario:
a. verdadero positivo (TP): eventos 
predichos correctamente.
b. "falso positivo" (FP): eventos 
pronosticados incorrectamente (Error Tipo 
I).
c. "verdadero negativo" (TN): sin eventos 
predichos correctamente.
d. "falso negativo" (FN): sin eventos 
pronosticados incorrectamente (Error Tipo 
II).
Cómo calcular la matriz de 
confusión
Los Falsos Positivos (FP) - Error Tipo I son 
predicciones que tienen la característica 
cuando no es cierto, por ejemplo predecir 
que un hombre está embarazado cuando 
es falso
Los Falsos Negativos (FN) - Error Tipo II 
son predicciones que NO tienen la 
característica pero realmente si la tienen, por 
ejemplo predecir que una mujer no está 
embarazada cuando realmente lo está.
Para pensar
Para las siguientes situaciones pensemos si 
serían Falso Positivo o Falso Negativo
✔  Una prueba de detección de cáncer da positivo, 
pero usted no tiene la enfermedad.
✔ Un algoritmo dice que una persona no cometerá 
fraude, pero logró cometer fraude la semana 
pasada
Contesta en el chat de Zoom 
Para pensar
Para estos casos las respuestas son
✔  Una prueba de detección de cáncer da positivo, 
pero usted no tiene la enfermedad (Falso 
Positivo: predicción +, valor real -).
✔ Un algoritmo dice que una persona no cometerá 
fraude, pero logró cometer fraude la semana 
pasada (Falso Negativo: predicción -, valor 
real +)
Contesta en el chat de Zoom 
Accuracy
Accuracy
Es una métrica que resume el rendimiento de 
un modelo de clasificación como el número de 
predicciones correctas dividido por el número 
total de predicciones. Se puede dar como 
porcentaje o fracción.
Precisión
Precisión
Es una métrica que resume el rendimiento de 
un modelo de clasificación cuando se tienen 
dos clases con tamaño desigual, se calcula 
como el número de verdaderos positivos 
dividido por el número total de verdaderos 
positivos y falsos positivos.
Sensibilidad
Sensibilidad
Es una métrica que resume el rendimiento de 
un modelo de clasificación cuando se tienen 
dos clases con tamaño desigual, a diferencia 
de la Precisión en lugar de observar la cantidad 
de falsos positivos, analiza la cantidad de 
falsos negativos 
Especificidad
Especificidad
Es una métrica que resume el rendimiento de 
un modelo de clasificación cuando se tienen 
dos clases con tamaño desigual, se calcula 
como el número de predicciones negativas 
correctas dividido por el número total de 
negativos. También se llama tasa de 
verdaderos negativos (TNR)
F1 Score
F1 Score
La puntuación F1 es una métrica que tiene en 
cuenta tanto la precisión y el recall 
(sensibilidad). Solo es 1 cuando la precisión y 
la sensibilidad son 1 ambas, también se 
entiende como la media armónica de la 
precisión y la sensibilidad. Se define de la 
siguiente manera
Resumen
Ejemplo aplicado
Ejemplo aplicado
Imaginen que tenemos estos            En este caso pudiéramos usar cualquiera de 
datos                                 estos métodos
Ejemplo aplicado
Separamos train/test
Ejemplo aplicado
                    Una forma de hacer esto es con la matriz de 
                    confusión
Ejemplo aplicado
En cada fila tenemos los valores predichos por el algoritmo y en las columnas los 
valores reales
Ejemplo aplicado
En la diagonal principal de la matriz de confusión tendremos en verde a los 
Verdaderos Positivos (TP) y Verdaderos Negativos (TN) que son predicciones 
correctas
Ejemplo aplicado
Fuera de la diagonal tenemos a los Falsos Negativos (FN) y a los Falsos Positivos 
(FP) que representan a los errores Tipo I y Tipo II
Ejemplo aplicado
Para el ejemplo que estamos analizando podemos ver que 142+110 individuos 
quedaron bien clasificados, mientras que 22+29 quedaron mal clasificados
Ejemplo aplicado
Para pensar
¿Se puede realizar una matriz de confusión 
cuando tenemos más de dos categorías?
Justifiquen su respuesta.
Contesta en el chat de Zoom 
Ejemplo aplicado
La respuesta es sí si miramos para el caso de 3 categorías como se observa en 
las figuras de arriba tenemos un ejemplo donde en verde están las buenas 
clasificaciones y en rojo las malas
Ejemplo aplicado
                     Incluso para el caso de más de 
                     3 categorías podríamos 
                     construir una matriz de 
                     confusión aunque sería más 
                     complejo la interpretación de 
                     los resultados y la 
                     cuantificación de los errores 
Ejemplo aplicado
La sensibilidad para este caso se enfoca principalmente en el número de 
pacientes que fueron identificado correctamente
Ejemplo aplicado
81 % de las personas con ataque cardíaco      83 % de las personas con ataque 
fueron identificadas con el modelo de         cardíaco fueron identificadas con el 
regresión logística                           modelo de Random Forest
Ejemplo aplicado
La especificidad por el contrario se enfoca en el número de pacientes que no 
tienen la característica y que fueron correctamente identificados 
Ejemplo aplicado
85 % de las personas sin ataque cardíaco       83 % de las personas sin ataque 
fueron identificadas con el modelo de          cardíaco fueron identificadas con el 
regresión logística                            modelo de Random Forest
Ejemplo aplicado
Usamos el modelo de Regresión               Usamos el modelo de Random 
Logística si identificar pacientes sin      Forest si identificar pacientes con 
heart disease es más importante que         heart disease es más importante que 
identificar pacientes con heart             identificar pacientes sin heart disease 
disease 
Curva ROC
Curva ROC
La curva AUC-ROC es una métrica de 
rendimiento que se utiliza para medir el 
rendimiento del modelo de clasificación 
en diferentes valores de umbral.
Cuanto mayor sea el valor de AUC (Área bajo la 
curva), mejor será nuestro clasificador para 
predecir las clases. 
AUC-ROC se utiliza principalmente en 
problemas de clasificación binaria.
Interpretaciones
Curva ROC
Interpretaciones: 
• AUC cercano a 1: performance casi 
perfecta AUC
• AUC cercano a 0.5: performance a 
nivel  chance  (funciona  igual  que 
lanzar una moneda)
Siempre se debe tener presente que 
alcanzar un valor de 1 para el AUC es 
bastante  complejo  pero  un  umbral 
entre  0.8-0.9  es  un  valor  apropiado 
para un modelo que generaliza y no 
memoriza
Ejemplo aplicado
Ejemplo aplicado
Para este caso enfoquemonos en un modelo de regresión Logística para resolver el 
problema de clasificación binaria para detectar si un individuo es obeso o no con 
base en el peso.
Ejemplo aplicado
Al hacer el proceso de ajuste de los datos al algoritmo podemos cuantificar que tan 
bien o mal se desempeña en los datos de test y construimos una matriz de 
confusión
Ejemplo aplicado
Si modificamos el valor del threshold o límite de clasificación nuestra matriz de 
confusión variará por cada nuevo valor que establezcamos como límite.
Ejemplo aplicado
Podemos comenzar con un threshold bajo como 0 o 0.1 y podemos calcular la 
matriz de confusión y todas sus métricas que ya conocemos 
Ejemplo aplicado
Podemos ir cambiando the threshold de manera iterativa 
Como resultado vamos observando que podemos ir creando diferentes puntos 
cruzando la False Positive Rate vs True Positive Rate 
Ejemplo aplicado
Podemos ir cambiando the threshold de manera iterativa 
Como resultado vamos observando que podemos ir creando diferentes puntos 
cruzando la False Positive Rate vs True Positive Rate 
Ejemplo aplicado
Podemos ir cambiando the threshold de manera iterativa 
Como resultado vamos observando que podemos ir creando diferentes puntos 
cruzando la False Positive Rate vs True Positive Rate 
Ejemplo aplicado
Podemos ir cambiando the threshold de manera iterativa 
Como resultado vamos observando que podemos ir creando diferentes puntos 
cruzando la False Positive Rate vs True Positive Rate 
Ejemplo aplicado
Como resultado si unimos todos los puntos que generamos tenemos la curva que se 
observa a la derecha
Ejemplo aplicado
Podemos calcular para el mismo problema la curva ROC para otros modelos y con 
base en esto comparar cual tiene mejor desempeño
PCA (Principal Component 
Analysis)
Si calculamos la media de la primera        De igual forma si calculamos la media 
variable tenemos lo siguiente               de la segunda variable tenemos lo 
                                            siguiente
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Ejemplo en vivo
Dentro de la carpeta de clase analizaremos 
cómo evaluar modelos de clasificación 
(SVM, Random Forest, Regresión Logística y 
árboles de decisión) haciendo uso de la 
matriz de confusión y sus métricas
Evaluando desempeño 
Modelo de Clasificación
Utilizaremos lo aprendido en clase para poder evaluar 
el desempeño de modelos de clasificación
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Evaluando 
desempeño de 
En esta oportunidad nos reuniremos en grupos de máximo 4 
Modelo de 
personas. Trabajaremos sobre el dataset de la clase anterior: 
Telco Customer
1. Elegir 4 variables independientes que consideren 
Clasificación
relevantes para predecir el “churn” o “fuga/baja” de 
clientes
2. Realizar el Encoding de las variables independientes 
(una persona codea, los demás dan instrucciones y 
leen documentación) para llevar el dataset a forma 
matricial
3. Elegir uno de los modelos aprendidos (e.g Hnn, RF, Reg 
Logística, árboles de decisión) y entrenarlo
4. Crear la matriz de confusión para evaluar el 
performance del modelo
                                1
                                1
       Evaluando modelos 
     de Machine Learning
     Deberás  entregar  el  décimo  primer  avance  de  tu  proyecto  final.  Continuaremos 
     hablando sobre lo trabajado en el desafío “Entrenando un algoritmo de Machine 
     Learning”. Crearás un notebook donde se complementará el proceso de Feature 
     Engineering del desafío anterior con el fin de mejorar el proceso de entrenamiento 
     de algoritmos de Machine Learning, se compararán resultados obtenidos con los del 
     desafío anterior. 
                  Recordemos…
                                       Combinamos datos de 
                                         diversas fuentes
                                      Realizamos proceso de 
                                        Encoding y Feature 
                                           Engineering
                                     Creamos un modelo MVP 
                                              simple
        Clase 42
  Desafío entregable: 
 Entrenando un algoritmo 
   de Machine Learning              Entrenamos algoritmo de 
                                        Machine Learning
    DESAFÍO 
    ENTREGABLE
Evaluando modelos de Machine 
Learning
Consigna                                    Formato
✓ Realizar una segunda ronda de Feature      ✓ Se debe entregar un Jupyter notebook 
    Engineering con el fin de ampliar el        con el nombre: 
    número de variables incluidas en el         “Desafío_EvaluaciónML_+Nombre_ 
    modelo MVP de la entrega anterior.          +Apellido.ipynb”.
✓ Realizar una segunda ronda de 
    entrenamiento con más variables        Sugerencias
Aspectos a incluir                            ✓ Se recomienda elegir datasets curados 
✓ Notebook donde se detallen todos los          para que la mayor parte del tiempo se 
                                                utilice para el entrenamiento de 
    pasos seguidos                              modelos y no en limpieza de datos
Ejemplo                                     Explicación del desafío
✓ Evaluando Modelos Machine Learning,        ✓ ¡Click aquí!
¿Preguntas?
CLASE N°45
Glosario
Matriz de confusión:  técnica para          Sensibilidad: número de Verdaderos 
resumir el desempeño de un algoritmo de     positivos divididos por verdaderos positivos 
                                           más Falsos negativos
clasificación, puede dar una mejor idea de 
qué está haciendo bien el modelo de         Especificidad: número de predicciones 
clasificación y qué tipos de errores está   negativas correctas dividido por el número 
cometiendo.                                 total de negativos
Accuracy: número de predicciones            F1 Score: métrica que cuantifica la 
correctas dividido por el número total de   precisión y recall de un algoritmo de 
                                           clasificación
predicciones
                                           Curva ROC: métrica de rendimiento que 
Precisión: número de verdaderos             se utiliza para medir el rendimiento del 
positivos dividido por el número total de   modelo de clasificación en diferentes 
verdaderos positivos y falsos positivos.    valores de umbral.
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Matriz de confusión
      ✓ Accuracy
      ✓ Precisión 
      ✓ Sensibilidad 
      ✓ Especificidad 
      ✓ F1 Score 
      ✓ Curva ROC
Opina y valora 
esta clase
Esta clase va a ser
grabad
  a
      Clase 31. DATA SCIENCE
  Exploratory Data 
       Analysis
 Temario
                          30                                 31                                 32
                   Workshop:                     Exploratory Data                    Introducción a la 
                  Limpieza de                      Analysis (EDA)                     visualización de 
                       datos                                                                  datos
                 ✓ Repaso Data                                                         ✓ Historia de las 
                     Wrangling + tips de           ✓ Análisis estadístico                   visualizaciones
                     buenas prácticas
                                                   ✓ Identificación de                 ✓ Principios 
                 ✓ Recomendaciones                                                          generales del 
                     para Data Wrangling                outliers                            diseño analitico
                 ✓ ¿Cómo hacer una                 ✓ Valores perdidos                  ✓ Gestalt
                     revisión de pares y 
                     dar feedback?                                                     ✓ Visualizaciones 
                                                                                            engañosas
Objetivos de la clase
         Identificar utilidades del EDA
         Realizar un EDA en Python

 MAPA DE CONCEPTOS
                             Análisis 
                            estadístico
                            Análisis 
 Correlaciones y        Exploratorio de             Identificación 
   variables                                          outliers
                         Datos (EDA)
                              Valores 
                             perdidos
Análisis estadístico
Definición
¿Qué es un EDA?
El Análisis Exploratorio de Datos o  
Exploratory Data Analysis, tiene como 
finalidad examinar los datos previamente a 
la aplicación de cualquier técnica 
estadística. De esta forma el Científico de 
Datos, consigue un entendimiento básico de 
sus datos y de las relaciones existentes 
entre las variables analizadas.
¿Qué hace el EDA?
El  EDA,  proporciona  métodos  sencillos 
para  organizar  y  preparar  los  datos, 
detectar fallos en el diseño y recogida de 
datos, tratamiento y evaluación de datos 
ausentes, identificación de casos atípicos 
y mucho más.
Para pensar
Resulta importante destacar, que el examen previo de los 
datos es un paso necesario, que lleva tiempo, y que 
habitualmente se descuida por parte de los analistas de datos. 
Las tareas implícitas en dicho examen pueden parecer 
insignificantes y sin consecuencias a primera vista… pero, 
¿son una parte esencial de cualquier análisis 
estadístico? 
Responder en el chat de Zoom 
Para pensar
La respuesta es: sí lo son.
Un examen previo de los datos y la metadata disponible 
permite tener más claro el panorama de los datos, escalas de 
medición, variables involucradas, tipos de datos y restricciones 
que indudablemente afectan los análisis posteriores
Responder en el chat de Zoom 
Utilidades del EDA
Algunas de las preguntas que podemos responder 
gracias a realizar un EDA, son las siguientes:  
✔ ¿Existe algún sesgo en los datos recogidos?
✔  ¿Hay errores en la codificación de los datos?
✔  ¿Cómo se sintetiza y presenta la información 
contenida en un conjunto de datos?
✔  ¿Existen datos atípicos (outliers)? ¿Cuáles 
son? ¿Cómo tratarlos?
✔  ¿Hay datos ausentes (missing)? ¿Tienen 
algún patrón sistemático? ¿Cómo tratarlos?
Etapas del EDA
Etapas del EDA
  1                  2                 3                   4                  5                6
Preparar los      Realizar un         Analizar           Evaluar si         Identificar      Establecer 
datos para         examen          correlacion            fuera              posibles         si fuera 
hacerlos       gráfico de las       es entre           necesario             casos         necesario el 
accesibles a      naturaleza de      variables y         supuestos            atípicos         impacto 
cualquier       las variables     dependencia         distribucion        (outliers) y    potencial que 
 técnica      individuales y un        s                 ales y            determinar     pueden tener 
estadística        análisis                             asimetría            impacto         los datos 
                estadístico                                                 potencial       ausentes
Preparación de 
datos
1) Preparación de datos
Como bien comentamos, el primer 
paso de un EDA es hacer accesible 
los datos a cualquier técnica 
estadística. Para ello, tendremos 
que realizar un input de los datos, 
los cuales recordemos pueden 
provenir de diferentes orígenes 
como ser por ejemplo: Excel, csv, 
Bases de Datos, etc. 
1) Preparación de datos
Luego tendremos que elegir el software de analítica de datos 
que utilizaremos para la manipulación y el procesamiento del 
dataset. En nuestro caso utilizaremos Python.
1) Preparación de datos
La gran mayoría de los softwares orientados al análisis de datos, permiten realizar 
manipulaciones de los datos previas a un análisis de los mismos. Algunas 
operaciones útiles para realizar son las siguientes:
✔ Combinar conjuntos de datos de dos o más archivos distintos.
✔ Seleccionar subconjuntos de los datos.
✔ Dividir el archivo de los datos en varias partes.
✔ Transformar variables.
1) Preparación de datos
✔ Filtrar y ordenar el dataset.
✔ Agregar nuevos datos y/o variables.
✔ Eliminar datos y/o variables.
✔ Guardar datos y/o resultados.
Examen gráfico de 
los datos
2) Examen gráfico 
de los datos
Una vez organizados los datos, el segundo paso 
dentro de un EDA consiste en realizar un 
análisis estadístico gráfico y numérico de las 
variables del dataset, con el fin de tener una 
idea inicial de la información que se encuentra 
contenida en el conjunto de datos, así como 
detectar también en el caso de que existan 
posibles errores de codificación. 
2) Examen gráfico 
de los datos
Es importante entender que el tipo de análisis 
que deberemos realizar va a depender de la 
escala de medida de la variable analizada.
Recordemos que tenemos variables numéricas 
(Discretas y continuas) y categóricas (Nominal 
u ordinal). Cada una de estas variables requiere 
métodos característicos para el analisis.
Correlaciones y 
variables
3) Correlaciones
y dependencia
La correlación estadística simplemente es una medida de 
dependencia lineal entre dos variables. Por ende NO es 
correcto asociar una alta correlación con el concepto de 
causalidad
La causalidad es uno de los fenómenos más difíciles de 
explicar ya que requiere de reglas de asociación que funcionen 
de manera generalizada sin importar el contexto, por esto 
siempre hay que tener cuidado a la  hora de establecer 
conclusiones respecto a las correlaciones que se obtienen.
3) Correlaciones
y dependencia
La  correlación  es  la  covarianza  pero  dividida  por  los  desvíos 
estándares  de  las  dos  variables.  Presenta  la  siguiente  fórmula 
matemática: 
3) Correlaciones
y dependencia
La correlación siempre va a darnos un número 
entre -1 y 1
✔ Mientras más cercanos del valor 1, más fuerte 
es la relación lineal directa entre las variables. 
✔ Mientras más cercanos del valor -1, más fuerte 
es la relación lineal inversa entre las variables.
✔ Si  nos  da  0  entonces  no  hay  relación  lineal 
entre las variables.
3) Correlaciones
y dependencia
En lo que respecta a la fuerza de la correlación, hablamos siempre 
de una correlación: 
  
  
     Nula                Débil              Fuerte
3) Correlaciones
y dependencia
También  es  importante  tener  en  cuenta  2 
aspectos relevantes de destacar:
1. La ausencia de correlación significa 
que no hay una relación lineal, pero 
no que no hay relación.
2. Correlación no es, ni implica, 
causalidad. 
3) Correlaciones
y dependencia
3) Correlaciones
y dependencia
3) Correlaciones
y dependencia
La variable dependiente es aquella cuyo valor depende del valor numérico que adopta 
la variable independiente dentro en la función matemática. 
Distribución de 
variables
4) Supuestos 
distribucionales
En  este  paso  resulta  importante  estudiar  por 
ejemplo,  las  “Medidas  de  Forma”  dentro  del 
ámbito de la Estadística.
Pero  ¿Qué  son  las  medidas  de  forma?  Son 
aquellas  que  estudian  las  características  de  la 
distribución     de    probabilidades     observada. 
Podemos destacar: 
✔ Asimetría.
✔ Curtosis.
4) Asimetría
Una variable es simétrica, si los valores que equidistan de la 
media son iguales. Para una mayor comprensión observemos la 
siguiente imagen:
4) Curtosis
La curtosis mide el grado de apuntamiento o achatamiento de la distribución de frecuencia. Es 
decir, nos ayuda a entender “cuán empinada está la curva”. Adicionalmente, existen diferentes 
tipos de curtosis: 
          ¡Lanzamos la
          Bolsa de 
          Empleos!
         Un espacio para seguir potenciando tu carrera y 
         que tengas más oportunidades de inserción 
         laboral.
         Podrás encontrar la Bolsa de Empleos en el menú 
         izquierdo de la plataforma.
         Te invitamos a conocerla y ¡postularte a tu futuro 
         trabajo!
           Conócela
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Identificación de 
outliers
5) Identificación de outliers
Como  ya  hemos  estudiado  en  otras  unidades  del  curso, 
tenemos que prestar especial atención a los outliers, dado que 
pueden tener un potencial negativo dentro de nuestro EDA. 
5) Identificación de outliers
También, es muy importante aclarar que no debemos eliminar los outliers por 
el  sólo  hecho  de  serlo.  A  menos  que  estemos  100%  seguros  que  ese  valor 
extremo se debe a un error de registro, una falla en el instrumento de medición o 
algún problema externo que sea verificable
Actividad colaborativa
Checkpoint proyectos finales II
Formaremos grupos de 4-5 personas en break-out Rooms y 
analizaremos el siguiente proyecto disponible EDA, 
verificaremos si la estructura planteada es consecuente 
con los resultados obtenidos, además identificaremos 
fortalezas y debilidades en las fases de limpieza , 
transformación y análisis descriptivo de los datos
Posteriormente resolveremos dudas acerca del proyecto 
final.
Duración: 30 minutos
Valores perdidos
6) Impacto de datos 
ausentes
Una  situación  a  la  que  se  enfrenta 
frecuentemente cualquier científico de datos 
es  el  tratamiento  de  los  valores  perdidos. 
Los valores faltantes son aquellos que para 
una  variable  determinada  no  constan  en 
algunas filas o patrones.
¿Por qué se pierden?
Los  3  motivos  principales  por  los  que  se 
suelen tratar los valores perdidos son:
✔ Pueden     introducir  un    sesgo 
  considerable  (una  diferencia  notable 
  entre  los  datos  observados  y  los  no 
  observados).
✔ Hacen el análisis y el manejo de los 
  datos más complicado.
✔ Generalmente  ocasionan  pérdidas  de 
  información.
¿Qué hacer con ellos?
Existen  multitud  de  procedimientos  para  aplicar 
cuando  tenemos  valores  perdidos.  Aunque 
básicamente existen dos aproximaciones posibles:
✔ Eliminar muestras o variables que tienen datos 
faltantes (Riesgoso).
✔ Imputar  los  valores  perdidos,  es  decir, 
sustituirlos por estimaciones.
Para pensar
¿Qué técnicas han utilizado para la 
manipulación de datos ausentes y outliers? 
¿Siempre se tiene la misma estrategia para 
manipularlos?
Responder en el Chat de Zoom
Actividad colaborativa
Exploración dataset GDP per capita
Formaremos grupos de 4-5 personas en break-out Rooms y 
utilizaremos el dataset en el siguiente enlace Stocks y 
analizaremos los siguientes puntos:
✔ Analizar las correlaciones de las acciones
✔ Analizar si hay valores faltantes
✔ Analizar la presencia de outliers 
Duración: 15-20 minutos
CLASE N°31
Glosario
EDA: procedimiento que nos permite              Correlación: medida que permite 
entender y examinar de manera básica un         cuantificar la dependencia entre dos 
conjunto de datos con el objetivo de            variables, puede ser de Pearson (Intervalo-
comprender mejor las relaciones                 Intervalo), Spearman (Ordinal-Ordinal), 
existentes                                      Kendall (Nominal-Nominal). La correlación 
                                               no implica dependencia.
Etapas del EDA: conjunto de pasos para          Outliers: son valores dentro de un 
llevar a cabo la exploración de unos datos      conjunto de datos que varían mucho de los 
(preparación, examen gráfico,                   demás; son mucho más grandes o 
correlaciones, evaluación de                    significativamente más pequeños. Los 
distribuciones, asimetría, valores atípicos     valores atípicos pueden indicar 
e impacto de ausentes)                          variabilidades en una medición, errores 
                                               experimentales o una novedad.
¿Preguntas?
Opina y valora 
esta clase
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Análisis estadístico
      ✓ Correlaciones y variables
      ✓ Identificación de outliers
      ✓ Valores perdidos
Esta clase va a ser
grabad
  a
      Clase 37. DATA SCIENCE 
Análisis Multivariado
Temario
                     36                            37                             38
          Análisis bivariado                    Análisis                    Workshop: 
                                            Multivariado                     Revisión 
                                                                             de pares
              ✓ Análisis                                                  ✓ Revisión de 
                  bivariado               ✓ Objetivos                         pares 
              ✓ Pasos a seguir            ✓ Ventajas y desventajas        ✓ Narrativa de 
                                          ✓ Integración con R y               Presentación 
              ✓ Tipos                         PowerBI                         de datos + 
                                                                              tips
Objetivos de la clase
         Identificar las particularidades del Análisis 
         multivariado de datos
         Reconocer ejemplos y aplicaciones de Python
MAPA DE CONCEPTOS                  Tipos
                 Análisis 
                univariado y 
                 gráficos         Gráficos 
                                 estadísticos
                                Pasos a seguir
Análisis          Análisis 
estadístico         bivariado        Tipos
                                 Objetivos
                 Análisis        Ventajas y 
                Multivariado     desventajas
                               Integración con R y 
                                  PowerBI
Análisis Multivariado: 
Objetivos
Definición
¿Qué es el Análisis 
Multivariado?
✔ Es  una  rama  de  la  estadística  que  abarca  la 
observación  y  el  análisis  simultáneos  de  más  de 
una variable respuesta.
✔ Los  estudios  multivariados  son  similares  a  los 
univariados y bivariados con la diferencia de que 
tienen más de dos variables independientes y en 
algunos casos múltiples variables dependientes. 
¿Qué es el Análisis 
Multivariado?
 ✔ Otra diferencia importante es que en un análisis de 
     múltiples  variables  no  hablamos  de  “correlación 
     simple” ni de estadísticos descriptivos por sí solos, 
     sino    que    apelamos      a   otras    herramientas 
     estadísticas  llamadas  “multivariantes”,  tal  es  el 
     caso de por ejemplo: Análisis de varianza (ANOVA), 
     Estudio multifactorial o Regresiones Múltiples.
¿Qué es el Análisis 
Multivariado?
Los investigadores emplean estudios multivariantes 
cuando requieren examinar la relación entre múltiples 
factores al mismo tiempo. Se diferencia claramente de 
los estudios univariados y bivariados en que plantean 
más de una variable dependiente y varias 
independientes.
Ejemplo de contexto
Por ejemplo si deseamos examinar la capacidad de 
tres nuevos productos químicos para limpiar un 
derrame de aceite, las tres sustancias químicas 
serían las variables variables independientes. En 
un análisis multivariado se podrían mediar las 
propiedades de las sustancias químicas y el efecto 
sobre el medio ambiente como variables 
dependientes variables dependientes
Objetivos
Objetivos del Análisis 
Multivariado
El  objetivo  del  análisis  multivariado  es  variable  en 
relación a lo que queremos conseguir con él. Estos son 
los  diferentes  escenarios  que  explican  el  objetivo  del 
análisis multivariado:
✔ Optimizar los datos o simplificar la estructura: 
Esto  ayuda  a  simplificar  los  datos  en  la  mayor 
medida posible sin sacrificar información valiosa y 
sirve para facilitar la explicación de datos.
Objetivos del Análisis 
Multivariado
 ✔ Ordenar y agrupar: Cuando tengamos múltiples 
     variables,  se  creará  un  conjunto  de  objetos  o 
     variables    "similares"    en    función    de     las 
     características medidas para ordenar y agrupar los 
     datos.
 ✔ Investigar  la  relación  de  dependencia  entre 
     variables: La relación entre variables es algo que 
     puede  resultar  preocupante  para  muchos.  El 
     análisis multivariado nos servirá para saber si todas 
     las  variables  son  independientes  o  dependientes 
     entre sí. 
Objetivos del Análisis 
Multivariado
✔ Relación  predictiva  entre  variables:  Deben 
    determinarse para predecir el valor de una o más 
    variables  a  partir  de  observaciones  de  otras 
    variables.
✔ Construcción  y  prueba  de  hipótesis:  Se 
    prueban    hipótesis   estadísticas  específicas 
    expresadas    en    parámetros     poblacionales 
    multivariados.  Esto  se  puede  hacer  para  probar 
    hipótesis o reafirmar hipótesis previas.
Ventajas y 
Desventajas
Ventajas
Ventajas
✔ Permite a los investigadores ver la relación entre 
variables y cuantificar la relación entre ellas: Se 
puede usar la tabulación cruzada, correlación 
parcial y regresión múltiple para controlar la 
asociación entre variables. 
✔ Muestra capacidad de obtener una visión general 
más realista y precisa que cuando se analiza una 
sola variable. 
Ventajas
✔ Permite una fácil visualización e interpretación de 
los datos
✔ Se analiza más información en simultáneo, dando 
mayor potencia en el análisis
✔ Se entienden mejor las relaciones entre las 
variables
✔ Permite encontrar relaciones que no son posibles de 
ver con el análisis univariado y bivariado 
Desventajas
Desventajas
   ✔ Sus      técnicas    son    complejas,     involucran 
      matemáticas         avanzadas       y     requieren 
      procedimientos estadísticos para analizar datos. 
   ✔ Los  resultados  del  modelado  estadístico  no 
      siempre  son  fáciles  de  entender  para 
      estudiantes o personas sin mucha formación. 
Métodos disponibles
Métodos disponibles
Para  el  análisis  multivariado  tenemos  técnicas  de  regresión  o  clasificación  con  una 
amplia gama de algoritmos disponibles. Estudiaremos algunos de los más usados
Fuente: Hair J. Multivariate Data Analysis: An Overview 
Regresión lineal múltiple
También conocida como regresión múltiple, es 
una  técnica  estadística  que  utiliza  varias 
variables  explicativas  (independientes)  para 
predecir  el  resultado  de  una  variable  de 
respuesta (dependiente). La regresión múltiple 
es  una  extensión  de  la  regresión  lineal  (OLS) 
que usa solo una variable explicativa.
Regresión PCR
PCR  (Principal  Component  Regression)  es  una 
técnica de análisis de regresión que se basa en 
el  análisis  de  componentes  principales  (PCA). 
Se  usa  para  estimar  los  coeficientes  de 
regresión  desconocidos  en  un  modelo  de 
regresión  lineal  estándar.  Los  componentes 
principales  de  las  variables  explicativas  se 
utilizan como regresores
PLSR
 La  regresión  de  mínimos  cuadrados  parciales 
 (PLS)  es  un  método  de  regresión  rápido, 
 eficiente y óptimo basado en la covarianza. Se 
 recomienda  en  casos  de  regresión  donde  el 
 número  de  variables  explicativas  es  alto,  y 
 donde   es   probable  que   las  variables 
 explicativas estén correlacionadas.
ANN
Las redes neuronales artificiales (ANN) utilizan 
algoritmos  de  aprendizaje  que  pueden  hacer 
ajustes de forma independiente, o aprender, en 
cierto  sentido,  a  medida  que  reciben  nuevos 
datos.  Esto  los  convierte  en  una  herramienta 
muy  eficaz  para  el  modelado  de  datos 
estadísticos no lineales.
SVM
Una máquina de vectores de soporte (SVM) es 
un  algoritmo  de  aprendizaje  automático  que 
analiza  datos  para  clasificación  y  análisis  de 
regresión.  SVM  es  un  método  de  aprendizaje 
supervisado que analiza los datos y los clasifica 
en una de dos categorías.  Se utilizan funciones 
kernel  para  encontrar  mejores  planos  de 
separación
KNN
Una máquina de vectores de soporte (SVM) es 
un  algoritmo  de  aprendizaje  automático  que 
analiza  datos  para  clasificación  y  análisis  de 
regresión.  SVM  es  un  método  de  aprendizaje 
supervisado que analiza los datos y los clasifica 
en una de dos categorías.  Se utilizan funciones 
kernel  para  encontrar  mejores  planos  de 
separación
Ejemplo en vivo
Utilizaremos el notebook denominado “Clase 15 - 
Análisis Multivariado.ipynb” con el fin de 
repasar conceptos importantes asociados al 
Análisis Multivariado e interpretación de resultados. 
Realizaremos dos ejemplos. 
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Integración con R y 
PoweBI
    R
R en Jupyter Notebooks
✔ Examinaremos los pasos a seguir dentro de la 
carpeta  de  clase  en  el  archivo  llamado 
Utilizar R en Jupyter Notebook - Instructivo
✔ Finalizada la instalación, correremos en Jupyter 
Notebook el archivo: “Ejemplo R.ipynb”.
✔ Listo! 
Posibles errores
1. No  utilizar  anaconda  Navigator  para  hacer  el 
proceso
2. No crear un ambiente separado del de Python
3. No habilitar la opción Packages: R al momento 
de crear el nuevo ambiente
4. Al  momento  de  ejecutar  olvidar  activar  el 
ambiente creado para R
R en Google Colab
Existen dos formas de utilizar R en Colab
✔ Usar  el  paquete  rpy2  en  el  entorno  de 
ejecución  de  Python.  Este  método  le  permite 
ejecutar la sintaxis de R y Python juntas.
✔ La segunda forma es iniciar un notebook en el 
entorno de ejecución de R.
R en Google Colab
Los pasos a seguir son:
1. Ir al siguiente enlace: Google Colab en R
2. En  una  celda  copiar  el  siguiente  código: 
 %load_ext rpy2.ipython
3. Cada  vez  que  vayan  a  utilizar  código  de  R 
 deberán colocar al inicio de las celdas: %%R. Por 
 ejemplo:
 %%R
 x <- seq(0, 2*pi, length.out=50)
PowerBI
PowerBI en Jupyter 
Notebooks
 Dado que para realizar la integración entre Jupyter 
 Notebook  y  Power  Bi  es  requisito  contar  con  una 
 cuenta activa en el servicio de Power BI, se explicará 
 el proceso a realizar de manera genérica. 
 Por   lo  tanto,   no  es  necesario  realizarlo  es 
 simplemente a modo de ejemplo de aplicación.  
Tableau
 Tableau en Jupyter 
 Notebooks
   Para  el  caso  de  Tableau  es  posible 
   conectarse con Jupyter Notebook por medio 
   de  un  Data  Source  Connector  que 
   permite  utilizar  funciones  analíticas  y 
   utilizar   código  de  Python  para  traer 
   utilidades de Tableau. Si desean saber más 
   sobre  el  tema  les  dejamos  este  enlace   
   Tableau with Jupyter
Análisis multivariado
Aplicaremos los conocimientos aprendidos hasta el 
  momento de Análisis multivariado
     Duración: 15-20 mins
  ACTIVIDAD EN CLASE
Análisis Multivariado
Les proponemos los siguientes ejercicios:
1. Cargar los datos de iris disponibles en la 
   librería seaborn con el siguiente comando: 
   df=sns.load_dataset('iris') para generar gráficos 
   de dispersión comparando las diferentes 
   variables según el tipo de especie.
2. Cargar los datos de diabetes disponibles en 
   Diabetes Dataset y utilizar herramientas 
   (visualizaciones, modelos, tablas) que 
   permitan comprender el funcionamiento de la 
   variable respuesta llamada Outcome
 Primera entrega
En la clase que viene se presentará la consigna de la primera 
parte del Proyecto final,  que nuclea temas vistos entre las 
       clases 1 y 20. 
Recuerda que tendrás 7 días para subirla en la plataforma.
CLASE N°37
Glosario
Análisis multivariado: Es una rama de           Desventajas del análisis multivariado: 
la estadística que abarca la observación y      sus técnicas pueden llegar a ser complejas 
el análisis simultáneos de más de una           desde el punto de vista metodológico y eso 
variable respuesta.                             hace que los resultados en algunas 
                                               ocasiones no se puedan transmitir de 
Ventajas del análisis multivariado:             manera intuitiva
Permite una fácil visualización e 
interpretación de los datos. Analiza más        Integración con r y PowerBI: Python es 
información en simultáneo, dando mayor          una excelente herramienta que permite la 
potencia en el análisis. Se entienden           integración de R y PowerBI. Para el caso de 
mejor las relaciones entre las variables        R es bastante simple ya que es un lenguaje 
                                               libre, en cambio PowerBI requiere de 
                                               alguna licencia para poder hacer la 
                                               integración.
¿Preguntas?
¿Aún quieres conocer 
  más?
Te recomendamos el 
siguiente material
 MATERIAL AMPLIADO
Recursos multimedia
 Microsoft Power BI
  ✓ Microsoft Power BI Blog | Power BI | PowerBI Blog
 Tutorial PowerBI
  ✓ Tutorial básico de PowerBI | Datdata | PowerBI tutorial
Disponible en nuestro repositorio.
Opina y valora 
esta clase
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Análisis Multivariado: Objetivos
      ✓ Ventajas y Desventajas
      ✓ Integración con R y PowerBI
       MANUAL DE DESAFÍOS
    Data Science
¡Bienvenidas y 
bienvenidos!
Qué bueno encontrarlos/as en este            A continuación presentamos el sistema de 
espacio el cual hemos creado para que        entregas de los cursos de Coder. Luego, 
puedan conseguir en un mismo lugar, de       en un tablero, podrán ver las clases 
manera rápida y ágil, todos los desafíos     establecidas en el programa además 
entregables que plantea el curso.            de la clase 0, marcando con el ícono 
                                            correspondiente las clases que sí tienen 
                                            entregables (incluido el proyecto final). 
                                            De esta forma podrás tener un pantallazo 
                                            del cronograma de clases y los desafíos 
                                            que deberás completar. 
                  Sistema de entregas
       Desafíos                       Pre-entrega del PF              Proyecto Final
       entregables
       Tienen una vigencia de 7       También tiene una               A diferencia de los 
       días, es decir, a partir de    vigencia o duración de 7        anteriores puntos, cuenta 
       la fecha (de la clase) en      días antes de que el            con un lapso de 20 días 
       la que se lanza el desafío,    botón de “entrega” se           continuos luego de 
       empezarán a correr 7           deshabilite. Por este           finalizada la última clase. 
       días continuos, para que       motivo te recomendamos          Posterior a ese tiempo, el 
       puedas cargar tu desafío       estar al día con todas las      botón de la entrega quedará 
       en la plataforma.              actividades planteadas.         inhabilitado y no será 
                                                                      posible entregarlo o 
                                                                      recibirlo por otros medios. 
GRILLA DE ENTREGAS
Clases     0 1 2  3 4 5 6  7 8 9 1  1 1 1  1
                               0  1 2 3  4
Desafío entregable
Proyecto final
 GRILLA DE ENTREGAS
Clases          1  1  1  1  1  2   2  2  2  2  2  2  2  2  2
               5  6  7  8  9  0   1  2  3  4  5  6  7  8  9
Desafío entregable
Proyecto final
 GRILLA DE ENTREGAS
Clases          3  3  3  3  3  3   3  3  3  3  4  4  4  4
               0  1  2  3  4  5   6  7  8  9  0  1  2  3
Desafío entregable
Proyecto final
 GRILLA DE ENTREGAS
    Clases          4  4  4  4  4  4  5  5  5   5  5  5
                    4  5  6  7  8  9  0  1  2   3  4  5
    Desafío entregable
    Proyecto final
 Módulos de clase
                                Clases 0-             Clases 22 -          Clases 39 - 55
                                   21                     38
                                Módulo 1:             Módulo 2:             Módulo 3: 
                              Fundamentos             Análisis y             Machine 
                              de la Ciencia          exploración             Learning
                                de Datos               de Datos
 TABLERO DE CLASES
                 Clase 0                          Clase 1                          Clase 2                         Clase 3
       Introducción a la Ciencia de           La necesidad de                 Introducción a la                Introducción a la 
                   Datos                    transformación de la         programación con Python I       programación con Python II
                                                Industria 4.0
                 Clase 4                          Clase 5                          Clase 6                         Clase 7
         Introducción a la librería      Programación con Arrays:         Introducción a la librería      Visualizaciones y primeros 
          científica con Python:           Introducción a Numpy             científica con Python:       pasos con Data Science PT. I
                 Pandas I                                                         Pandas II
                 Clase 8                          Clase 9                         Clase 10                         Clase 11
        Visualizaciones y primeros        Estadística Descriptiva:            Herramientas de                 Preprocesamiento 
        pasos con Data Science PT.        como conocer los datos                visualización              estadístico de los datos
                     II
                 Clase 12                        Clase 13                         Clase 14                         Clase 15
          Introducción al análisis      Modelos analíticos para DS I    Modelos analíticos para DS II     Modelos analíticos para DS 
         predictivo con Regresión                                                                                      III
TABLERO DE CLASES
               Clase 16                       Clase 17                       Clase 18                       Clase 19
          Estudios de casos de           Estudios de casos de         Introducción al ML y la IA    Algoritmos y Validación de 
           modelo analítico I             modelo analítico II                                          Modelos de Machine 
                                                                                                             Learning
               Clase 20                       Clase 21                       Clase 22                       Clase 23
           Stack Tecnológico I        Stack Tecnológico del Data       Retomando impulso…               Data Acquisition I
                                              Scientist II
               Clase 24                       Clase 25                       Clase 26                       Clase 27
        Fundamentos de base de        Lenguaje Estructurado de       Lenguaje Estructurado de           Data Acquisition II
                 datos                  Consulta SQL - Parte 1         Consulta SQL - Parte 2
               Clase 28                       Clase 29                       Clase 30                       Clase 31
            Data Wrangling I               Data Wrangling II        Workshop: Limpieza de Data      Exploratory Data Analysis 
                                                                                                              (EDA)
 TABLERO DE CLASES
                 Clase 32                         Clase 33                          Clase 34                         Clase 35
             Introducción a la          Visualización efectiva y Data        GIS y Datos espaciales            Análisis univariado y 
          visualización de Datos                 Storytelling                                                         gráficos
                 Clase 36                         Clase 37                          Clase 38                         Clase 39
             Análisis bivariado             Análisis multivariado            Workshop: Revisión de         En foco: selección de mejora 
                                                                                      pares                         de modelos
                 Clase 40                         Clase 41                          Clase 42                         Clase 43
          Introducción al Machine        Algoritmos de clasificación      Algoritmos de clasificación y     Algoritmos de agrupación I
                  Learning                                                          regresión
                 Clase 44                         Clase 45                          Clase 46                         Clase 47
        Algoritmos de agrupación I         Selección del Algoritmo y        Selección del Algoritmo y        Validación de modelos - 
                                         entrenamiento del Modelo I       entrenamiento del Modelo II                métricas
TABLERO DE CLASES
               Clase 48                      Clase 49                      Clase 50                      Clase 51
          Mejora de modelos de          Mejora de modelos de        Modelos de Ensamble y         Despliegue de Modelos 
           Machine Learning I           Machine Learning II             Boosting Models                   MLOps
               Clase 52                      Clase 53                      Clase 54                      Clase 55
          Introducción a Deep              Introducción al               Introducción al                 Datathon
               Learning             Procesamiento de Lenguaje     Procesamiento de Lenguaje 
                                             Natural I                     Natural II
          CLASE 5
 Programación con 
Arrays: Introducción 
       a Numpy
                    1
  Elección de Datasets 
          potenciales
Deberás entregar el primer avance de tu proyecto final. Identificarás 3 datasets 
potenciales con las siguientes características: i) al menos 2000 filas, ii) al menos 
15 columnas. Posterior a esto crearás un notebook donde cargarás los datos 
utilizando la librería pandas y finalmente describirás las variables que sean más 
interesantes teniendo en cuenta el contexto comercial y analítico del problema 
             que se quiera resolver.
    DESAFÍO 
    ENTREGABLE
Datasets con la librería 
Pandas
Consigna
✓ Identificar 3 datasets que cumplan con         ✓ Cargar los archivos correspondientes 
    las siguientes condiciones: a) al menos         por medio de la librería pandas
    2000 filas y b) al menos 15 columnas.        ✓ Describir las variables 
    Pueden buscar en las siguientes                 potencialmente interesantes en cada 
    fuentes: GitLab, Github, Kaggle, Google         archivo teniendo en cuenta el 
    Dataset Search (Si desean trabajar con          contexto comercial y analítico 
    un archivo propio se puede también)             involucrado
✓ Algunas API recomendadas para                 Aspectos a incluir
    obtener información:                          ✓ El código debe estar hecho 
    Marvel,PokeApi,CovidTracking,Nomics              en un notebook y debe 
    (Criptomonedas),Wheater API                      estar probado.
DESAFÍO 
ENTREGABLE
Datasets con la librería 
Pandas
Formato                        Explicación en video
✓ Entregar un archivo con      ✓ ¡Clickea aquí!
 formato .ipynb. Debe tener el 
 nombre “Datasets+Apellido.ipynb”.
Sugerencias
✓ Preparar el código y probar los 
 resultados con distintas entradas
          CLASE 8
  Visualizaciones y 
primeros pasos con 
 Data Science PT. II
                    2
            Práctica 
        integradora: 
    Visualizaciones en 
             Python
    DESAFÍO 
    ENTREGABLE
Visualizaciones en 
Python                                          Formato
Consigna
 ✓ Deberás entregar el segundo avance de        ✓ Entregar un archivo con formato .ipynb. 
    tu proyecto final. Elegirás uno de los          Debe tener el nombre 
    datasets del desafío “Elección de               “Visualización+Apellido.ipynb”. 
    Potenciales Datasets e importe con 
    la librería Pandas”. Posteriormente,       Sugerencias
    crearás un notebook donde cargaran el       ✓ Preparar el código y probar los resultados 
    archivo utilizando funciones de pandas          con subconjuntos del conjunto original.
    para luego proceder a realizar 3 gráficos 
    diferentes con Matplotlib y 3 con          Aspectos a incluir
    Seaborn. Finalmente, cada gráfico será      ✓ El código debe estar hecho en un 
    interpretado con el fin de obtener              notebook y debe estar probado.
    insights relevantes que permitan dar 
    respuesta a la pregunta problema.
       DESAFÍO 
       ENTREGABLE
 Visualizaciones en 
 Python
Consigna paso a paso                                      Video explicativo
  1.  Escoger uno de los 3 datasets utilizados              ✓ Link al video complementario
      para la Clase 5
  2.  Cargar el dataset con la librería pandas 
      por medio de la función pd.read_csv() o 
      pd.read_excel()
  3.  Realizar al menos tres gráficos (lineplot, 
      scatterplot, histogramas, barchart, 
      boxplot) usando la librería Matplotlib
  4.  Realizar al menos tres gráficos (lineplot, 
      scatterplot, histogramas, barchart, 
      boxplot) usando la librería Seaborn
  5.  Interpretar los resultados de cada gráfica 
      obtenida
          CLASE 12
   Introducción al 
 análisis predictivo 
    con Regresión
                     3
     Estructurando un 
       Proyecto de DS 
Deberás entregar el tercer avance de tu proyecto final. Crearás un notebook que deberá 
            (parte I)
tener en primera instancia un abstract (250/500 palabras) de acuerdo al dataset elegido 
del desafío “Visualizaciones en Python”. Además se deben establecer las preguntas 
e hipótesis de interés sobre el dataset elegido. Finalmente, deberás generar 
visualizaciones (univariadas, bivariadas o multivariadas) junto con resúmenes 
numéricos acompañado de la interpretaciones respectivas que permitan responder la 
              pregunta problema.
Recordemos
                                     Elegimos un dataset de interés
    Desafío anterior:                Realizamos gráficos con 
 Visualización en Python             Matplotlib
                                    Realizamos gráficos con Seaborn
                                   Obtención de Insights preliminares
                                                  ������
     DESAFÍO 
     ENTREGABLE
Estructurando un 
Proyecto de DS (parte I)
                                                Formato
Consigna                                          ✓ Entregar un archivo con formato .ipynb. 
Continuaremos trabajando con base en lo               Debe tener el nombre 
realizado en el Desafío entregable:                   “ProyectoDS_ParteI_+Apellido.ipynb”
Visualización en Python, en esta 
oportunidad deberemos complementar con lo        Sugerencias
siguiente:                                        ✓ Preparar el código y probar los 
 1. Generar preguntas de interés o                   resultados con subconjuntos del 
    hipótesis de interés sobre el dataset            conjunto original.
    elegido para el proyecto final.              ✓ Link video explicativo
 2. Crear visualizaciones (univariados, 
    bivariados o trivariados) junto con         Aspectos a incluir:
    resúmenes numéricos básicos acordes          ✓ El código debe estar hecho en un 
    con los tipos de variables disponibles.          notebook y debe estar probado.
 3. Interpretar los resultados obtenidos
          CLASE 17
Estudios de casos de 
 modelo analítico II
                     4
     Estructurando un 
       Proyecto de DS 
Deberás entregar el cuarto avance de tu proyecto final. Continuaremos hablando sobre 
            (parte II)
lo trabajado en el desafío Estructurando un proyecto DS-Parte I. Crearás un 
  notebook donde se resuelvan los siguientes apartados: i) Abstracto, ii) 
Preguntas/hipótesis, iii) EDA, iv) recomendaciones con base en insights observados y v) 
tener definido en el notebook las secciones: Objetivo, Contexto Comercial, Problema 
   Comercial Contexto analítico y Exploratory Data Analysis (EDA)
Recordemos
                                     Generamos hipótesis de interés
    Desafío anterior:                Creamos visualizaciones y 
Estructurando un proyecto             resúmenes numéricos
       DS- Parte 1
                                    Encontramos patrones de interés
                                    Exploratory Data Analysis (EDA)
                                                  ������
       DESAFÍO 
       ENTREGABLE
Estructurando un 
Proyecto de DS (parte II)
                                                           Formato
Consigna                                                     ✓ Entregar un archivo con formato .ipynb. 
Deberás complementar la parte I con:                              Debe tener el nombre 
  1.  Abstracto con motivación y audiencia                        “ProyectoDS_ParteII_+Apellido.ipynb”
  2.  Preguntas/Hipótesis que queremos resolver 
      mediante el análisis de datos
  3.  Análisis Exploratorio de Datos (EDA)                 Sugerencias
  4.  Con base en las visualizaciones y                      ✓ Preparar el código y probar los 
      resúmenes numéricos generados del desafío                   resultados con subconjuntos del 
      anterior dar recomendaciones basados en                     conjunto original.
      los insights observados.                               ✓ Link video explicativo
  5.  Para esta oportunidad se deberán tener 
      avances en los apartados: Definición de              Aspectos a incluir:
      objetivo, Contexto comercial, Problema                 ✓ El código debe estar hecho en un 
      Comercial, Contexto analítico, Exploración 
      de datos (EDA)                                              notebook y debe estar probado.
          CLASE 19
     Algoritmos y 
    Validación de 
Modelos de Machine 
       Learning
                    5
 Práctica integradora 
Deberás entregar el quinto avance de tu proyecto 
final. Continuaremos hablando sobre lo trabajado en 
el desafío “Estructurando un proyecto de DS Parte II”. 
                  Recordemos…
                                   Generamos 
                                   recomendaciones basados 
                                   en insights obtenidos
                                   Definimos Objetivo, Contexto 
                                   y Problema comercial
                                   Contexto analitico, Limpieza 
                                   de datos y EDA 
      Clase 17
  Estructurando un 
Proyecto DS- Parte II              Obtenemos conclusiones y 
                                   puntos importantes a resaltar
     DESAFÍO 
     ENTREGABLE
Estructurando un proyecto de 
DS-parte III
Consigna                                               Aspectos a incluir
✓ Crearás un notebook que complemente el              ✓ El código debe estar hecho en un 
  trabajo realizado en los siguientes                   notebook y debe estar probado.
  apartados:                                       Formato
    -   i) elegir un método de feature              ✓ Entregar un archivo con formato .ipynb. 
        selection para reducir la                       Debe tener el nombre 
        dimensionalidad del dataset,                    “Proyecto_ParteIII_+Apellido.ipynb”  
    -   ii) elegir un algoritmo de regresión o 
        clasificación para entrenar con los 
        datos elegidos,                            Sugerencias
    -    iii) cálculo de métricas para validar      ✓ Preparar el código y probar los 
        el modelo                                       resultados con subconjuntos del 
    -   iv) generar conclusiones con base en            conjunto original.
        los resultados obtenidos.                   ✓ Video explicativo
          CLASE 21
 Stack Tecnológico 
del Data Scientist II
         Primera entrega 
      de tu Proyecto final
   Debes entregar el Análisis de datos con Python
  correspondiente a la primera entrega de tu proyecto 
                          final.
  Crearás la notebook de un análisis exploratorio de datos sobre un problema a 
 resolver para una industria, negocio o proyecto personal. La notebook contendrá 
 un informe que detalle tus hipótesis primarias y secundarias, el código utilizado 
               para probarlas y su posible resolución.
Recordemos…
                  Aplicamos Feature Selection
                  Elegimos un algoritmo de 
                  Machine Learning 
                  Evaluamos el desempeño del 
                  algoritmo elegido
 Clase 19
Estructurando un 
proyecto de DS-III Obtención de conclusiones con 
                  base en resultados obtenidos
    ENTREGA DEL PROYECTO 
    FINAL
Análisis de datos con 
Python
Objetivos generales                          Formato
✓ Estructurar un problema en función         ✓ Link a repositorio de Github o 
   de múltiples, pero simples                   documento de Jupyter.
   preguntas/hipótesis a responder           ✓ El archivo debe tener el nombre 
✓ Analizar datos tabulares (e.g excel,          “PrimeraEntrega+Apellido.ipynb”
   csv, etc)  usando Python
✓ Utilizar modelos de Machine              Sugerencias
   Learning con Python
                                             ✓ La entrega es individual y se realiza 
                                                a través de la plataforma.
     ENTREGA DEL PROYECTO 
     FINAL
Primera entrega
El modelo de Jupyter Notebook debe contener
 ✓ Abstracto con motivación y audiencia: Descripción de 
     alto nivel de lo que lo motiva a analizar los datos                 Video explicati
     elegidos y qué tipo de audiencia se podría beneficiar de            vo
     este análisis
 ✓ Preguntas/Hipótesis que queremos responder mediante 
     el análisis de datos
 ✓ Estructura acorde.
 ✓ Análisis Exploratorio de datos (EDA): Análisis descriptivo 
     de los datos mediante visualizaciones mediante el 
     análisis de datos
          CLASE 27
 Data Acquisition II
                                6
        Descarga de datos 
      desde APIs públicas
     Deberás entregar el sexto avance de tu proyecto final. Continuaremos hablando sobre lo 
     trabajado en la primera pre entrega del proyecto final. Crearás un notebook donde se 
     seleccionará  una  API  de  interés,  luego  crearás  una  API  key  y  finalmente  extraerás  la 
     información  para  ser  almacenada  en  un  DataFrame  (se  sugiere  que  esta  información 
     complemente o enriquezca el dataset elegido en el desafío  “Elección de potenciales 
     Datasets e importe con la librería Pandas”)
    DESAFÍO 
    ENTREGABLE
Descarga de datos desde APIs 
Consigna                                    Formato
públicas
✓ Buscar información en APIs públicas        ✓ Se debe entregar un jupyter notebook 
    (i.e Twitter, NewsAPI, Spotify, Google      con el nombre 
    Apis, etc).                                 “Desafio_APIS_+Nombre_ 
✓ Extraer datos e importarlos a un              +Apellido.ipynb”.
    dataframe realizando una exploración 
    simple (i.e filas, columnas, tipos de 
    datos). Se sugiere que estos datos     Sugerencias
    complementen el dataset elegido en el    ✓ No compartir sus tokens personales
    Desafio “Elección de potenciales         ✓ Comprender primero el funcionamiento 
    Datasets e importe con la librería          de las APIs a detalle para después 
    Pandas”                                     utilizarla
Aspectos a incluir                            ✓ La limpieza de datos en APIS no es 
✓                                               fácil
    Notebook donde se detallen todos los     ✓ Tratar de obtener datos principalmente 
    pasos seguidos                              numéricos (al menos 20 columnas y 
Ejemplo                                          10000 filas.
✓ Ejemplo Desafío APIS,
          CLASE 29
  Data Wrangling II
                                7
       Desafío entregable: 
           Data Wrangling
     Deberás entregar el séptimo avance de tu proyecto final. Continuaremos hablando 
     sobre lo trabajado en el desafío “Descarga de datos desde APIs públicas”. 
     Crearás un notebook donde se desarrollará la limpieza de los datos elegidos para tu 
     proyecto final, deberás tener en cuenta técnicas vistas en clase para el tratamiento 
     de valores duplicados, nulos y outliers con su respectiva justificación.  
       Recordemos…
                   Exploramos diversas APIs
                   Extrajimos los datos en un 
                       dataframe
                  Realizamos una exploración 
                        simple
  Clase 27
Desafío entregable: 
Data Acquisition    Obtención de Insights 
                      preliminares
  DESAFÍO 
  ENTREGABLE
Data Wrangling
Consigna                              Formato
✓ Iniciar el proceso de limpieza y     ✓ Se espera un notebook en 
  exploración de datos según el          formato .ipynb. Dicho notebook debe 
  dataset elegido para el proyecto       tener el siguiente nombre 
  final                                  “Data_Wrangling+Apellido.ipynb”.
Aspectos a incluir                    Sugerencias
✓ Notebook con código y estructura     ✓ Utilizar las herramientas vistas en el 
  eficiente                              curso
                                      ✓ Manejo de duplicados nulos y análisis 
Ejemplo                                    exploratorio
✓ Data Wrangling                     Explicación del desafío
                                      ✓ ¡Click aquí!
          CLASE 33
    Visualización 
   efectiva y Data 
     Storytelling
                      8
   Desafío entregable: 
      Data Storytelling
  Deberás  entregar  el  octavo  avance  de  su  proyecto  final.  Continuaremos 
  hablando sobre lo trabajado en el desafío “Data Wrangling”. Crearás un 
  notebook donde se desarrolle una narrativa que permita dar respuesta a las 
  preguntas/hipótesis formuladas para el proyecto final.
       Recordemos…
                 Extrajimos datos de interés
                 Comenzamos el proceso de 
                 limpieza y estructuración 
                 Desarrollamos algunas 
                 hipótesis 
  Clase 29
Desafío entregable
Data Wrangling     Data Wrangling/Munging
    DESAFÍO 
    ENTREGABLE
Data Storytelling
Consigna                                     Formato
✓ Iniciar el proceso de Data                 ✓ Se espera un notebook en 
   Storytelling respondiendo las                formato .ipynb. Dicho notebook debe 
   preguntas que se quieran responder           tener el siguiente nombre 
                                                “Data_StoryTelling+Apellido.ipynb”
                                                .
Aspectos a incluir                           Sugerencias
✓ Notebook con código y estructura           ✓ Utilizar las herramientas vistas en el 
   eficiente                                    curso
Ejemplo                                        ✓ Manejo de duplicados nulos y análisis 
✓ Data StoryTelling                             exploratorio
                                             ✓ Comenzar por preguntas de alto nivel y 
                                                luego más específicas
                                           Explicación del desafío
                                             ✓ ¡Click aquí!
          CLASE 36
 Análisis bivariado
                      9
   Desafío entregable: 
         Obtención de 
  Deberás  entregar  el  noveno  avance  de  su  proyecto  final.  Continuaremos 
             Insights
  hablando sobre lo trabajado en el desafío “Data Storytelling”. Crearás un 
  notebook donde se pueda observar las fases de análisis univariado y bivariado 
  que  junto  con  el  trabajo  previo  realizado  permitan  obtener  insights  que 
  ayuden a dar respuesta a la(s) pregunta(s) problema del proyecto final.
Recordemos…
                  Generamos visualizaciones
                  Respondimos preguntas de 
                  interés
                  Soluciones a la pregunta 
                  problema
Clase 33
Desafío entregable:
Data Storytelling
                  Mejora de Insights obtenidos
    DESAFÍO 
    ENTREGABLE
Obtención de insights
Consigna
  ✓ Generar insights que permitan dar      Formato
     respuesta a las preguntas por           ✓ Se espera un notebook en 
     responder                                  formato .ipynb. Dicho notebook debe 
                                                tener el siguiente nombre 
                                                “Data_StoryTelling+Apellido.ipynb”
Aspectos a incluir                              .
  ✓ Notebook con código y estructura         ✓ Presentación en formato pptx o pdf
     eficiente                             Sugerencias
  ✓ Presentación ejecutiva                   ✓ Se recomienda que la historia cuente 
Ejemplo                                         con una estructura similar a la 
  ✓                                             presentación de referencia
     Ejemplo de presentación
                                           Explicación del desafío
                                             ✓ ¡Click aquí!
          CLASE 38
Workshop: Revisión 
       de pares
Obtención de insights a 
partir de visualizaciones
Deberás  entregar  la  segunda  pre  entrega  de  tu  Proyecto  Final.   
Entrenarás  y  optimizarás  versos  modelos  de  machine  learning  para 
resolver  una  problemática  específica,  detectada  en  la  instancia  de 
entrega anterior. El objetivo es que puedan utilizar modelos de Machine 
Learning para resolver el problema de una industria o negocio. 
                 Recordemos…
                                Generamos visualizaciones
                                Obtuvimos insights
                                Desarrollamos una narrativa 
                                correcta
     Clase 36
Desafío entregable:
Obtención de insights             Responder las preguntas 
                                problema
PREENTREGA DEL PROYECTO 
FINAL
Obtención de insights a partir de 
visualizaciones
Objetivos generales
✓ Obtener datos de diversas fuentes como APIs o Bases de datos públicas para luego 
analizarlos mediante el lenguaje Python con el fin de contestar una pregunta de 
interés para una industria, negocio o proyecto personal. Se deberán utilizar datasets 
complejos implementando técnicas avanzadas para la limpieza y adquisición de 
datos
Objetivos específicos
✓ Estructurar un problema en función de múltiples pero simples preguntas/hipótesis a 
responder
✓ Importar datos crudos de APIs o bases de datos usando Python
✓ Limpiar y transformar los datos para permitir un posterior análisis
✓ Contar una historia mediante el análisis exploratorio de datos
PREENTREGA DEL PROYECTO 
FINAL
Obtención de insights a partir de 
visualizaciones
Requisitos base
✓ Un notebook (Colab o Jupyter) que debe contener:
1. Abstracto con motivación y audiencia: Descripción de alto nivel de lo que 
motiva a analizar los datos elegidos y que audiencia  se podrá beneficiar de este 
análisis
2. Preguntas/hipótesis que queremos responder: Lista de preguntas que se 
busca responder mediante el análisis de datos. Bloques de código donde se 
importan los datos desde una API o base de datos pública y los guarda en un 
archivo local csv o json. El estudiante puede luego de descargar los datos, 
comentar este bloque de código
3. Análisis exploratorio de datos (EDA): Análisis descriptivo de los datos 
mediante visualizaciones y herramientas estadísticas
 PREENTREGA DEL PROYECTO 
 FINAL
Obtención de insights a partir de 
visualizaciones
Requisitos base
✓ Una presentación (PDF; PowerPoint o Google Slides) que debe contener
1. Abstracto con motivación y audiencia: Descripción de alto nivel de lo que motiva a 
   analizar los datos elegidos y que audiencia  se podrá beneficiar de este análisis
2. Resumen de metadata: resumen de los datos a ser analizados es decir, número de 
   filas/columnas, tipos de variables, etc
3. Preguntas hipótesis que queremos responder: Lista de preguntas que se busca 
   responder mediante el análisis de datos
4. Visualizaciones ejecutivas que responden nuestras preguntas: utilización de 
   gráficos que responden las preguntas de interés de nuestro proyecto.
5. Insights: resumen de hallazgos del proyecto. Aquí consolidamos las respuestas a las 
   preguntas/hipótesis que fuimos contestando con las visualizaciones
PREENTREGA DEL PROYECTO 
FINAL
Obtención de insights a partir de 
visualizaciones
Sugerencias
Es conveniente retomar el dataset trabajado en la primera pre entrega y enriquecerlo (e.g 
joins, y creación de nuevas columnas) con información proveniente de APIs públicas 
siempre que se pueda con el fin de practicar las nuevas habilidades adquiridas. Se 
recomienda retomar la metodología de trabajo y reutilizar algoritmos ya entrenados, de 
ser necesario.
Requisitos extra
✓ Subir el proyecto a Github
PREENTREGA DEL PROYECTO 
FINAL
Obtención de insights a partir de 
visualizaciones
Dont’s
✓ Utilizar jerga demasiado técnica en la presentación (recordar que la audiencia de la 
misma son roles ejecutivos)
✓ Sobrecargar las diapositivas 
✓ Realizar una presentación con más de 12 slides de extensión
Modelo de Proyecto final
✓ Proyecto final (Notebook) (Se debe abrir con Google Collaboratory o Jupyter 
Notebook)
✓ Ejemplo Presentación 
Explicación del desafío
✓ ¡Click aquí!
          CLASE 42
    Algoritmos de 
    clasificación y 
       regresión
                                1
                                0
             Entrenando un 
     algoritmo de Machine 
                    Learning
     Deberás entregar el décimo avance de tu proyecto final. Continuaremos hablando 
     sobre lo trabajado en la segunda pre entrega del proyecto final. Crearás un 
     notebook donde trabajarás sobre los datos elegidos en la primera y segunda pre 
     entrega del proyecto final. Posteriormente, realizarás las etapas de: i) Encoding, ii) 
     Ingeniería  de  atributos  y  iii)  Entrenamiento  de  un  modelo  de  Machine  Learning 
     Supervisado  (Clasificación  o  Regresión)  o  no  supervisado  dependiendo  de  la 
     pregunta problema.
    DESAFÍO 
    ENTREGABLE
Entrenando un algoritmo de Machine 
Learning
Consigna                                       Formato
 ✓ Utilizar una fuente de datos para           ✓ Se debe entregar un jupyter notebook con 
    resolver problemas de clasificación o          el nombre 
    regresión.                                     “Desafio_AlgoritmoML_MVP_+Nombre
 ✓ Realizar los procesos de Encoding,              _ +Apellido.ipynb”.
    Feature Engineering y entrenamiento       Sugerencias
    de un modelo de Machine Learning           ✓ Se pueden utilizar fuentes de datos 
    (Clasificación o Regresión)                    conocidas en sitios como Kaggle o UCI
Aspectos a incluir                              ✓ Se recomienda elegir datasets curados 
 ✓ Notebook donde se detallen todos los            para que la mayor parte del tiempo se 
    pasos seguidos                                 utilice para el entrenamiento de modelos 
                                                   y no en limpieza de datos.
Ejemplo                                        Explicación del desafío
 ✓ Ejemplo Desafío Entrenamiento ML,           ✓ ¡Click aquí!
          CLASE 45
    Selección del 
     Algoritmo y 
 entrenamiento del 
       Modelo I
                                1
                                1
       Evaluando modelos 
     de Machine Learning
     Deberás entregar el décimo primero avance de tu proyecto final. Continuaremos 
     hablando sobre lo trabajado en el desafío “Entrenando un algoritmo de Machine 
     Learning”. Crearás un notebook donde se complementará el proceso de Feature 
     Engineering del desafío anterior con el fin de mejorar el proceso de entrenamiento 
     de algoritmos de Machine Learning, se compararán resultados obtenidos con los del 
     desafío anterior. 
                  Recordemos…
                                       Combinamos datos de 
                                         diversas fuentes
                                      Realizamos proceso de 
                                        Encoding y Feature 
                                           Engineering
                                     Creamos un modelo MVP 
                                              simple
        Clase 42
  Desafío entregable: 
 Entrenando un algoritmo 
   de Machine Learning              Entrenamos algoritmo de 
                                        Machine Learning
    DESAFÍO 
    ENTREGABLE
Evaluando modelos de Machine 
Learning
Consigna                                    Formato
✓ Realizar una segunda ronda de Feature      ✓ Se debe entregar un jupyter notebook 
    Engineering con el fin de ampliar el        con el nombre 
    número de variables incluidas en el         “Desafio_EvaluacionML_+Nombre_ 
    modelo MVP de la entrega anterior.          +Apellido.ipynb”.
✓ Realizar una segunda ronda de 
    entrenamiento con más variables        Sugerencias
Aspectos a incluir                            ✓ Se recomienda elegir datasets curados 
✓ Notebook donde se detallen todos los          para que la mayor parte del tiempo se 
    pasos seguidos                              utilice para el entrenamiento de 
                                                modelos y no en limpieza de datos
Ejemplo                                     Explicación del desafío
✓ Evaluando Modelos Machine Learning,        ✓ ¡Click aquí!
          CLASE 47
    Validación de 
 modelos - métricas
                                1
                                2
Ingeniería de atributos y 
  selección de variables
     Deberás  entregar  el  duodécimo  avance  de  tu  proyecto  final.  Continuaremos 
     hablando  sobre  lo  trabajado  en  el  desafío  “Evaluando  modelo  de  Machine 
     Learning”.  Crearás  un  notebook  donde  se  terminará  el  proceso  de  Feature 
     Engineering del desafío anterior, se busca que se puedan crear nuevas variables 
     sintéticas que ayuden a mejorar el desempeño de los modelos de Machine Learning. 
     Finalmente, deberás realizar un PCA sobre todas las variables utilizadas con el fin de 
     determinar el peso relativo de cada variable en los modelos. 
       Recordemos…
                   Terminamos de realizar el 
                     proceso de Feature 
                       Engineering
                   Ampliamos el número de 
                     variables en el MVP
                  Realizamos una segunda ronda 
                   de entrenamiento con más 
                        variables
  Clase 45
Desafío entregable: 
Evaluando modelos de 
Machine Learning    Evaluamos al algoritmo
     DESAFÍO 
     ENTREGABLE
Ingeniería de atributos y selección de 
variables
Consigna                                           Formato
 ✓ Crear variables sintéticas adicionales que       ✓ Se debe entregar un jupyter notebook con 
     permitan mejorar el desempeño del                  el nombre 
     modelo de ML en la entrega anterior.               “Desafio_FeatureSelection_+Nombre_ 
 ✓ Probar distintos modelos y elegir el mejor           +Apellido.ipynb”.
     teniendo en cuenta el Bias-Variance 
     tradeoff                                     Sugerencias
 ✓ Realizar PCA sobre las variables usadas y        ✓ Se recomienda realizar el PCA con el fin 
     explorar las cargas de los 2 primeros              de obtener las variables sintéticas y 
     componentes, identificar las variables             reducir el número de inputs con el fin de 
     más relevantes                                     mejorar el desempeño de los modelos de 
Aspectos a incluir                                       ML elegidos 
                                                    ✓ Dedicar un buen tiempo a la explicación 
 ✓ Notebook donde se detallen todos los                 de la metodología usada.
     pasos seguidos
                                                  Explicación del desafío
Ejemplo                                              ✓
 ✓ Feature Selection (Filter Method),                   ¡Click aquí!
          CLASE 55
       Datathon
    Entrenamiento y 
optimización de Modelos de 
Deberás entregar tu Proyecto Final. Entrenarás y optimizarás diversos 
   Machine Learning
modelos de machine learning para resolver una problemática específica, 
detectada en la instancia de entrega anterior. El objetivo es que puedas 
utilizar modelos de ML para resolver el problema de una industria o negocio
ENTREGA DEL PROYECTO 
FINAL
Entrenamiento y optimización de 
modelos de Machine Learning
Objetivos generales
✓ Utilizar modelos de Machine Learning para resolver un problema de una industria o 
negocio
Objetivos específicos
✓ Retomar el trabajo realizado en la segunda pre entrega, sumando el trabajo con 
Machine Learning 
✓ Modelar la situación como un problema de Machine Learning 
✓ Entrenar modelos de Machine Learning 
✓ Realizar ingeniería de atributos y normalización/estandarización de variables
✓ Seleccionar el modelo con mejor performance
       ENTREGA DEL PROYECTO 
       FINAL
 Entrenamiento y optimización de 
 modelos de Machine Learning
 Requisitos base
   ✓ Un Notebook (Colab o Jupyter) que debe contener:
   1.  Abstracto con motivación y audiencia:  Descripción de alto nivel de lo que motiva a analizar los datos 
       elegidos y audiencia que se podría beneficiar de este análisis.
   2.  Preguntas/Problema que buscamos resolver: Si bien puede haber más de una problemática a resolver, la 
       problemática principal debe encuadrarse como un problema de clasificación o regresión.
   3.  Breve Análisis Exploratorio de Datos (EDA): Análisis descriptivo de los datos mediante visualizaciones y 
       herramientas estadísticas, análisis de valores faltantes.
   4.  Ingeniería de atributos: Creación de nuevas variables, transformación de variables existentes (i.e 
       normalización de variables, encoding, etc.)
   5.  Entrenamiento y Testeo: Entrenamiento y testeo de al menos 2 modelos distintos de Machine Learning 
       utilizando algún método de validación cruzada.
   6.  Optimización: Utilizar alguna técnica de optimización de hiperparámetros (e.g gridsearch, randomizedsearch, 
       etc.)
   7.  Selección de modelos: utilizar las métricas apropiadas para la selección del mejor modelo (e.g AUC, MSE, etc.)
   ENTREGA DEL PROYECTO 
   FINAL
Entrenamiento y optimización de 
modelos de Machine Learning
Piezas sugeridas
✓ Librerías:
numpy - pandas - matplotlib - sklearn - xgboost - shap
✓ Claridad de código:
Estructura - Markdown - Comentarios
Modelo de Proyecto final
✓ Ensemble Models
Explicación del desafío
✓ ¡Click aquí!
Para finalizar…
Llegar hasta aquí se traduce en esfuerzo, sudor, 
dedicación, trabajo, lágrimas, coraje y más, así que 
FELICITACIONES por haber dado el 100% de ti en 
toda la cursada.
Esto no termina acá porque recuerden que cuentan 
con 20 días corridos para la entrega del  
Proyecto Final, el cual estamos seguros de que 
quedará impecable.
Celebramos que ya estén coderizados y que vayan a 
donde vayan, ¡tendrán éxito! Que sigan alcanzando 
todo lo que se propongan ������
Esta clase va a ser
grabad
  a
      Clase 46. DATA SCIENCE
Selección del algoritmo 
 y Entrenamiento del 
        Modelo II
Temario
               45                      46                     47
          Selección de            Selección de           Validación de 
           Algoritmo y             Algoritmo y         modelos - Métricas
       entrenamiento del        entrenamiento del 
            Modelo I                Modelo II
        ✓ Matriz de confusión   ✓ Métricas para 
        ✓ Accuracy                  algoritmos de 
                                    regresión           ✓ Análisis de 
        ✓ Precisión                                        Clustering
                                ✓ RMSE
        ✓ Sensibilidad                                  ✓ Métricas para la 
                                ✓ MAE                      calidad de 
        ✓ Especificidad                                    Clustering
        ✓ F1 Score y curva ROC  ✓ R2
Objetivos de la clase
         Evaluar métricas de modelos de Regresión
  MAPA DE CONCEPTOS
                                                         MSE
                                                        RMSE
                                                         MAE
  Selección del            Métricas para 
  Algoritmo y              algoritmos de                MAPE
Entrenamiento del 
   Modelo II                regresión
                                                         RAE
                                                       R2 y R2 
                                                       ajustado
Métricas para 
algoritmos de 
Regresión
Para pensar
En clases pasadas hablamos de diferentes 
métricas para Algoritmos de regresión…
¿Cómo se podía medir el desempeño de 
algoritmos de Regresión? ¿Recuerdan la 
métrica R2? ¿Para qué servía y cómo se 
interpretaba?
Contesta en el chat de Zoom 
Métricas para algoritmos de 
regresión
•  Para los modelos de Regresión no podemos usar las 
   mismas métricas que para algoritmos de 
   Clasificación 
•  El performance de los modelos de regresión se 
   cuantifica por medidas de error respecto a las 
   predicciones y los valores reales 
•  Existen tres métricas principales para evaluar el 
   desempeño y son: i) error cuadrático medio (MSE), 
   ii) raiz de error cuadrático medio (RMSE) y iii) error 
   absoluto medio (MAE)
Métricas para algoritmos de 
regresión
•  Sin embargo existe una amplia gama de métricas 
   que podemos utilizar como:
 a) Mean Absolute Error (MAE)
 b) Mean Squared Error (MSE)
 c) Mean Squared Log Error 
 d) Median Absolute Error
 e) Mean Absolute Percentage Error (MAPE)
 f) r2 score (r2)
Entre muchas otras más, si quieren conocerlas les 
dejamos el link: Scikit Learn Regression metrics
  MSE
MSE (Mean Square Error)
Error  cuadrático  medio  (MSE), 
calcula  el  promedio  del  cuadrado       2   import numpy as np
                                           3   def mse(actual, predicted):
de  los  errores  entre  los  valores              return np.mean(np.square(actual-
reales y predichos.                            predicted))
Cuanto menor sea el valor, mejor 
será el modelo de regresión.
Aquí yi denota la puntuación 
verdadera para el i-ésimo punto de 
datos, ŷi indica el valor predicho y 
n es el número de puntos de datos
MSE (Mean Square Error)
Sin embargo no todo es perfecto a 
la hora de utilizar esta métrica, el 
problema con MSE es que, dado 
que los valores se elevan al 
cuadrado, se cambia la unidad de 
medida y la interpretación no es 
tan sencilla a la luz de los datos.
Para superar este problema, 
utilizamos la métrica raíz del 
error cuadrático medio (RMSE, 
por sus siglas en inglés)
  RMSE
RMSE (Root Mean Square Error)
RMSE es la métrica más popular para 
medir  el  error  de  un  modelo  de     2   def rmse(actual, predicted):
regresión.                               3       return np.sqrt(np.mean(np.square(actual-
                                             predicted)))
Raíz   cuadrada    de   la   distancia 
cuadrática promedio entre los valores 
reales y predichos.
Al sacar la raíz cuadrada se revierte la 
unidad de medida a su escala original.
Puede usarse para comparar modelos 
sólo cuyos errores se miden en las 
mismas unidades.
  MAE
MAE (Mean Absolute Error)
El MAE se calcula como la media  2        def mae(actual, predicted):
                                     3       return np.mean(np.abs(actual-predicted))
de la diferencia absoluta entre los 
valores reales y predichos.
Donde ŷi es el valor predicho de 
la i-ésima muestra, yi es el valor 
real  correspondiente  y  n  es  el 
número de muestras.
MAE (Mean Absolute Error)
Tanto RMSE como MAE                 2  def mae(actual, predicted):
dependen de la escala y pueden      3      return np.mean(np.abs(actual-predicted))
usarse para comparar modelos 
solo si se miden en las mismas 
unidades.
Para comparar modelos con 
diferentes unidades, podemos 
utilizar métricas como MAPE u 
otras como el RAE
  MAPE
MAPE (Mean Absolute Percentage 
Error)
MAPE mide el error en términos de 
porcentaje.                                  2   def mape(actual, predicted):
                                             3       return np.mean(np.abs((actual - predicted) / 
                                                 actual)) * 100
El  MAPE  se  calcula  dividiendo  la 
diferencia absoluta entre los valores 
reales  y  pronosticados  en  cada 
observación.
Se    multiplica     por    100     para 
convertirlo  en  un  porcentaje  de 
error.
MAPE (Mean Absolute Percentage 
Error)
                                         2   def mape(actual, predicted):
A pesar de ser una métrica que nos        3       return np.mean(np.abs((actual - predicted) / 
                                             actual)) * 100
permite cuantificar el error en 
términos porcentuales tiene un 
inconveniente, que radica en que se 
pueden producir valores infinitos o 
indefinidos (e.g división por cero) 
para valores reales como el cero o 
cercanos a cero.
 RAE
RAE (Relative Absolute Error)
El RAE se define como la relación  2      def rae(actual, predicted):
entre    la   suma  de  errores  3            numerator = np.sum(np.abs(predicted - actual))
                                             denominator = np.sum(np.abs(np.mean(actual) - 
absolutos     y   la    suma     de       actual))
                                             return numerator / denominator
desviaciones absolutas.
En la fórmula dada el valor de pi 
se refiere a los valores predichos, 
mientras que ai es el valor real y 
a_bar es la media de los valores 
reales.
RAE (Relative Absolute Error)
                                             2   def rae(actual, predicted):
                                             3       numerator = np.sum(np.abs(predicted - actual))
Esta es una métrica que tiene varias                 denominator = np.sum(np.abs(np.mean(actual) - 
                                                 actual))
ventajas y se usa para muchas                        return numerator / denominator
aplicaciones científicas. 
Dentro de una de sus ventajas como 
es una sumatoria en numerador y 
denominador no tiene problemas de 
división por 0.
 R2
R2
También       conocido      como 
coeficiente  de  determinación 
(R2), es una de las métricas de 
evaluación      de       regresión 
comúnmente utilizadas.
Mide la proporción de varianza de  2    def r_squared(actual, predicted):
                                   3       sse = np.sum(np.square(actual-predicted))
la variable dependiente explicada           sst = np.sum(np.square(actual-
                                       np.mean(actual)))
por      la      variable      (s)          return 1 - (sse/sst)
independiente(s).
R2
Esta métrica tiene un rango entre 0 a 
1,  donde  0  indica  que  el  ajuste  es 
deficiente y 1 ajuste lineal perfecto.
Sin embargo, el problema con el R2 es 
que el valor aumenta falsamente a 
medida que se agregan más variables 
independientes en los modelos.
El R2 no SIEMPRE se puede usar 
para tener una comparación 
significativa entre modelos.
R2 ajustado
R2 ajustado
Para contrarrestar el problema al que 
se  enfrenta  el  R2,  el  R2_ajustado 
penaliza la adición de más variables 
independientes  que  no  aumentan  el 
poder  explicativo  del  modelo  de 
                                        2   def adj_r_squared(X, actual, predicted):#X data 
regresión.                               3   train
                                                r_squ = r_squared(actual, predicted)
                                                numerator = 1 - (1 - r_squ) * len(actual)-1
El  valor  de  R2_ajustado es siempre            denominator = len(actual) - X.shape[1]-1 
menor o igual al valor de R2 donde:          #X.shape[1] da el número de variables 
                                            independientes
                                                return numerator/denominator
n = el tamaño de la muestra
k   =    el   número  de  variables 
independientes
Med AE
Median Absolute Error (Med AE)
Calculado  como  la  mediana  de 
todas  las  diferencias  absolutas 
entre    los    valores    reales    y 
predichos.
                                          2   def median_abs_error(actual, predicted):
Donde yi es el valor real y ŷ es el        3       return np.sum(np.median(np.abs(actual - 
                                              predicted)))
valor predicho.
Le métrica Med AE es bastante 
robusta  (resistente  a  valores 
atípicos)
Ventajas y 
desventajas 
métricas regresión
Ventajas y desventajas métricas 
regresión
      Métrica                   Ventajas                              Desventajas
        MSE        Permite garantizar que modelos       Si nuestro modelo hace una sola 
                   entrenados no tenga predicciones     predicción muy mala, la métrica magnifica 
                   atípicas con grandes errores         el error.
       RMSE        Resuelve los problemas del MSE       Solo se puede usar para comparar 
                   dando una métrica con las mismas     modelos que tengan las mismas unidades 
                   unidades que la variable de interés  de medida porque de lo contrario no tiene 
                                                        sentido
        MAE        Dado que se toma el valor absoluto,  Pondera de la misma forma grandes y 
                   todos los errores se ponderarán en   pequeños errores lo cual genera 
                   la misma escala lineal.              inconsistencias
       MAPE        Es una métrica que utiliza un error  Se pierden las unidades de media de la 
                   porcentual (adimensional) para       variable objetivo que en algunos casos es 
                   cuantificar el error                 necesario para evaluar modelos
Ventajas y desventajas métricas 
regresión
      Métrica                     Ventajas                               Desventajas
        RAE         Muestra la magnitud del error en       No es tan conveniente cuando existe poca 
                    términos relativos                     variabilidad en los errores
         R2         Mide la proporción de varianza de la  Aumenta falsamente a medida que se 
                    variable dependiente explicada por  agregan más variables independientes en 
                    la variable independiente.             modelos lineales
    R2 ajustado     Contrarresta el problemas principal  Por propiedades matemáticas de 
                    del  R2  penalizando  la  adición  de  construcción al igual que el R2 nunca 
                    más variables independientes           decrece sino que siempre aumenta
      Med AE        Es una métrica robusta lo cual la      Cuantifica el error solo en términos 
                    hace resistente a valores atípicos     medios por lo que no analiza de fondo el 
                    en los datos                           error global de todos las estimaciones
Resumen métricas 
Regresión
Resumen métricas regresión
1. Las métricas son vitales para cualquier 
modelo de aprendizaje automático.
2. El error cuadrático medio (MSE), que mide el 
error cuadrático medio de nuestras 
predicciones. Su problema es que, dado que los 
valores se elevan al cuadrado, la unidad de 
medida cambia.
3. Para llenar esta deficiencia, analizamos otra 
métrica llamada RMSE, que revierte el valor a su 
unidad de medida original tomando una raíz 
cuadrada.
Resumen métricas regresión
4. El MAPE y RAE, que se pueden usar para 
comparar dos modelos de diferentes escalas.
5. Entendieron por qué no se puede usar el R2 
para tener una comparación sensata entre dos 
modelos.
6. El R2_ajustado resuelve en parte el problema 
con el R2
7. La mediana de errores absolutos es resistente 
a valores atípicos.
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Ejemplo en vivo
Dentro de la carpeta de clase analizaremos 
cómo calcular diferentes métricas para 
algoritmos de Regresión con su respectiva 
interpretación.
Evaluando desempeño 
Modelos de Regresión
Utilizaremos lo aprendido en clase para poder evaluar 
 el desempeño de modelos de regresión
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Evaluando 
desempeño Modelos 
Trabajaremos con base en lo desarrollado en clases 
de Regresión
previas con los datos de: Medical Insurance
1. Identificar variables independientes/dependiente 
que sean de utilidad para el problema
2. Generar un algoritmo de regresión y calcular sus 
métricas
3. Escribir (en ppt, archivo txt, papel, etc) los pros y 
contras de cada una de las métricas utilizadas
4. Seleccionar la mejor métrica para evaluar los 
diferentes modelos elegidos
¿Preguntas?
CLASE N°46
Glosario
MSE: métrica de regresión que calcula el      RAE: métrica de regresión que calcula la 
promedio del cuadrado de los errores          relación entre la suma de errores absolutos 
                                             y la suma de desviaciones absolutas
entre los valores reales y predichos.
                                             R2: métrica de regresión que calcula la 
RMSE: métrica de regresión más popular        proporción de varianza de la variable 
que calcula la raíz cuadrada de la            dependiente explicada por la variable(s 
distancia cuadrática promedio entre los       independiente(s)
valores reales y predichos.
                                             R2 ajustado: métrica de regresión que 
MAPE: métrica de regresión que calcula        resuelve los problemas generados al 
la diferencia absoluta entre los valores      utilizar el R2 penalizando la adición de 
                                             muchas variables independientes  
reales y pronosticados en cada 
observación.
Opina y valora 
esta clase
                 Resumen 
           de la clase hoy
           ✓ MSE
           ✓ RMSE
           ✓ MAE
           ✓ MAPE
           ✓ RAE
           ✓ R2 y R2 ajustado
           ✓ Med AE
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 34. DATA SCIENCE
GIS y Datos Espaciales
Temario
                 33                       34                        35
          Visualizaciones                GIS y            Análisis univariado y 
         efectivas y Data                                        gráficos
            Storytelling          Datos espaciales
           ✓  Beneficios del       ✓ Análisis espacial       ✓ Tipos de 
              Storytelling y                                    análisis 
              componentes          ✓ Mapas y 
                                      animaciones de            estadísticos
           ✓  Visualizaciones de 
              datos                   Python                 ✓ Gráficos 
           ✓  Elevator Pitch       ✓ Uso de la librería         estadísticos
           ✓  Interfaz de usuario     Plotly
           ✓  Principios de usabilidad
Objetivos de la clase
         Definir el concepto de análisis espacial
         Acceder a mapas y animaciones en Python
         Utilizar la librería Plotly
MAPA DE CONCEPTOS
                         Análisis 
                         espacial
                      GIS y Datos              Mapas y 
Datos espaciales                              animaciones de 
                      Espaciales               Python
                       Uso de librería 
                          Plotly
Cuestionario de tarea
¿Te gustaría comprobar tus 
conocimientos de la clase anterior?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
Análisis espacial
Definición
¿Qué es el 
Análisis espacial?
Antes de hablar de Mapas, tenemos que 
entender de qué estamos hablando cuando 
hablamos de Análisis Espacial. Si lo pensamos 
en un sentido estricto, son un conjunto de 
herramientas que nos permiten analizar datos 
geográficos. 
Ahora   bien,   ¿Qué    son    los   datos 
geográficos?  Son  datos  que  de  manera 
implícita o explícita tienen una ubicación en la 
tierra.
Tipos de datos 
espaciales
Tipos de datos espaciales
Los datos espaciales pueden expresarse en dos modelos clásicos: 
                                      Modelo ráster.
Modelo vectorial.                  Se divide a un espacio en 
En este modelo los datos            grillas de igual tamaño. Cada 
espaciales son puntos,             una de estas unidades (a veces 
líneas o polígonos.                llamadas píxeles o celdas) 
                                    tienen uno o más valores 
                                        asociados. 
Referencia: mygisnotebook.blog 
Tipos de datos espaciales
                            Suele ser muy usado para 
                            el análisis de imágenes 
                            satelitales, también para 
                            mostrar la elevación de una 
                            superficie.
Archivos de datos 
espaciales
Archivos de datos 
espaciales
Tanto vectores como rasters tienen 
tipos de archivos distintos.                    ✔ Geojson:  Son  listas  basadas  en  el 
A  continuación,  les  compartimos                  standard JSON. Por definición desde 
los  archivos  más  comunes  para                   ya  hace  unos  años  es  obligatorio 
vectores:                                           usar la proyección WGS84.
✔ Shapefile: De los formatos más                ✔ KLM: Son etiquetas basadas en XML 
   ampliamente  difundidos.  Están                 Modelo ráster. 
   compuestos  por  más  de  un 
   archivo y es propiedad de ESRI 
   (creadores de ArcGis).
Archivos de datos 
espaciales
Tanto vectores como rasters tienen tipos de archivos distintos: 
Archivos más comunes para rasters: 
✔ GeoTIFF
✔ JPEG(2000)
Sistemas de 
coordenadas de 
referencia
Sistemas de referencia
  Como mencionamos, todos los 
  datos     espaciales     tienen 
  implícita o explícitamente una 
  ubicación  en  la  tierra  y  para 
  trabajar   con    datos   geo-
  referenciados,  tenemos  que 
  saber  en  qué  unidad  de 
  medida      se      encuentran 
  expresados.
Sistemas de referencia
PARA RECORDAR
“Sigue nos dice que algo peso 10 no podemos 
saber si son gramos, kilos o toneladas. Esto 
significa que un conjunto de coordenadas por sí 
solas, no alcanzan para saber de qué posición en la 
tierra estamos hablando”
Por tanto, los Sistemas de Coordenada de 
referencia (CRS) identifican la “unidad” en la que 
están expresados nuestros datos espaciales
Sistemas de referencia
           Hay dos diferentes tipos de sistemas de referencia de coordenadas:
         Sistema de Coordenadas                   Sistema de Coordenadas 
               Geográfica                               Proyectada
Sistemas de referencia
Utilizan los grados de latitud y 
longitud y en ocasiones un valor 
de altitud para definir la situación 
de un punto sobre la superficie 
terrestre. El sistema más popular 
se denomina WGS 84 (World 
Geodetic System).
Sistemas de referencia
Se define sobre una superficie 
plana  de  dos  dimensiones.  A 
diferencia  de  un sistema  de 
coordenadas  geográficas,  un 
sistema     de    coordenadas 
proyectadas posee longitudes, 
ángulos y áreas constantes en 
las dos dimensiones.
Sistemas de referencia
El mundo del Análisis de Datos geoespaciales es 
un tema muy apasionante y también complejo, 
pero al final de la clase, en la diapositiva de 
material ampliado, te compartimos algunos links 
de interés para que puedas seguir aprendiendo 
aún más.
Mapas y 
animaciones en 
Python
Mapas en Python
Mapas en Python
Ahora que ya tenemos un 
entendimiento común acerca del 
análisis espacial, hablemos 
exclusivamente de cuáles son las 
principales librerías que existen en 
Python para la creación y 
representación de mapas como así 
también, para la manipulación de 
datos geoespaciales.
       
Mapas en Python
 Resulta importante mencionar, que 
 existen     múltiples      paquetes 
 disponibles  en  Python,  de  los 
 cuales podemos destacar…
 Geopandas:  proyecto  de  código 
 abierto para facilitar el trabajo con 
 datos  geoespaciales  en  Python. 
 GeoPandas  extiende  los  tipos  de 
 datos  utilizados  por  los  pandas 
 para      permitir      operaciones 
 espaciales en tipos geométricos.  
Mapas en Python
GDAL/OGR:  Librería  fundamental 
para  procesar  formatos  de  datos 
vectoriales y ráster.
GeoPy: Facilita         a        los 
desarrolladores de Python localizar 
las  coordenadas  de  direcciones, 
ciudades,  países  y  puntos  de 
referencia  en  todo  el  mundo 
utilizando   geocodificadores     de 
terceros y otras fuentes de datos.
Mapas en Python
Cartopy:  Esta  librería  permite 
dibujar  mapas  para  que  el 
análisis  y  la  visualización  de 
datos  sea  lo  más  intuitivo 
posible.
Mapas en Python
 Rasterio:  Los  SIG  utilizan  el 
 formato  GeoTIFF,  entre  otros, 
 para   organizar   y   almacenar 
 conjuntos  de  datos  ráster  como 
 imágenes satelitales. Rasterio lee 
 y   escribe  estos  formatos  y 
 proporciona  una  API  de  Python 
 basada  en  matrices  Numpy  N-
 dimensional y GeoJSON.
Animaciones en 
Python
Animaciones en Python
  Para    crear   nuestras     animaciones 
  usaremos  la  función  FuncAnimation 
  dentro   de  matplolib.  Un  aspecto 
  fundamental  para  poder  crear  nuestras 
  animaciones  es  entender  que  este 
  paquete no crea animaciones, sino que 
  simplemente     se    limita   a    crear 
  animaciones  a  partir  de  una  serie  de 
  gráficos que le pasemos.
Animaciones en Python
Pasos importantes
✔ Crear un background: es el marco 
de fondo que sirve de referencia para 
los gráficos.
✔ Crear una función: que permita 
graficar a cada paso de tiempo y frame
✔ Crear el objeto animación: la clase 
animacion incluye la función 
FuncAnimation que nos permite crear 
la animación
Animaciones en Python
Parámetros importantes de la función 
FuncAnimation
✔ fig: es el objeto que utilizaremos para pintar 
nuestro gráfico.
✔ func: es una función que debe de devolver el 
estado de la animación para cada frame. 
Básicamente lo que debemos hacer es crear 
una función que devuelva todos los gráficos. 
Animaciones en Python
Parámetros importantes de la función Funcanimation
✔ interval: es el delay en milisegundos entre los 
diferentes frames de la animación.
✔ frames: número de imágenes en las que se va a 
basar el gráfico. Esto dependerá de cuántos 
“estados” tenga la animación. Si tenemos una 
animación con datos en 5 estados diferentes 
(imaginemos, 5 años), el número de frames será 
5
Ejemplo en vivo
Exploremos cómo hacer uso de la función 
FuncAnimation de matplotlib. Aprenderemos a 
desarrollar animaciones para series de tiempo, 
barplots y scatterplots, para esto utilizaremos 
el notebook dentro de la carpeta de clase.
Tiempo estimado: 15-20 minutos
         ☕
       Break
     ¡10 minutos y 
     volvemos!
 Actividad colaborativa
Graficando datos espaciales de 
Properati
En esta oportunidad trabajaremos con los datos de Properati de 
2015 disponibles en el siguiente enlace: Data Properati
Se sugiere trabajar con la librería Folium y utilizar heatmaps para 
proyectar el procesamiento de los datos
Duración: 15-20 minutos
Uso de la librería 
Plotly
Definición de Plotly
Plotly
La biblioteca plotly de Python es una biblioteca 
de gráficos interactiva de código abierto que 
admite más de 40 tipos de gráficos únicos que 
cubren una amplia gama de casos de uso 
estadísticos, financieros, geográficos, 
científicos y tridimensionales.
Construido sobre la biblioteca Plotly JavaScript 
(plotly.js), plotly permite a los usuarios de 
Python crear hermosas visualizaciones 
interactivas basadas en la web que se pueden 
mostrar en cuadernos Jupyter
Nociones 
elementales
Nociones básicas
# Importar librerias
import pandas as pd
import numpy as np
import chart_studio.plotly as py
import seaborn as sns
import plotly.express as px
arr= np.random.randn(50,4)
df= pd.DataFrame(arr, columns=['A','B','C','D'])
df.head()
df.plot()                         Nada mal para una primera aproximación, 
                                 sin embargo este gráfico no es dinámico.
Nociones básicas
import plotly.graph_objects as go
# Informacion de datos disponibles: 
https://plotly.com/python-api-reference/generated/plotly.data.html
# Precio de stocks de ['date', 'GOOG', 'AAPL', 'AMZN', 'FB', 'NFLX', 'MSFT']
# para 2018/2019
df_stocks = px.data.stocks()
px.line(df_stocks, x='date',y='GOOG', labels={'x':'Fecha', 'y':'Precio'})
                                           Plotly nos ofrece la 
                                           oportunidad de poder 
                                           interactuar de manera 
                                           dinámica con la gráfic, entre 
                                           otras opciones.
Nociones básicas
px.line(df_stocks, x='date', y=['GOOG','AAPL'],labels={'x':'Date','y':'Price'},
     title='Apple vs Google')
En esta oportunidad podemos crear múltiples gráfico en un mismo frame. Adicionalmente 
podemos generar zoom, recortar, escalar, y guardar la imagen a nuestro equipo.
Nociones básicas
fig = go.Figure()
fig.add_trace(go.Scatter(x=df_stocks.date, y=df_stocks.AAPL,mode='lines', name='Apple'))
fig.add_trace(go.Scatter(x=df_stocks.date, y=df_stocks.AMZN,mode='lines+markers', name='Amazon'))
fig.add_trace(go.Scatter(x=df_stocks.date, y=df_stocks.GOOG,mode='lines+markers', name='Google',
                      line=dict(color='firebrick',width=2,dash='dashdot')))
fig.update_layout(title='Precio de acciones 2018/19',xaxis_title='Precio',yaxis_title='Fecha')
                                          Podemos generar 
                                          diferentes marcadores en 
                                          una serie de tiempo para 
                                          comparación.
Nociones básicas
fig = go.Figure()
fig.add_trace(go.Scatter(x=df_stocks.date, y=df_stocks.AAPL,mode='lines', name='Apple'))
fig.add_trace(go.Scatter(x=df_stocks.date, y=df_stocks.AMZN,mode='lines+markers', name='Amazon'))
fig.add_trace(go.Scatter(x=df_stocks.date, y=df_stocks.GOOG,mode='lines+markers', name='Google',
                      line=dict(color='firebrick',width=2,dash='dashdot')))
fig.update_layout(
 xaxis=dict(showline=True, showgrid=False, showticklabels=True,
            linecolor='rgb(204,204,204)', linewidth=2, ticks='outside',
            tickfont=dict(family='Arial', size=12, color='rgb(82,82,82)')
            ),
            yaxis=dict(showgrid=False, zeroline=False, showline=False,
                       showticklabels=False                  
            ),
            autosize=False,
            margin=dict(autoexpand=False,l=100, r=20, t=110),
                        showlegend=True, plot_bgcolor='white'
     )
Nociones básicas
                                     Podemos modificar 
                                     formatos en los ejes x, y, 
                                     así como también tamaño 
                                     y tipo de letra, ticks en los 
                                     ejes y demás atributos 
                                     gráficos.
Barplots
Barplots
df_us= px.data.gapminder().query("country=='United States'")
px.bar(df_us, x='year', y='pop')
Barplots
df_tips= px.data.tips()
px.bar(df_tips, x='day',y='tip',color='sex',
   title='Propinas por genero en cada dia',labels={'tip':'Propina', 'day':'Dia de 
trabajo'})
Barplots
df_europe= px.data.gapminder().query("continent == 'Europe' and year == 2007 and pop > 
2.e6")
fig=px.bar(df_europe, y='pop', x='country', text='pop', color='country')
fig.update_traces(texttemplate= '%{text:.2s}', textposition='outside')
fig.update_layout(uniformtext_minsize=8)
fig.update_layout(xaxis_tickangle= -45)
fig
Scatterplots
Scatterplots
df_iris= px.data.iris()
px.scatter(df_iris, x='sepal_width', y='sepal_length',color='species', 
size='petal_length',
       hover_data=['petal_width'])
Scatterplots
fig= go.Figure()
fig.add_trace(go.Scatter(
x=df_iris.sepal_width, y= df_iris.sepal_length, mode='markers',
marker_color=df_iris.sepal_width, text=df_iris.species, marker=dict(showscale=True)
))
fig.update_traces(marker_line_width= 2, marker_size=10)
Piecharts
Piecharts
df_america= px.data.gapminder().query("year==2007").query("continent= ='Americas' 
and (country in 
('Colombia','Uruguay','Argentina','Peru','Chile','Brazil','Ecuador','Venezuela','Par
aguay','Bolivia'))")
px.pie(df_america, values='pop', names='country', title='Poblacion America',
   color_discrete_sequence= px.colors.sequential.RdBu)
Histogramas
Histogramas
df_tips= px.data.tips()
px.histogram(df_tips, x='total_bill',color='sex')
Boxplots
Boxplots
px.box(df_tips, x='day',y='tip',color='sex')
Boxplots
df_stocks= px.data.stocks()
fig= go.Figure()
fig.add_trace(go.Box(y=df_stocks.GOOG, 
boxpoints='all',fillcolor='blue',jitter=0.5,
                  whiskerwidth=0.2))
fig.add_trace(go.Box(y=df_stocks.AAPL, 
boxpoints='all',fillcolor='red',jitter=0.5,
                  whiskerwidth=0.2))
fig.update_layout(title='Google vs 
Apple',yaxis=dict(gridcolor='rgb(255,255,255)',
gridwidth= 3),paper_bgcolor='rgb(243, 243, 
243)',plot_bgcolor='rgb(243, 243, 243)')
Heatmaps
Heatmaps
f= sns.load_dataset('flights')
fig= px.density_heatmap(f, x='year', y='month',z= 'passengers',
                    marginal_x= 'histogram',
                    marginal_y='histogram')
fig
   Actividad colaborativa
Comparando el GPD de diferentes 
países
En esta oportunidad elegiremos el gráfico más apropiado para 
representar el GPD (Gross Domestic Product) de todos los países 
disponibles en el dataset. Los datos se encuentran disponibles en el 
siguiente enlace: Gapminder Dataset
Sugerencia: Utilizar la librería Plotly
Duración: 10-15 minutos
CLASE N°34
Glosario
Análisis espacial: son un conjunto de            Animaciones Python: Son una sucesión 
                                                de imágenes a lo largo del tiempo, deben 
herramientas que nos permiten                    contar con un background, una función y 
analizar datos geográficos                       un objeto que permita la animación.
Tipos de datos espaciales: Pueden ser            Plotly: biblioteca de gráficos interactiva de 
de dos tipos:  vectorial (representados por      código abierto que admite más de 40 tipos 
líneas o polígonos) o tipo raster donde se       de gráficos únicos que cubren una amplia 
divide los pixeles de igual tamaño.              gama de casos de uso estadísticos, 
                                                financieros, geográficos, científicos y 
Sistemas de referencia: Pueden ser de            tridimensionales
dos tipos: sistema de coordenadas 
geográficas o sistema de coordenadas 
proyectadas
¿Preguntas?
¿Aún quieres conocer 
  más?
Te recomendamos el 
siguiente material
 MATERIAL AMPLIADO
Recursos multimedia
 Introducción a GIS
  ✓ Una introducción fácil a GIS | GIS | GIS
 Sistemas de coordenadas
  ✓ Sistemas de coordenadas, proyecciones y transformaciones | 
     ProGIS | ProGIS
 GeoTIFF
  ✓ Uso de GeoTIFF | EarthData | GeoTIFF
 JPEG2000
  ✓ JPEG2000 | JPEG | JPEG2000
Disponible en nuestro repositorio.
Opina y valora 
esta clase
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Definir el concepto de Análisis espacial
      ✓ Acceder a mapas y animaciones en Python
      ✓ Utilizar la librería Plotly
Esta clase va a ser
grabad
  a
      Clase 54. DATA SCIENCE
   Introducción al 
  procesamiento de 
 Lenguaje Natural II
Temario
                53                      54                       55
         Introducción al          Introducción al             Datathon
        Procesamiento de         Procesamiento de 
        Lenguaje Natural I      Lenguaje Natural II
        ✓ Procesamiento del 
           lenguaje natural
        ✓ Expresiones regulares  ✓ Introducción a         ✓ Introducción
        ✓ Bag of words              spaCY
                                                          ✓ Proyecto final
        ✓ NLTK                   ✓ Análisis de 
        ✓ Corpus, Document team,    sentimiento 
           Matrix y Term Document 
           Matrix
Objetivos de la clase
         Introducir diferentes librerías para el 
         procesamiento de texto
         Identificar beneficios de usar técnicas de 
         análisis de sentimiento
   MAPA DE CONCEPTOS
                                                     NLP y características
                                                       Bag of Words
                                                      Intro NLTK, DTM y 
                                                        TF-IDF score
         Introducción al 
       procesamiento de                              Otras técnicas NLP
        Lenguaje Natural
                                                                         Clase 
                                                                         54
                                                       Intro a spacy
                                                        Análisis de 
                                                       sentimiento
Introducción a spaCy
spaCY
spaCY
spaCy es una librería gratuita y de código abierto para NLP en Python con muchas 
capacidades integradas que se está volviendo cada vez más popular. Es una alternativa 
para procesar los datos textuales no estructurados que se producen a gran escala
spaCY
Está escrito en Cython y está diseñado para construir sistemas de extracción de 
información o comprensión del lenguaje natural. También está diseñado para uso en 
producción y proporciona una API concisa y fácil de usar.
Procesamiento con 
spaCY
1) Creación de modelo de 
procesamiento
                                                      import re
                                                      import string
Con spaCY es posible la creación de modelos pre       !python -m spacy download 
entrenados para diversos lenguajes (e.g español,      es_core_news_md
inglés, italiano, francés, entre otros)               import spacy
                                                      import es_core_news_md
El modelo de español tiene cerca de 500000 keys       nlp = es_core_news_md.load()
con 20000 vectores únicos (300 dimensiones) con 
componentes como: parser, tok2vec, 
morphologizer, senter, lemmatizer entre otros.
2) Detección de oraciones
                                                          text=('Gus es un desarrollador 
                                                          en Python actualmente 
 La detección de oraciones es el proceso de               trabajando para una compañía 
 localizar el comienzo y el final de las oraciones en     Fintech en Londres Inglaterra. 
 un texto determinado. Esto le permite dividir un         Se encuentra interesado en 
                                                          aprender NLP.')
 texto en unidades lingüísticamente significativas.       t=nlp(text)
 Utilizará estas unidades cuando esté procesando          oraciones= list(t.sents)
 su texto para realizar tareas como el etiquetado         print(len(oraciones))
                                                          for x in oraciones:
 de parte del discurso y la extracción de                   print(x)
 entidades.
3) Tokenización
                                                       text=('Gus es un desarrollador 
                                                       en Python actualmente 
 La tokenización es el siguiente paso después de la    trabajando para una compañia 
 detección de oraciones. Permite identificar las       Fintech en Londres Inglaterra. 
 unidades básicas en el texto. Estas unidades          Se encuentra interesado en 
                                                       aprender NLP.')
 básicas se denominan tokens. La tokenización es       t=nlp(text)
 útil porque divide un texto en unidades               for token in t:
 significativas. Estas unidades se utilizan para         print(token, token.idx)
 análisis posteriores, como parte del etiquetado de 
 voz.
3) Tokenización
                                                      for token in t:
 Además es posible obtener diferentes atributos de      print(token, token.idx, 
 los tokens generados en el proceso como:             token.text_with_ws,
                                                              token.is_alpha, 
  1. is_alpha: detecta si es alfanumérico             token.is_punct, token.is_space,
  2. is_punct: detecta si es un signo de                      token.shape_, 
                                                      token.is_stop)
      puntuación
  3. is_space: detecta si es un espacio
  4. shape_: detecta el shape
  5. is_stop: verifica si es stopword
4) Uso de Stopwords
                                                          import spacy
                                                          spacy_stopwords = 
 Las stopwords son las palabras más comunes en            spacy.lang.es.stop_words.STOP_W
 cualquier idioma. En el idioma español, algunos          ORDS
 ejemplos de palabras vacías son el, ellos, o, tú         print(len(spacy_stopwords))
                                                          for stop_word in 
 entre otras. La mayoría de las oraciones deben           list(spacy_stopwords)[:10]:
 contener stopwords para que sean oraciones                 print(stop_word)
 completas que tengan sentido. Sin embargo las 
                                                          for token in t:
 podemos remover y encontrar por medio de                   if not token.is_stop:
 spaCY                                                        print(token)
5) Lematización
                                                   import spacy
Recordemos que la lematización es el proceso       spacy_stopwords = 
de reducir las formas flexionadas de una palabra   spacy.lang.es.stop_words.STOP_W
y, al mismo tiempo, garantizar que la forma        ORDS
reducida pertenezca al idioma. Esta forma          for token in t:
reducida o raíz de la palabra se llama lema y        print(token, '-', 
permite comprender de manera esencial el           token.lemma_)
sentido de cada palabra
6) Word Frequency
                                                      doc= nlp(texto1)
 Esta librería también nos permite encontrar la       # Remover stopwrods
 frecuencia de palabras dentro de un corpus           words= [token.text for token in doc if 
                                                      not token.is_stop and not token.is_punct]
 determinado. Para esto podemos utilizar la           from collections import Counter
 función Counter de la librería Collections y         word_freq= Counter(words)
                                                      # Sacar las 5 mas frecuentes 
 después del paso a través del procesador de texto  common_words= word_freq.most_common(5)
 (nlp) permite encontrar las palabras más             print(common_words)
                                                      unique_words = [word for (word, freq) in 
 frecuentes dentro de cualquier texto                 word_freq.items() if freq == 1]
                                                      print(unique_words)
7) POS Tagging for token in doc:
                                                        print(token,' -', token.tag_, ' 
 Parte del discurso o POS es un rol gramatical que    -', token.pos_,' 
                                                      -' ,spacy.explain(token.tag_))
 explica cómo se usa una palabra en particular en 
 una oración. Hay ocho partes del discurso:           nouns=[]
 sustantivos,pronombres, adjetivos, verbos,           adjectives=[]
 adverbios, preposiciones, conjunciones,              for token in doc:
                                                        if token.pos_ =='NOUN':
 interjections                                            nouns.append(token)
                                                        if token.pos_ =='ADJ':
 Consiste en asignar una etiqueta POS a cada              adjectives.append(token)
 token según su uso en la oración.                    print(nouns)
                                                      print(adjectives)
Hands on Lab
“El se encuentra interesado en 
aprender lenguaje Natural”. 
Con la frase de arriba, utilizaremos la 
función displacy.render para generar una 
visualización de conexiones entre 
palabras. Podemos usar el siguiente 
ejemplo como base.
Cualquier inquietud pueden consultar 
a su tutor o profesor
10-15 min
Solución
                                                    from spacy import displacy
                                                    texto = ('el se encuentra 
 spaCy viene con un visualizador incorporado        interesado en aprender 
 llamado displaCy. Pueden usarlo para visualizar    Procesamiento de Lenguaje 
 un análisis de dependencia o entidades con         Natural')
 nombre en un navegador o en un Jupyter             t = nlp(texto)
                                                    displacy.render(t, 
 Notebook.                                          style='dep',jupyter=True)
Para pensar
Ahora bien, ¿Han escuchado previamente de 
librerías diferentes a spaCy y NLTK para el 
procesamiento de lenguaje natural?
Contesta en el chat de Zoom 
Análisis de 
sentimiento
Definición
Análisis de sentimiento
El análisis de sentimientos se enfoca en la polaridad 
de un texto (positivo, negativo, neutral) pero también 
va más allá de la polaridad para detectar sentimientos 
y emociones específicas (enojado, feliz, triste, etc.), 
urgencia (urgente, no urgente) e incluso intenciones 
(interesado). v. no interesado).
Análisis de sentimiento
Dependiendo de cómo desee interpretar los 
comentarios y las consultas de clientes, puede definir 
y adaptar sus categorías para satisfacer sus 
necesidades de análisis de opiniones. Los tipos de 
análisis más comunes son:
1. Análisis de sentimiento calificado
2. Detección de emociones
3. Aspect-based Sentiment Analysis
4. Multilingual sentiment analysis
Clasificación
Análisis de sentimiento 
calificado
Si la polaridad es importante para el negocio se 
podrían tener diversas categorías (muy positivo, 
positivo, neutro, negativo y muy negativo). 
De esta forma se pueden detectar cuándo comentarios 
son negativos y poder establecer estrategias para 
mitigar impactos o deserción de clientes
Detección de emociones
Permite ir más allá de la polaridad para detectar 
emociones, como felicidad, frustración, ira y tristeza.
Muchos sistemas de detección de emociones utilizan 
léxicos (es decir, listas de palabras y las emociones 
que transmiten) o algoritmos complejos de 
aprendizaje automático.
Aspect-based sentiment 
Analysis
Por lo general, al analizar los sentimientos de los 
textos, queremos saber qué aspectos o 
características particulares mencionan las personas 
de manera positiva, neutral o negativa.
Un clasificador basado en aspectos podría determinar 
que la oración expresa una opinión negativa sobre la 
duración de la batería.
Multilingual sentiment 
analysis
El análisis del sentimiento multilingüe puede ser 
difícil. Implica una gran cantidad de 
preprocesamiento y recursos. La mayoría de estos 
recursos están disponibles en línea (p. ej., léxicos de 
sentimientos), mientras que otros deben crearse (p. 
ej., corpus traducidos o algoritmos de detección de 
ruido), pero necesitará saber codificar para usarlos.
Importancia
Importancia del análisis de 
sentimiento
Dado que los humanos expresan sus 
pensamientos y sentimientos más abiertamente 
que nunca, el análisis de sentimientos se está 
convirtiendo rápidamente en una herramienta 
esencial para monitorear y comprender los 
sentimientos en todo tipo de datos.
Importancia del análisis de 
sentimiento
El análisis automático de los comentarios de los 
clientes, como las opiniones en las respuestas de 
las encuestas y las conversaciones en las redes 
sociales, permite a las marcas saber qué hace 
felices o frustrados a los clientes, de modo que 
puedan adaptar los productos y servicios para 
satisfacer las necesidades de sus clientes.
Importancia del análisis de 
sentimiento
Algunos beneficios que genera la implementación 
del análisis de sentimiento son:
1. Identificación de necesidades
2. Ordenamiento de data a escala
3. Análisis en tiempo real
4. Criterios consistentes
Ejemplos de 
aplicación
Ejemplos de análisis de 
sentimiento
Algunos ejemplos en la industria actual son:
✔ Monitoreo de marcas
✔ Mejorando soporte al cliente
✔ Revisando feedback de empleados
✔ Proveer mejores productos de analitica
✔ Monitoreo de mercado
✔ Análisis de competencia 
Ejemplos de análisis de 
sentimiento
✔ Monitoreo del contenido generado por usuarios
✔ Descubriendo influenciadores para marcas
✔ Monitoreo de redes sociales para hacer branding 
y publicidad
✔ Mejor Manejo de crisis 
✔ Creación de recomendaciones de acuerdo a las 
necesidades de los clientes
Veamos a continuación algunas aplicaciones 
puntuales.
1)Campañas Políticas
En la campaña presidencial de EEUU 2022 
se utilizó análisis de sentimientos para 
determinar cómo recibe el público los 
anuncios. La administración de Biden aplicó 
el análisis de sentimientos para evaluar los 
anuncios de políticas. Además, esta forma 
de análisis se puede utilizar para estudiar la 
cantidad de menciones negativas sobre 
candidatos en varias fuentes de noticias y 
medios.
2) Atención al cliente
Dada la gran cantidad de solicitudes que 
deben procesarse y evaluarse, así como los 
diversos niveles de urgencia de varias 
solicitudes. Al usar herramientas de 
comprensión del lenguaje natural, una 
empresa  como Mercadolibre puede 
procesar automáticamente una gran 
cantidad de llamadas telefónicas, correos 
electrónicos y chats en línea, y clasificarlos 
en categorías según los rasgos comunes.
3) Prevención de crisis 
Herramientas como Brand24 son útiles para 
monitorear las redes sociales en tiempo 
real. Recopilan todas las menciones de 
palabras en sitios de noticias, sitios web y 
foros de discusión. Esto garantiza que los 
especialistas en relaciones públicas reciban 
alertas sobre los comentarios negativos 
inmediatamente después de que 
aparezcan, de modo que la empresa pueda 
implementar medidas.
4) Finanzas
En el caso del mercado de valores es de 
suprema importancia el análisis de 
sentimientos en los mercados de futuros, 
commodities y criptomonedas. Es por esto 
que se necesita del procesamiento de 
lenguaje natural para poder identificar 
puntos de entrada y salidas en posibles 
inversiones que podamos hacer.
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Ejemplo en vivo
Analizaremos un ejemplo donde utilizaremos 
técnicas de análisis de sentimiento con el fin de 
caracterizar revisiones de películas de Amazon. 
Análisis de 
sentimiento: Caso 
aplicado
¿Cómo podemos predecir el 
sentimiento asociado con 
una interacción con el 
cliente?
Eres un científico de datos para una gran empresa de 
comercio electrónico. Tienes decenas de miles de 
clientes que escriben reseñas sobre productos cada 
día. Cada revisión contiene comentarios textuales 
junto con un sistema de calificación de 1 a 5 estrellas 
(siendo 1 la menos satisfecha y 5 la más satisfecha)
Contexto empresarial
Su empresa también recopila comentarios sobre las 
experiencias de sus clientes con la interacción del sitio 
web después de cada compra. La empresa quiere 
cuantificar la satisfacción del cliente proveniente de 
estas interacciones no calificadas para ayudar con 
futuras decisiones comerciales (por ejemplo, 
determinar qué tan bien se están desempeñando sus 
diversos agentes de servicio al cliente).
Problema comercial
La pregunta problema para este caso particular tiene 
que ver con: 
¿Cómo construir modelos que puedan identificar 
el sentimiento (positivo o negativo) de cada una 
de estas interacciones no calificadas?.
Contexto analitico
Los datos son un conjunto de reseñas en formato de 
archivo CSV. Combinaremos lo que aprendimos sobre 
el procesamiento de texto y los modelos de 
clasificación para desarrollar algoritmos capaces de 
clasificar las interacciones por sentimiento.
Contexto analitico
Las columnas del dataset son:
✔ ID: valor único para cada fila
✔ ProductId: una referencia del producto sobre la 
reseña
✔ UserId: una referencia del usuario que dejo el 
review
✔ HelpfulnessNumerator: número de lectores 
que han indicado que la reseña ayuda o es 
interesante
✔ HelpfulnessDenominator: número total de 
personas que han dado una indicación de si la 
reseña ha sido útil o no
Contexto analitico
Las columnas del dataset son:
✔ Score: rating (1-5)
✔ Time: fecha formato timestamp que indica 
cuando se creó la revisión
✔ Summary: El resumen escrito por el 
usuario de lo que trata la reseña
✔ Text: la revisión escrita por el usuario
Análisis Descriptivo 
básico
El número medio de palabras por revisión es 77. Por 
otro lado se observa que muchas revisiones son 
calificadas con scores 5 ó 4 y hay muy pocas entre 1 y 
3
Análisis Descriptivo 
básico
Al realizar el Wordcloud de 
las revisiones se puede ver 
que muchos reviews hablan 
sobre temas asociados a 
comida y palabras como 
good, love y best
Estandarización de reviews
Para propósitos del análisis de sentimiento 
convertiremos todos los ratings en valores binarios 
con las siguientes reglas:
✔ ratings de 4 ó 5 serán convertidos a 1 (positivo)
✔ ratings de 1 ó 2 serán convertidos a 0 (negativo)
✔ ratings de 3 serán removidos del análisis
Esto nos permite generar un problema de clasificación 
binario que es suficiente en este caso para resolver la 
pregunta problema 
Estandarización de reviews
Luego de este proceso podemos ver que quedan 
muchos registros en la categoría “positio” y pocos 
en la categoría “negativo”. Esto en algunos casos 
puede generar problemas ya que estamos ante un 
desbalance marcado de las clases, para esto existen 
técnicas como el oversampling entre otras pero que 
no se tratan a detalle en este caso.
Preprocesamiento
Recordemos que el preprocesamiento de texto y la 
normalización es crucial antes de desarrollar un 
modelo de NLP, algunos pasos importantes son:
1. Convertir palabras a minúsculas
2. Remover caracteres especiales
3. Remover stopwords y palabras de alta 
frecuencia
4. Stemming y lematización
Preprocesamiento
¿Es la eliminación de caracteres especiales 
incluso una buena idea?
¿Cuáles son algunos ejemplos de caracteres 
que probablemente sería seguro eliminar y 
cuáles no?
Preprocesamiento
Eliminar caracteres especiales es una decisión 
subjetiva, especialmente en casos como este. Las 
personas a menudo usan caracteres especiales para 
expresar sus emociones y pueden dejar una reseña 
como "¡¡¡Este producto es el peor!!!", mientras que 
una reseña positiva podría ser "Este producto es el 
mejor". ¡Me encantó!' Aquí, la presencia de signos de 
exclamación indica claramente algo sobre el 
sentimiento subyacente, por lo que eliminarlos puede 
no ser una buena idea.
Preprocesamiento
Por otro lado, eliminar la puntuación sin carga 
emocional, como las comas, los puntos y el punto y 
coma, probablemente sea seguro.
En aras de la simplicidad, procederemos eliminando 
todos los caracteres especiales; sin embargo, vale la 
pena tener en cuenta que esto es algo para revisar 
dependiendo de los resultados que obtengamos más 
adelante
Resultados previos
Al obtener el percentil de palabras más frecuentes 
observamos palabras como [the, I, and, a, it, to, of, is, 
this y br] por otro lado usando la misma metodología 
obtenemos las menos frecuentes (último percentil) y 
se obtienen palabras como [slick, cloured, innocuous, 
marketer, destroyers], la mayoría con errores 
ortograficos 
Modelo de análisis de 
sentimiento
Las variables a tener en cuenta son: Text, Score, 
Sentiment_rating
Las variables independientes o características del 
modelo se derivan del texto de revisión. En la clase 
anterior, discutimos cómo podemos usar n-grams 
para crear características o features.
Modelo de análisis de 
sentimiento
Tenemos diferentes metodologías para realizar el 
modelo:
1. Bag of Words
2. TF-IDF
3. Word Embeddings
Para todos los casos utilizaremos el modelo de 
regresión logística para poder comparar los resultados 
y determinar cuál es la mejor metodología 
Modelo de análisis de 
sentimiento
Los resultados obtenidos luego de entrenar el modelo 
en cada caso son:
1. Bag of Words (Accuracy: 0.90, F1=0.94)
2. TF-IDF (Accuracy: 0.91, F1=0.95)
3. Word Embeddings (Accuracy: 0.89, F1=0.94)
En cada caso se utilizaron diversas formas de 
entrenamiento (1-gram a 4-grams). Además se probó 
el algoritmo random forest pero sin muy buenos 
resultados
Algunos insights 
importantes
Los resultados obtenidos por Word embeddings no 
fueron tan buenos como las representaciones de bolsa 
de palabras o TF-IDF. Además, aunque los Word 
Embeddings  fueron realmente efectivas para reducir 
el número total de dimensiones, adolece del problema 
de la interpretabilidad. Esto significa que es muy difícil 
para nosotros incluso diagnosticar qué está causando 
su bajo rendimiento
Algunos insights
importantes
Sin embargo, ¿recuerdan cómo "bueno" y "malo" 
estaban juntos en el espacio vectorial? Esta es una de 
las razones por las que WE pueden no funcionar tan 
bien para el análisis de sentimientos en conjuntos de 
datos pequeños: las WE son buenas para usar 
"conocimiento" del mundo externo (latente en WE 
preentrenadas) para inferir información adicional 
sobre un conjunto de datos más pequeño, pero en el 
caso del análisis de sentimientos, esto podría hacer 
más daño que bien 
Algunos insights 
importantes
En nuestro caso, la creación de características usando 
TF-IDF nos dio una precisión del 92 % con 
características muy interpretables. Esta es una buena 
combinación, por lo que consideramos que este es el 
mejor modelo para nosotros aquí.
Algunos insights 
importantes
Tenga en cuenta que para un experimento real, 
habríamos dividido nuestro conjunto de datos en tres 
partes, no solo en dos. Cuando se ejecuta un 
experimento varias veces con diferentes parámetros, es 
casi seguro que algunos resultados serán mejores 
simplemente por casualidad, y es mala ciencia 
seleccionar el modelo con mejor desempeño después 
de docenas o cientos de ejecuciones.
Algunos insights 
importantes
Para evitar este problema, los datos deben dividirse en 
conjuntos de "entrenamiento", "prueba" y "validación". 
El conjunto de "prueba" debe reservarse al comienzo 
del experimento y nunca mirarse. El modelo debe 
ajustarse utilizando el conjunto de "validación".
Análisis de sentimiento
En esta oportunidad utilizaremos lo aprendido en clase 
para poner en práctica la metodología de análisis de 
        sentimiento
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Análisis de 
sentimiento
Nos reuniremos en breakout rooms y formaremos grupos, 
utilizaremos los siguientes datos de Twitter se les propone:
1. Utilizar el dataset train (columnas: label y tweet) 
para generar un resumen descriptivo de las reseñas
2. Generar un modelo de clasificación binaria que 
pueda ser utilizado para determinar si un comentario 
es negativo (0) o positivo, para esto calcular 
métricas con el dataset test provisto en la página 
web
¿Preguntas?
CLASE N°54
Glosario
spaCY: es una librería gratuita y de código     Análisis de sentimiento calificado:  tipo 
abierto para NLP en Python con muchas           de análisis de sentimiento que permite 
capacidades integradas que se está              detectar cuándo comentarios son negativos 
volviendo cada vez más popular.                 y poder establecer estrategias para mitigar 
                                               impactos o deserción de clientes
Análisis de sentimiento: se enfoca en la 
polaridad de un texto (positivo, negativo,      Detección de emociones: Permite ir más 
neutral) pero también va más allá de la         allá de la polaridad para detectar 
polaridad para detectar sentimientos y          emociones, como felicidad, frustración, ira 
emociones específicas (enojado, feliz,          y tristeza. Muchos sistemas de detección 
triste, etc.), urgencia (urgente, no urgente)   de emociones utilizan léxicos (es decir, 
                                               listas de palabras y las emociones que 
e incluso intenciones (interesado). v. no       transmiten) o algoritmos complejos de 
interesado).                                    aprendizaje automático.
Opina y valora 
esta clase
           Resumen 
       de la clase hoy
      ✓ Introducción a spaCY
      ✓ Preprocesamiento con spaCY
      ✓ Análisis de sentimiento con spaCY y NLTK
      ✓ Caso aplicado: Análisis de sentimiento
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 02. DATA SCIENCE
  Introducción a la 
 programación con 
       Python I
Temario
                  01                         02                        03
         La necesidad de             Introducción a            Introducción a 
          Información en                     la                        la 
          la Industria 4.0           programación               programación 
          ✓ Industria 4.0                                        con Python 
                                       con Python               ✓ Estructura de 
                                                                   (Parte II)
          ✓ Transformación digital       (Parte I)                  control
                                      ✓ Definición de 
          ✓  Ciclo de vida de un          programa              ✓ Funciones
             proyecto de ciencia de   ✓ Instalación de 
             datos                        Python                ✓ Datos
          ✓  Valor y retorno de la                              ✓ IPython
             Ciencia de Datos         ✓ Nociones 
          ✓ Estrategia data-driven        básicas               ✓ Instalación
Objetivos de la clase
         Realizar una primera aproximación al lenguaje 
         de programación Python.
         Conocer las distintas formas de desarrollo con 
         Python.
MAPA DE CONCEPTOS
                      Programación y 
Toma de                lenguajes
contacto con                               Python como 
Python                                     lenguaje
                      Interpretado vs 
                      Compilado           Basics: Variables, 
                                          asignación, 
                                          operaciones
                      Python tradicional
Formas de                                  Estructuras
desarrollo con 
Python
                      IPython y notebooks
                                          Funciones
                                          Tipos de datos
Cuestionario de tarea
¿Te gustaría comprobar tus 
conocimientos de la clase anterior?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
Librería Pandas
Estructuras fundamentales series
 y Data Frames
Indexamiento de series y Data Frames
Datos nulos 
También aprendimos a leer archivos planos (.txt, .csv, .xlsx) locales o por APIs en Python.
Definición de
programa
Desde el principio: 
programación y 
Python
¿Qué es la programación?
✔ La programación es una forma de 
ejecutar un algoritmo.
✔ Un algoritmo es una secuencia de 
pasos que lleva a un resultado.
✔ Una receta es un algoritmo.
✔ Si se sigue el algoritmo, se llega al 
resultado.
Programa y computadora
✔ La computadora nació para 
   resolver cálculos.                           ✔ La computadora entiende 
✔ La programación es un                            ceros y unos (lenguaje 
   complemento para la                             binario), nosotros no.
   computadora.                                 ✔ Por lo tanto, un programa 
✔ Es una forma de que la computadora               traduce un lenguaje 
   entienda el funcionamiento de                   humano a lenguaje 
   un algoritmo y lo ejecute.                      binario.
Programación y lenguajes
✔ No existe un solo lenguaje que               ✔ Para Data Science, existen 
   solucione todos los problemas                  algunos lenguajes que 
✔ Cada lenguaje resuelve un                       funcionan muy bien: 
   conjunto de problemas posibles:                Python, R, Julia y Scala 
   Empresariales, Web, Ciencia, Salud,            son algunos de ellos.\
   etc.
         Python              R                 Julia          Scala
Lenguaje 
interpretado vs. 
compilado
¿Interpretado o Compilado?
                                                ✔ Usa un programa intérprete que 
                                                   traduce en tiempo casi real nuestras 
Python es un lenguaje interpretado,                 órdenes a binario.
esto quiere decir que:
                                                ✔ La traducción se hace línea por 
                                                   línea.
                                                ✔ Podemos probar código “de a 
                                                   pedacitos”.
                                                ✔ El lenguaje compilado se traduce 
                                                   todo junto al final.
                                                ✔ No es simplemente una mejora, es 
                                                   una forma de trabajar muy útil para 
                                                   Data Science.
Python como 
lenguaje
PARA RECORDAR
Contenido destacado
Python es el lenguaje más solicitado en las 
búsquedas laborales relacionadas con Data Science 
y se ubica entre el segundo y tercer puesto en 
2021 de acuerdo a varios rankings de lenguajes de 
desarrollo general (no sólo Data Science).
PARA RECORDAR
En otras palabras, Python no puede faltar 
en un portfolio de Data Science.
Python en 
pocas palabras
Lenguaje Open Source: un código abierto y                         REEMPLAZAR 
accesible diseñado para que cualquier pueden ver,                 POR IMAGEN
modificar y distribuirlo.
Creado por Guido Van Rosssum y su primera 
versión se dio en 1991 en el CWI (Centrum 
Wiskunde Informática)
Python, Open Source: 
componentes
                    1                            2                           3
              Intérprete                        IDE                     Paquetes
                programa                    entorno de                 conjuntos de 
                intérprete,              desarrollo, lugar             funciones pre-
                traductor a                    donde                   armadas para 
                  binario.                 escribiremos                  problemas 
                                              código.                   habituales.
Instalación de 
Python vía 
miniconda
Python tradicional
Anaconda y 
Miniconda
Tradicionalmente, Python puede 
desarrollarse en Anaconda, o en su 
versión simplificada, Miniconda.
A su vez, puede utilizarse de 
varias formas más:
Formas
La forma más básica es escribiendo            Otra forma más útil es usando Python 
python en la terminal, lo que abre un         interactivo (IPython). Puede accederse 
entorno de trabajo dentro de la misma         escribiendo ipython en la terminal.
terminal.
                                              ������ No aporta muchas mejoras si se usa de 
������ No es la forma más cómoda, ni la más      esa forma.
utilizada.
                                             ¡No siempre es la mejor forma! 
Jupyter notebooks
IPython y notebooks
                                               ✔ Escribimos código en el navegador 
Las notebooks siguen siendo IPython,               que resulta ser el IDE.
pero con vitaminas ������                           ✔ El código pasa por el mismo 
                                                  intérprete que es el que usa la 
                                                  terminal, pero todo se trabaja en el 
                                                  navegador.
                                               ✔ El código se escribe en cajas de 
                                                  texto que pueden ejecutarse de a 
                                                  una o todas juntas.
                                               ✔ El conjunto total de cajas de texto es 
                                                  una notebook.
                                             ������ Esta configuración es de las más 
                                             utilizadas para Data Science.
IPython y notebooks
Podemos encontrar 4 partes principales:
1. Nombre del notebook (termina con 
extensión .ipynb)
2. Barra de menú: Permite ejecutar 
código y opciones genéricas 
3. Toolbar: Permite ejecutar celdas de 
código, guardar, añadir, borrar, 
cortar o pegarlas
4. Celdas de Código: Pueden ser 
Markdown (texto) o Código Python
¿Cómo usar Google 
Collab?
Google Colab
Permite trabajar en un entorno no local y         ✔ Es un producto de Google Research. 
la creación de Notebooks ������                          Está especialmente adecuado para 
                                                    tareas de aprendizaje automático, 
                                                    análisis de datos y educación.
                                                 ✔ Jupyter es el proyecto de código 
                                                    abierto en el que se basa Colab.
                                                 ✔ Nos permite compartir notebooks sin 
                                                    la necesidad de descargar ningún 
                                                    software extra.
                                                 ✔ El código se ejecuta en una máquina 
                                                    virtual dedicada a tu cuenta y 
                                                    pueden eliminarse luego de cierto 
                                                    tiempo.
Ejemplo en vivo
¿Cómo podemos usar Google Colab como 
un entorno para programar lenguaje de 
Python?
¡Vamos a verlo conjuntamente!
Cómo usar Google Colab
Si queremos crear un modelo de              Google Colaboratory es un entorno de 
aprendizaje automático, pero no tienen      portátil Jupyter, gratuito proporcionado 
una computadora que pueda asumir la         por Google donde puede usar GPU y TPU 
carga de trabajo, Google Colab es la        gratuitas que pueden resolver todos estos 
plataforma ideal.                           problemas.
Incluso si tiene una GPU o una buena 
computadora, crear un entorno local con 
anaconda e instalar paquetes y resolver 
problemas de instalación es una molestia.
Cómo usar Google Colab
                      EXAMPLES: Contiene ejemplos de 
                      Jupyter notebooks con diversos 
                      ejemplos.
                      RECENT: Jupyter notebooks que has 
                      trabajado recientemente.
                      GOOGLE DRIVE: Jupyter notebooks en 
                      tu google drive.
                      GITHUB: Puedes añadir Jupyter 
                      notebooks desde Github pero es 
                      necesario conectar Colab con GitHub.
                      UPLOAD: Si deseas subir un Jupyter 
                      notebook desde tu equipo local.
Ir al siguiente enlace: https://colab.research.google.com
Cómo usar Google Colab
De lo contrario, puede crear un nuevo 
cuaderno de Jupyter haciendo clic en 
Nuevo cuaderno de Python3 o Nuevo 
cuaderno de Python2 en la esquina 
inferior derecha.
Al crear un nuevo cuaderno, creará un 
cuaderno Jupyter llamado 
Untitled0.ipynb y lo guardará en su 
unidad de Google en una carpeta llamada 
Colab Notebooks. Todos los comandos de 
los cuadernos de Jupyter funcionarán 
aquí.
IDE’s
IDE’s
Son aplicaciones de software que            ✔ Editor de código.
permiten a programadores desarrollar        ✔ Depuradores (Debuggers) que 
código en diferentes lenguajes.                permiten encontrar errores en el 
Consta, usualmente, de:                        código 
                                          ✔ Herramientas automáticas
               IDE’s para el 
               desarrollo de 
               Python
               Permite trabajar en un entorno no local y la creación de 
               Notebooks ������
               Las herramientas que mostramos anteriormente no son 
               las únicas en donde compilar código de Python…
    ¡Atención!
Recuerda instalar Python con Anaconda para la 
      próxima clase.
        Ver tutorial
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Nociones básicas: 
Variable, asignación, 
expresiones
Variable
Variables
Las variables se utilizan para almacenar        Es útil pensar en las variables como 
información para ser referenciada y             contenedores de información. 
manipulada en un programa de                    Su único propósito es etiquetar y 
computadora.                                    almacenar datos en la memoria. 
Proporcionan una forma de etiquetar los 
datos con un nombre descriptivo, para 
que los programas puedan ser entendidos 
con mayor claridad. 
Variables
Los tipos de datos estándar o 
integrados de Python: 
a) Numérico 
b) Tipo de secuencia 
c) Booleano 
d) Conjuntos 
e) Diccionario 
Asignación
               Asignación
               Nombrar variables es una tarea compleja. 
               Cuando nombre variables, piense detenidamente en los 
               nombres (Comprensible).
               La asignación de lleva a cabo por medio del símbolo =
               El nombre de la variable va a la izquierda y el valor que 
               desea almacenar en la variable va a la derecha.
Asignación
Reglas para asignación de variables        ✔ Los nombres de las variables 
en Python                                      distinguen entre mayúsculas y 
✔ El nombre de una variable debe             minúsculas (nombre, Nombre y 
   comenzar con una letra o el carácter      NOMBRE son tres variables 
   de subrayado.                             diferentes).
✔ Un nombre de variable no puede         ✔ Las palabras reservadas (palabras 
   comenzar con un número.                   clave) no se pueden usar para 
✔  Un nombre de variable solo puede          nombrar la variable.
   contener caracteres alfanuméricos y 
   guiones bajos (A-z, 0-9 y _).
Objetos y punteros
Objetos y punteros
Python es un lenguaje orientado a objetos       ✔ Datos
Es así que en Python todo es un objeto, o       ✔ Metadatos, atributos o propiedades 
sea, cuenta con:                                   (un punto y una palabra sin 
                                                  paréntesis):
                                                  X.atributo ������ Un atributo caracteriza 
                                                  al dato
                                               ✔ Funcionalidad o métodos (un punto y 
                                                  una palabra con paréntesis):
                                                  x.método()  ������ Un método es algo 
                                                  que el dato puede hacer, por lo 
                                                  tanto al ejecutarlo le estamos 
                                                  pidiendo al dato que ejecute una 
                                                  acción
x=1.0
x.is_integer() # ¿es x un entero? se lo preguntamos con el método is_integer()
x=1.4
x.is_integer() # ¿y ahora? se lo preguntamos de vuelta
print(x.real,x.imag)  # miramos los atributos de x, en este caso su parte real 
       # y su parte imaginaria
Objetos y punteros
Las variables en Python no contienen 
los datos, sino que apuntan a los 
datos. 
Esta es la forma de trabajo de los 
punteros, lo que hace que el lenguaje 
sea más eficiente.
   Para pensar
¿Cuáles son las salidas de los siguientes 
bloques de código?
                       Contesta mediante el chat de 
                       Zoom 
¿No notaste algo raro en el 
ejercicio anterior...?
Objetos y punteros
Cuando operamos sobre una variable          Cuando realizamos una asignación (=) 
(método) operamos sobre el objeto al        conectamos (apuntamos) la variable 
que apunta.                                 al objeto. Aquí no cambiamos el objeto.
Objetos y punteros
x = [1, 2, 3]     # x es una lista
y = x                   # el objeto al que apunta x ([1, 2, 3]) ahora es también 
                        # apuntado por y
print(y is x)     # x e y son el mismo objeto (True)
print(x,y)        # [1, 2, 3] [1, 2, 3]
x.append(4)             # aquí operó sobre el objeto [1, 2, 3] apuntado por x.
                        # Los métodos se identifican luego de un punto (x.método())  
print(y)          # como x e y apuntan al mismo objeto, y refleja los cambios
x = "hola"              # al realizar asignación, ahora x apunta al objeto texto 
                        # (string) "hola" 
print(x is y)     # x e y ahora no apuntan al mismo objeto (False)
print(x,y)        # x e y apuntan a dos objetos diferentes (“hola” [1, 2, 3, 4])
Objetos y punteros
La diferencia es muy sutil y en general no 
afecta el trabajo de Data Science. No 
obstante, no todos los lenguajes se 
comportan así.
⚠  Hay que tener en cuenta esto para no 
cometer errores. 
✔Un método comienza por un punto 
después de la variable.
✔El método modifica el objeto 
apuntado por la variable.
✔La variable no es, ni contiene al 
objeto.
✔La asignación “conecta” a la variable 
con el objeto apuntado.
Expresiones
Expresiones
Una expresión es una combinación de                                  REEMPLAZAR 
operadores y operandos que se interpreta                             POR IMAGEN
para producir algún otro valor. 
En cualquier lenguaje de programación, una 
expresión se evalúa según la precedencia de 
sus operadores. 
Expresiones
Expresiones constantes: son las 
expresiones que solo tienen valores 
constantes.
x = 15 + 1.3                 ������   16.3
print(x)
Expresiones
Expresiones aritméticas: una expresión 
aritmética es una combinación de valores 
numéricos, operadores y, a veces, 
paréntesis. 
x = 40
y = 12                        ������    52
                                   28
add = x + y                        480
sub = x - y                         3.3333333333333335
pro = x * y
div = x / y
print(add);print(sub);print(pro);print(div)
Expresiones
Expresiones integrales: este es el tipo 
de expresiones que producen solo 
resultados enteros después de todos los 
cálculos.
a = 13
b = 12.0                      ������    25
 c = a + int(b)
print(c)
Expresiones
Expresiones flotantes: este es el tipo 
de expresiones que producen números de 
punto flotante como resultado de todos 
los cálculos
a = 13
b = 5                         ������    2.6
c = a / b
print(c)
Operadores
               Operadores
               ✔ Los operadores permiten trabajar sobre las 
                 variables, a la manera de las operaciones 
                 matemáticas.
               ✔ Cada operador da un resultado como salida.
               ✔ Identificamos 4 tipo de operadores:
                     ✓ Operadores aritméticos
                     ✓ Operadores de asignaciones
                     ✓ Operadores de identidad y pertinencia
 Operadores aritméticos
   Los operadores aritméticos son directamente 
   operaciones matemáticas estándar.
                                 Aritméticos
                    a + b                                           Suma
                     a - b                                          Resta
                     a * b                                 Multiplicación
                     a / b                                       División
                    a // b      División entera (resultado sin decimal)
                    a % b           Módulo (resto de la división entera)
                   a ** b                                Exponenciación
                        -a                                      Negativo
Operadores de                                                     Asignaciones
asignaciones                                                    a += b              a = a + b
                                                                a -= b              a = a - b
Los asignadores simplifican operadores aritméticos              a *= b              a = a * b
comunes.                                                         a /= b              a = a / b
                                                               a //= b             a = a // b
                                                               a %= b              a = a % b
                                                               a **= b            a = a ** b
          Comparadores                      Operadores de 
        a == b            a igual a b
         a != b       a distinto de b       comparación
          a < b          a menor a b
          a > b          a mayor a b        Los comparadores dan resultados lógicos (si/no, 
                a menor o igual que         true/false)
        a <= b                      b
                a mayor o igual que 
        a >= b                      b
Operadores de 
identidad y                                                  Identidad y pertenencia
                                                                         a es el mismo objeto 
pertenencia                                                        a is b             que b
                                                                       a no es el mismo objeto 
✔ Los operadores de identidad y pertenencia                    a is not b            que b
   verifican relaciones entre objetos.                           a in b  a está contenido en b
✔ Dentro de esta categoría, los operadores “in”,              a not in b a no está contenido en b
   como casos particulares, buscan objetos 
   dentro de listas. ¡Son muy útiles!
Uso de filtros booleanos
Los operadores nos permiten crear filtros booleanos que ayudan a obtener filtros 
rápidos para información de interés
import pandas as pd                                    index_bool=df['Eo']>10
import numpy as np                                     index_bool
df= pd.DataFrame(data=np.random.randint(64, 
size=(8,8)),columns=['Ja','Mu','Ct','Dn','Eo','Tp','Yn','Om'])
print(df)                                              df['Eo'][index_bool]
 Ja  Mu  Ct  Dn  Eo  Tp  Yn  Om                       0    41
0  26  41  62  50  41  52  49  35                      1    18
1   4  13  37   4  18   7  30  43
2  57  19  41  55  53  15  14  57                      2    53
3  47   6  53  47  58  15  39  49                      3    58
4  61  32  53  15  48  56  42  20                      4    48
5  60  56  40  55   7  32  51  13                      6    48
6  45   9  29  21  48  43  61  30
7  44   5  27  59   6  47   7  46                      Name: Eo, dtype: int64
Ejemplo en vivo
Examinemos un poco lo que se conoce 
como estructuras de control
Para pensar
¿Qué diferencia hay entre usar and/or? 
¿Qué significa el operador %?
                  Contesta 
                  mediante el 
                  chat de 
                  Zoom 
CLASE N°2
Glosario
Programación: formas de ejecutar un algoritmo      Variable: Cualquier estructura que permita 
(recetas)                                          almacenar información para su manipulación
Lenguajes: herramientas computacionales que        Asignación: Proceso mediante el cual se le 
permiten resolver problemas con estructuras de     asigna un valor particular a una variable 
código. En Data Science existen varios comunes: 
Python , R , Java, Julia, C, C++                   Punteros: herramientas que nos permiten 
                                                  conectar a las variables con sus valores 
Lenguaje interpretado: cualquier lenguaje de       respectivos
programación que se ejecute línea a línea y que 
convierta las órdenes a formato binario (e.g       Expresiones: combinaciones de operadores y 
Python , R)                                        operandos que dan como resultado un valor 
                                                  particular
IDE: aplicaciones donde escribimos el código de 
un lenguaje particular (e.g Spyder, Kite, Visual   Operadores: son los que permiten trabajar 
Studio, Atom)                                      sobre las variables, pueden ser de 4 tipos 
                                                  (aritméticos, relacionales, de asignación y 
                                                  lógicos)
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Definición de Programa
      ✓ Lenguaje Interpretado vs compilado
      ✓ Python como Lenguaje
      ✓ Nociones básicas: variable, asignación y 
        expresiones
      ✓ Objetos y punteros
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
        Clase 08. DATA SCIENCE
   Visualizaciones y 
 primeros pasos con 
 Data Science (parte 
             II)
Temario
                  07                        08                         09
          Visualizaciones           Visualizaciones 
             y primeros                y primeros                Estadística 
          pasos con DS I            pasos con DS II              descriptiva
                                    ✓ Seaborn                 ✓ Introducción 
           ✓ Matplotlib             ✓ Tipos de gráficos       ✓ Medidas de 
           ✓ Tipos de               ✓ Subplot y Facetgrip         resumen 
              gráficos              ✓ Customizaciones sobre   ✓ Distribución de 
                                       gráficos 
           ✓ Customizacione                                       variables
                                    ✓ Nutshell
              s sobre gráficos                                ✓ Intervalos de 
                                    ✓ Scikit - Learn              confianza
Objetivos de la clase
         Conocer las librerías más utilizadas para 
         visualización Python
         Aprender a graficar datos en Python
         Entender el uso básico de las librerías más 
         utilizadas: Matplotlib y Seaborn
                                             Seaborn: Comandos 
                                             básicos
     MAPA DE CONCEPTOS                       Diferencias entre 
                                             Matplotlib y Seaborn                Axis-level
                                             Tipos de funciones                  Figure-level
                                             Seaborn
       Introducción a                        Tipos de gráficos                   Líneas
       Seaborn
                                             Uso de Subplots y                   Puntos
                                             Facetgrid
                                             Customizaciones sobre               Barras
                                             gráficos
                                                                                 Histograma
                                             Ejemplo aplicado 
                                             Seaborn                             Boxplot
       Tipos de                              Supervisado
       aprendizajes
       Introducción                          No Supervisado
       Scikit-Learn
Cuestionario de tarea
¿Te gustaría comprobar tus 
conocimientos de la clase anterior?
Te compartimos a través del chat de 
Zoom / chat de la plataforma el enlace a un 
breve cuestionario de Kahoot.
Duración: 10 minutos
En la primera parte vimos los siguientes temas
            Visualización en 
               Python 
 Matplotlib                  Gráficos
                           Gráficos de línea.
                           Gráficos de puntos.
                           Gráficos de barra.
                           Histograma.
                           Piechart
Seaborn:
Comandos básicos
¿Qué es Seaborn?
✓ Librería para hacer gráficos 
estadísticos en Python 
(fundamentada en matplotlib y 
pandas).
✓ Funciona capturando marcos de 
datos completos o matrices que 
contienen todos sus datos 
✓ Realiza todas las funciones internas 
necesarias para el mapeo semántico 
y la agregación estadística para 
convertir datos en gráficos 
informativos.
Diferencias entre 
Seaborn y Matplotlib
Características                   Matplotlib                              Seaborn
Funcionalidad        Gráficos básicos                      Temas fascinantes. Compila datos a 
                                                           gráficos
   Sintaxis          Sintaxis larga y compleja e.g         Sintaxis simple y fácil de entender 
                     matplotlib.pyplot.bar(x_axis,         e.g seaborn.barplot(x_axis,y_axis)
                     y_axis)
Múltiples figuras      Se pueden tener figuras múltiples     Puede tener más problemas de 
                     simultáneamente                       memorias
 Flexibilidad        Altamente customizable y robusto      Evita superposición de temas 
DataFrames y         Funciona eficientemente y trata a     Más funcional y organizado y trata 
    Arrays           figuras y ejes como objetos           datasets como unidad simple
Casos de uso         Gráficas diversas usando Numpy        Versión extendida de Matplotlib con 
                     y Pandas                              el uso de Numpy y Pandas
Tipos de funciones 
en Seaborn
Axes-level & Figure-level
✓ Axes-level: grafican datos en un         Algunas diferencias
    objeto matplotlib.pyplot.Axes que             ✓ Figure-level la leyenda se 
    retorna el valor de la función                   coloca fuera del frame y 
✓ Figure-level: interactúa con                       pueden crear fácilmente 
    matplotlib mediante un objeto                    figuras con múltiples subplots.
    Seaborn que usualmente es                     ✓ Axes-level no modifican más 
    FacetGrid                                        allá de los ejes en los que se 
                                                     dibujan y  aceptan el 
                                                     argumento ax que se integra 
                                                     con la interfaz orientada a 
                                                     objetos.
    Funciones Axes-Level
En cada módulo hay 
una función Figure-
level que ofrece una     REEMPLAZAR 
interfaz para sus        POR IMAGEN
diversas funciones 
Axes-level (e.g 
distplot() es una 
función Figure-level)
                  Axes-level & Figure-
                                           level
       # Axes-level                                       # Figure-level
       sns.histplot(data=penguins,                        sns.displot(data=penguins, 
       x="flipper_length_mm", hue="species",              x="flipper_length_mm", hue="species", 
       multiple="stack")                                  multiple="stack")
          Figure: gráficos 
            segmentados
# Multiples figuras (Figure-level)
sns.displot(data=penguins, x="flipper_length_mm", hue="species", 
col="species")
En este tipo de funciones podemos incorporar gráficos segmentados por 
alguna categoría automáticamente .
           # Axis level
           import matplotlib.pyplot as plt
           f, axs = plt.subplots(1, 2, figsize=(8, 4), 
           gridspec_kw=dict(width_ratios=[4, 3]))
           sns.scatterplot(data=penguins, x="flipper_length_mm", 
           y="bill_length_mm", hue="species", ax=axs[0])
           sns.histplot(data=penguins, x="species", 
           hue="species", shrink=.8, alpha=.8, legend=False, 
           ax=axs[1]); f.tight_layout()
         # Figure-level
         tips = sns.load_dataset("tips")
         g = sns.relplot(data=tips, x="total_bill", y="tip")
Tipos de 
gráficos
                                 Gráficos 
Líneas           Puntos           Barras         Histograma         Boxplot
Gráfico de líneas
# Lineplot
flights = sns.load_dataset("flights")
flights.head()
# Axis-level
may_flights = flights.query("month == 'May'")
sns.lineplot(data=may_flights, x="year", 
y="passengers")
# Axis-level
flights_wide = flights.pivot("year", "month", 
"passengers")
flights_wide.head()
sns.lineplot(data=flights_wide)
Gráfico de puntos
# Scatterplot
tips = sns.load_dataset("tips")
tips.head()
# Axis-level
sns.scatterplot(data=tips, x="total_bill", y="tip")
# Axis-level
sns.scatterplot(data=tips, x="total_bill", y="tip", 
hue="time")
Gráfico de barras
# Barplot
import seaborn as sns
sns.set_theme(style="whitegrid")
tips = sns.load_dataset("tips")
# Axis-level
ax = sns.barplot(x="day", y="total_bill", data=tips)
# Barplot
sns.barplot(x="day", y="total_bill", hue="sex", 
data=tips)
Histograma
# Histograma
penguins = sns.load_dataset("penguins")
# Axis-level
sns.histplot(data=penguins, x="flipper_length_mm")
# Axis-level
sns.histplot(data=penguins, x="flipper_length_mm", 
hue="species")
Boxplot
# Boxplot
penguins = sns.load_dataset("penguins")
# Axis-level
sns.histplot(data=penguins, x="flipper_length_mm")
# Axis-level
ax = sns.boxplot(x="day", y="total_bill", data=tips)
Uso de Subplots y
Facetgrid
Subgráficos
Subgráficos
 ✓ Podemos definir una grilla de gráficos 
    dentro de una misma figura.
 ✓ En plt.subplots, especificamos:
El número de filas de la grilla = nrows
El número de columnas de la grilla   = ncols
 ✓ El objeto ax se convierte en un array. 
    Por lo tanto, debemos usar corchetes 
    ⚠ 
Ejemplo
Comparemos las precipitaciones de Enero,           Como los años son los mismos para 
Febrero y Marzo a lo largo de los años. ¿Cuál      todos los gráficos, ponemos el 
será el más seco? ������                               parámetro sharex en True
Primero, 
Definimos un objeto ax con tres filas y una 
sola columna
✓ En el eje x = los años
✓ En el eje y = las precipitaciones
fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12, 5), sharex=True, sharey=True)  
Ejemplo
Segundo,  
A cada fila, le asignamos las precipitaciones 
de un mes
ax[0].plot(df_lluvias.index, df_lluvias['Jan'], label='Precipitaciones de enero')
ax[1].plot(df_lluvias.index, df_lluvias['Feb'], label='Precipitaciones de febrero', 
color='C1')
ax[2].plot(df_lluvias.index, df_lluvias['Mar'], label='Precipitaciones de marzo', color='C2')
Ejemplo
Tercero,  
Añadimos textos y leyendas
 ax[0].set_title('Precipitaciones de los primeros tres meses del año') 
 ax[2].set_xlabel('Año')  
 ax[1].set_ylabel('Precipitación (mm.)')
 ax[0].legend()  
 ax[1].legend()
 ax[2].legend()    
Resultado
Algunas observaciones
✓ Al pasar sharey=True, los subgráficos 
comparten la escala en el eje y. Esto permitió 
comparar a simple vista el volumen de 
precipitaciones.
✓ Cada subgráfico puede tener su propio título 
y etiquetas.
✓ En caso de tener dos filas y dos columnas, ax 
se torna bidimensional:
El subgráfico superior izquierdo se referencia con 
ax[0,0]
El subgráfico superior derecho se referencia con 
ax[0,1]
FacetGrid Seaborn
Algunas observaciones
✓ Esta clase mapea un conjunto de          ✓ Puede representar niveles de una 
   datos en varios ejes dispuestos en          tercera variable con el parámetro 
   una cuadrícula de filas y columnas          hue, que traza diferentes 
   que corresponden a niveles de               subconjuntos de datos en diferentes 
   variables en el conjunto de datos.          colores.
✓ Los gráficos que produce a menudo 
   se denominan gráficos de "lattice", 
   "trellis" o "small-multiple".
Ejemplo
# FacetGrid
tips = sns.load_dataset("tips")
g = sns.FacetGrid(tips, col="time",  row="sex")
g.map(sns.scatterplot, "total_bill", "tip")  
# FacetGrid
g = sns.FacetGrid(tips, col="time", hue="sex")
g.map_dataframe(sns.scatterplot, x="total_bill", 
y="tip")
g.add_legend()
Customizaciones 
sobre gráficos
Personalizando 
Seaborn
Ejemplo
   sns.set_style(style="darkgrid", rc={"grid.color": ".6"})
   sns.set_style(rc={"grid.linestyle": ":"})
   sns.set_style(rc={"axes.titleweight": "normal"})
   sns.set_style(rc={"axes.titlelocation": "left"})
   sns.set_style(rc={"axes.titlecolor": "blue"})
   sns.set_style(rc={"axes.labelcolor": "red"})
   sns.set_style(rc={"axes.labelsize": "12"})
   sns.set_style(rc={"axes.labelweight": "normal"})
   sns.set_style(rc={"axes.linewidth": "0.5"})
   sns.set_style(rc={"grid.color": "purple"})
   sns.set_style(rc={"grid.linestyle": "--"})
   sns.set_style(rc={"grid.linewidth": "0.5"})
   sns.set_style(rc={"font.fantasy": "Comic Sans MS"})
   sns.set_style(rc={"font.serif": "Utopia"})
Ejemplo
  penguins = sns.load_dataset("penguins")
  # Axis-level
  sns.histplot(data=penguins, x="flipper_length_mm")
¡Importante!
Es posible restablecer los parámetro por 
defecto
sns.reset_orig()
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Ejemplo en vivo
Surfeando en Seaborn.
El profesor hará una demostración 
compartiendo pantalla de un ejemplo 
aplicado de Seaborn. 
Tiempo estimado: 15 minutos
Ejemplo en vivo
Los valores son mediciones del pulso de         ✓ Id: número identificador de la 
30 personas tras realizar algún tipo de            persona
actividad por un determinado tiempo. Las        ✓ Diet: dieta de la persona = baja en 
columnas son:                                      grasas o sin grasas
                                               ✓ Time: duración del ejercicio = 1 min, 
                                                  15 min o 30 min
                                               ✓ Kind: tipo de ejercicio    = reposo, 
                                                  caminar o correr
Ejemplo en vivo
Observemos la distribución de las 
mediciones luego de 30 minutos de 
realizar el ejercicio.
Primero, 
Extraigamos sólo aquellas observaciones 
que se corresponden con un ejercicio de 
30 minutos
df_30_min = df_ejercicio[df_ejercicio['time'] == '30 min']
df_30_min.head()c
Ejemplo en vivo
Segundo,                                         ✓ Debemos especificar la columna de 
Graficamos las distribuciones con                    valores en el parámetro x, en este 
sns.displot                                          caso nos interesa la columna pulse
                                                ✓ Como queremos separar las 
                                                    distribuciones según el tipo de 
                                                    actividad, pasamos el parámetro 
                                                    hue='kind'
   plt.figure()
   ax = sns.displot(data=df_30_min, x='pulse', kind='kde', hue='kind', fill=True)
   ax.set(xlabel='Frecuencia Cardíaca', ylabel='Densidad', title='Distribución  de las pulsaciones')
Ejemplo en vivo
Como era de esperar, aquellas personas 
que corrieron terminaron (en promedio) 
con una mayor frecuencia cardíaca 
respecto de las que no lo hicieron
Algunas observaciones
✓ Con el parámetro kind='kde', Seaborn 
realiza una estimación de la distribución 
a partir de los datos del Data Frame. A 
grandes rasgos, se puede pensar a este 
tipo de gráficos como una versión 
suavizada del histograma de frecuencias 
relativas.
✓ Con el parámetro kind='hist', Seaborn 
graficará un histograma.
✓ El parámetro fill añade el sombreado 
debajo de la distribución
Comparando
Para comparar las distribuciones en                                   REEMPLAZAR 
base a las dietas, sólo hay que cambiar                               POR IMAGEN
el parámetro hue
Ejemplo en vivo
Surfeando en Seaborn.
El profesor hará una demostración, 
compartiendo pantalla, de un ejemplo de 
Categorical plots en Seaborn
Tiempo estimado: 15 minutos
Comparación 
desgregando por dos 
categorías
✓ Si bien los ejemplos anteriores nos 
permitían visualizar las distribuciones de 
acuerdo a una categoría, podríamos 
querer visualizar los datos en base a dos 
variables distintas ������
✓ sns.catplot con el parámetro kind='violin' 
permite comparar distribuciones 
separando los datos en base a dos 
categorías simultáneamente.
Desgregación en base a 
duración y dieta
✓ Visualicemos las pulsaciones en base a la 
duración de la actividad y al tipo de 
dieta:
ax = sns.catplot(data=df_ejercicio, kind='violin', x='time', y='pulse', hue='diet', split=True)
ax.set(xlabel='Duración de ejercicio', ylabel='Frecuencia cardíaca', title='Categorización de la 
distribución de pulsaciones')
Resultado
Desgregación en base a 
actividad y dieta
✓ Para visualizar en base a la actividad, 
únicamente cambiamos el valor del 
parámetro x
ax = sns.catplot(data=df_ejercicio, kind='violin', x='kind', y='pulse', hue='diet', split=True)
ax.set(xlabel='Duración de ejercicio', ylabel='Frecuencia cardíaca', title='Categorización de la 
distribución de pulsaciones')
Resultado
Data Science in a 
Nutshell, 
primeros conceptos: 
clasificación, 
regresión y clustering
Aprendizaje 
Supervisado
Aprendizaje 
Supervisado
¿Qué tipo de problemas 
resuelve?
✓ Problemas de clasificación:           ✓ Problemas de regresión:
   Necesitan predecir la clase más         En vez de predecir categorías, 
   probable de un elemento, en             predicen valores numéricos. Es 
   función de un conjunto de variables     decir, la variable target en un 
   de entrada. Para este tipo de           problema de regresión es de tipo 
   algoritmos, la variable target o        cuantitativa.
   respuesta, es una variable de tipo 
   categórica.
✓ Entonces, ¿cómo sé si tengo que utilizar 
un algoritmo de clasificación o de 
regresión? Depende del tipo de problema 
que plantea mi variable a predecir ������
Aprendizaje Supervisado: 
Clasificación
Los algoritmos de clasificación 
intentan predecir una categoría.
Por ejemplo:
✓ Enfermo/No enfermo.
✓ Sobrevive/No sobrevive.
✓ Baja/ No baja
Aprendizaje Supervisado: 
Regresión
✓ Planteamos la hipótesis de que 
podría existir algún tipo de 
dependencia de una variable con 
respecto a la otra. 
✓ Si este tipo de dependencia existe, 
queremos ver de qué forma se 
da esa relación. 
Aprendizaje Supervisado: 
Regresión
Supongamos entonces, que tenemos dos 
variables: x e y, veamos el siguiente 
gráfico:
Pareciera que las variables tienen una 
fuerte correlación positiva, y si lo 
pensamos en términos de dependencia, 
quiere decir que cuando la variable x 
aumenta, entonces también lo hace la 
variable y, y viceversa.
¡Atención!
Cuando planteamos que ante un cambio 
en la variable x se produce un cambio en 
la variable y. A esto lo llamaremos 
dependencia de la variable y hacia la 
variable x.
Como una función 
matemática estándar
✓                 donde la variable y es una 
función de x, o sea que en definitiva 
y depende del cambio de x.
✓ Otra forma de decir lo mismo es que 
x es una variable independiente, o 
sea que su cambio no depende de 
nuestro modelo.
  Repaso
✓                              ������ donde a y b son 
números reales.
✓ Esta función genera una recta en el 
plano.
✓ El valor de a (ordenada al origen) 
muestra cuál es el valor de y cuando 
x vale 0.
✓ El valor de b (pendiente), por su 
parte, indica el grado de inclinación 
de la recta.
¡Importante!
✓ Una recta totalmente horizontal > tiene una 
pendiente igual a cero. 
✓ Una recta inclinada en el sentido de la 
correlación positiva > tiene una pendiente 
positiva. 
✓ Una recta inclinada en el sentido de la 
correlación negativa > tiene una pendiente 
negativa.
✓ Una recta vertical > tiene pendiente infinita.
Aprendizaje no 
Supervisado
        Aprendizaje no 
          Supervisado
Infieren patrones de 
un conjunto de 
datos sin referencia 
a resultados 
conocidos o 
etiquetados, esto 
significa en otras 
palabras, que no 
cuenta con una 
variable “y” solo tengo 
“X”.
Aprendizaje Supervisado 
vs. no Supervisado
                        Supervisado                                  No supervisado
                  input data       Anotacione                           input data
                                   s
                                       Son 
                                    manzana
                                        s
                                             ?
             Predicción        es una 
                             manzana
Clustering
Tipos de algoritmos no 
Supervisados
El aprendizaje supervisado se compone de 
dos grande tipos de problemas: 
✓ Clustering donde se busca encontrar    ✓ Reducción de dimensionalidad 
   grupos subyacentes en los datos, con      que busca encontrar pocas 
   algoritmos basados en Jerarquías,         dimensiones como combinaciones 
   Particiones, Densidad, Modelos o          lineales de las variables originales, 
   Grillas.                                  con algoritmos como el PCA; ICA; t-
                                             SNE, Isomap. Análisis Factorial entre 
                                             otros. 
K-means
K-means funciona como un algoritmo de                             REEMPLAZAR 
clustering utilizando particiones.                                POR IMAGEN
                  DBSCAN
 REEMPLAZAR       DBSCAN funciona como un algoritmo de 
 POR IMAGEN       clustering utilizando densidad.
ICA
El ICA (Independent Component                                         REEMPLAZAR 
Analysis) busca encontrar estructuras                                 POR IMAGEN
independientes de una señal que 
puede ser una serie de tiempo o 
algún fenómeno de interés. 
Para pensar
Supongamos que una empresa lanza una campaña 
de marketing para encontrar los diferentes 
segmentos para un nuevo producto. 
¿Que tipo de aprendizaje deberíamos utilizar 
en este caso? ¿Por qué? 
Contesta en el chat de Zoom 
Introducción a
Scikit-Learn
Scikit Learn
Scikit-learn es probablemente la librería 
más útil para Machine Learning en 
Python, es de código abierto y es 
reutilizable en varios contextos. 
Proporciona además una gama de 
algoritmos de aprendizaje 
supervisados y no supervisados en 
Python.
Este librería está construida sobre SciPy 
(Scientific Python) e incluye las 
siguientes librerías o paquetes:
Paquetes de Scikit 
Learn
                                                            NumPy: 
             Ipython:                                       librería de matriz n-
             consola interactiva mejorada                   dimensional base
             SciPy:                                         Matplotlib: 
             librería fundamental para la                   trazado completo 2D
             informática científica
             SymPy:                                         Pandas: 
             matemática simbólica                           estructura de datos y análisis
¿Por qué elegir Scikit 
Learn?
✓ Clustering.                              ✓ Extracción y selección de 
✓ Ensemble methods, es decir,                características de imágenes, 
                                           texto así como también para 
 algoritmos de aprendizaje                 identificar atributos significativos a 
 supervisados y no supervisados.           partir de los cuales crear modelos 
✓ Validación cruzada, es decir,              supervisados.
 dispone de varios métodos para          ✓ Reducción de la dimensionalidad.
 verificar la precisión de los modelos 
 supervisados.                           ✓ Optimización o ajuste de hiper 
✓ Varios conjuntos de datos o datasets       parámetros.
 de prueba.                              ✓ Feature selection.
 Ventajas
                    2
            Práctica 
        integradora: 
    Visualizaciones en 
             Python
    DESAFÍO 
    ENTREGABLE
Visualizaciones en 
Python                                          Formato
Consigna
 ✓ Deberás entregar el segundo avance de        ✓ Entregar un archivo con formato .ipynb. 
    tu proyecto final. Elegirás uno de los          Debe tener el nombre 
    datasets del desafío “Elección de               “Visualización+Apellido.ipynb”. 
    Potenciales Datasets e importe con 
    la librería Pandas”. Posteriormente,       Sugerencias
    crearás un notebook donde cargaran el       ✓ Preparar el código y probar los resultados 
    archivo utilizando funciones de pandas          con subconjuntos del conjunto original.
    para luego proceder a realizar 3 gráficos 
    diferentes con Matplotlib y 3 con          Aspectos a incluir
    Seaborn. Finalmente, cada gráfico será      ✓ El código debe estar hecho en un 
    interpretado con el fin de obtener              notebook y debe estar probado.
    insights relevantes que permitan dar 
    respuesta a la pregunta problema.
       DESAFÍO 
       ENTREGABLE
 Visualizaciones en 
 Python
Consigna paso a paso                                      Video explicativo
  1.  Escoger uno de los 3 datasets utilizados              ✓ Link al video complementario
      para la Clase 5
  2.  Cargar el dataset con la librería pandas 
      por medio de la función pd.read_csv() o 
      pd.read_excel()
  3.  Realizar al menos tres gráficos (lineplot, 
      scatterplot, histogramas, barchart, 
      boxplot) usando la librería Matplotlib
  4.  Realizar al menos tres gráficos (lineplot, 
      scatterplot, histogramas, barchart, 
      boxplot) usando la librería Seaborn
  5.  Interpretar los resultados de cada gráfica 
      obtenida
CLASE N°8
Glosario
Seaborn: librería de Python que permite la       Aprendizaje supervisado: tipo de 
creación de gráficos con mejor calidad que       aprendizaje donde existe retroalimentación 
Matplotlib ya que interactúa de manera           para el algoritmo debido a que hay 
más eficiente con estructuras de datos           etiquetas, los problemas típicos a resolver 
                                                son clasificación y regresión
Estructura axes-level: no se pueden              Aprendizaje no supervisado: tipo de 
modificar más allá de los ejes que se            aprendizaje donde NO  existe 
grafican y no es conveniente cuando se           retroalimentación para el algoritmo debido 
desea hacer múltiples subplots (e.g.             a que NO hay etiquetas, los problemas 
lineplot, scatterplot)                           típicos a resolver son clustering y reducción 
                                                de dimensionalidad
Estructura figure-level: leyendas por            ScikitLearn: librería fundamental para el 
fuera de los frames y permiten la creación       desarrollo de modelos de Machine Learning 
de múltiples subplots de manera más              (Algoritmos Supervisados y No 
rápida y eficiente (e.g relpot)                  supervisados entre otros) en Python
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Seaborn comandos básicos…
      ✓ Tipos de gráficos
      ✓ Customizaciones 
      ✓ Clasificación, regresión y clustering 
      ✓ Introducción a paquetes de Sclikit Learn 
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 06. DATA SCIENCE
  Introducción a la 
  manipulación de 
 datos con Pandas 
       (Parte II)
Temario
               05                      06                     07
         Programación          Introducción a la       Visualizaciones 
           con arrays:         manipulación de           y primeros 
         Introducción a            datos con 
                               Pandas (Parte II)       pasos con DS I
       ✓     Numpy              ✓ Fuentes de datos 
          Estructura de datos
       ✓ Numpy y ndarrays       ✓ Series y data frame   ✓ Matplotlib
       ✓ Tipos de datos         ✓ Selección de datos    ✓ Tipos de 
       ✓ Indexado y acceso      ✓ Operaciones              gráficos
       ✓ Operaciones básicas    ✓ Agregaciones
       ✓                        ✓ Strings               ✓ Customizacione
          Operaciones vectorizadas                         s sobre gráficos 
Objetivos de la clase
         Conocer las estructuras de datos en Pandas
         Comprender el uso de Pandas para la 
         manipulación de grandes volúmenes de datos
MAPA DE CONCEPTOS
                      Series
Estructuras de 
datos de Pandas
                      Data Frames         Lectura desde 
                                          archivos
                      Selección de 
                      elementos
Manipulación de                            Tratamiento de 
datos con Pandas                           datos ausentes
                      Operaciones
                                          Agregaciones
Introducción a         Conversión a        Operaciones sobre 
las Series de          Objetos de Tiempo   Strings
TIempo
Repaso en Kahoot
Ejemplo en vivo
Uso de filtros para obtener 
comportamiento de una acción
Aplicando conceptos de variables, índices 
booleanos y filtros. Los datos se 
encuentran en: 
https://raw.githubusercontent.com/JJTorresD
S/stocks-ds-edu/main/stocks.csv
Introducción a la librería en 
Pandas
           Pandas Data            Pandas     Manejo de datos nulos
              Frame                Series
Fuentes de
datos disponibles
Fuentes de datos 
disponibles
En esta pagina encontrarás muchas más APIs para descargar información: 
https://github.com/public-apis/public-apis#finance
Series y 
Data Frame
Recall: Pandas 
Series
Pandas Series
… como los Numpy arrays, pero con índices
 ✔ Se construyen a partir de otros 
     objetos particulares, como las listas 
     o los Numpy arrays
 ✔ Tienen índice propio.
     Numeros = range(50, 70, 2)
     Numeros_serie = pd.Series(Numeros)              ������
     print(Numeros_serie)
     print(Numeros_serie[2])                         ������
Pandas Series (Resumen)
                    En resumen, las series son objetos 1D que 
                    tienen 3 componentes principales.
                     ✔ Índices: Pueden ser cualquier tipo de 
                      variable
                     ✔ Valores: Asociados a cada índice
                     ✔ Nombre: Que caracteriza a la serie
Recall: Pandas Data 
Frames
Pandas Data Frames
Construyendo un tablero de ajedrez con 
Panda Data Frames ♟ 
Ajedrez_64 = np.arange(1,65).reshape(8,8)
Ajedrez_df = pd.DataFrame(
Ajedrez_64,
columns=range(1,9),          ������
index=['A','B','C','D','E','F','G','H']
) 
Ajedrez_df
Pandas DataFrames 
(Resumen)
                    En resumen las series son objetos 2D 
                    (filas x columnas) con las siguientes 
                    propiedades:
                     ✔ Índices: Pueden ser cualquier tipo de 
                      variable
                     ✔ Valores: Asociados a cada índice y a 
                      cada columna
                     ✔ Nombre: Que caracteriza al 
                      Dataframe
                    ¡Son una generalización de series!
Selección de
datos
Recall: Selección de datos
Recordemos como se hacía selección de elementos en Series y DataFrames
                          ¿Recuerdan la diferencia 
                             entre .loc y .iloc?
Recall: Selección de datos
Recordemos como se hacía selección de elementos en Series y DataFrames
Operaciones con
datos
Manejo de datos 
nulos
                   Manejo de datos 
                   nulos
                    ✔ Los datos nulos son uno de los problemas más 
                      comunes a los que se enfrenta un Data Scientist 
REEMPLAZAR          ✔
POR IMAGEN            Son un problema porque muchos algoritmos no 
                      están diseñados para trabajar con ellos y pueden 
                      disminuir el performance en general de las 
                      tareas de aprendizaje automático
                    ✔ Son más comunes de lo que las personas 
                      piensan.
                    ✔ Pueden surgir por malos esquemas de muestreo, 
                      falla de sensores, renuencia a responder en 
                      encuestas, malos diseños de captura de datos
Manejo de datos nulos
    Para poder manejar estos datos, en       ✔ Reemplazar por un valor 
    general, se usan las siguientes              seleccionado al azar de los 
    estrategias:                                 otros registros:
      ✔ Introducir un valor constante             ○   Usar la media, mediana 
         para los nulos o una categoría               o moda para rellenar el 
         llamada Desconocido en                       valor
         variables categóricas                    ○   Valor estimado usando un 
                                                      modelo 
Podemos aplicar estas técnicas 
de forma manual o usando 
SimpleImputer de ScikitLearn
Manejo de datos nulos
¡Les pedimos que abran el documento 
correspondiente en Google Collab para 
que podamos trabajar juntos el ejemplo 
en vivo sobre cómo trabajar datos nulos!
  Manejo de datos nulos: 
  Forma Manual
   Si  queremos  reemplazar  las  columnas 
   numéricas por media podemos hacer esto                                 0      1     2     3      4     5      6   7    8
                                                                      0  6.0  148.0  72.0  35.0    NaN  33.6  0.627  50  1.0
                                                                      1  1.0   85.0  66.0  29.0    NaN  26.6  0.351  31  NaN
url='https://raw.githubusercontent.com/jbrownlee/                      2  8.0  183.0  64.0   NaN    NaN  23.3  0.672  32  1.0
Datasets/master/pima-indians-diabetes.csv'                   ������        3  1.0   89.0  66.0  23.0   94.0  28.1  0.167  21  NaN
df= pd.read_csv(url,sep=',', header=None)                              4  NaN  137.0  40.0  35.0  168.0  43.1  2.288  33  1.0
print(df.shape)
                                                                           0      1     2      3       4     5     6   7    8
# reemplazar con la media                                              0  6.00  148.0  72.0  35.00  155.55  33.6  0.63  50  1.0
                                                                      1  1.00   85.0  66.0  29.00  155.55  26.6  0.35  31  1.0
df.fillna(df.mean(), inplace=True)                          ������         2  8.00  183.0  64.0  29.15  155.55  23.3  0.67  32  1.0
print(df.shape)                                                        3  1.00   89.0  66.0  23.00   94.00  28.1  0.17  21  1.0
                                                                      4  4.49  137.0  40.0  35.00  168.00  43.1  2.29  33  1.0
  Manejo de datos nulos: 
  SimpleImputer
   Si  queremos  reemplazar  las  columnas 
   numéricas por media podemos hacer esto
                                                                        0      1     2     3      4     5      6   7    8
                                                                    0  6.0  148.0  72.0  35.0    NaN  33.6  0.627  50  1.0
                                                                    1  1.0   85.0  66.0  29.0    NaN  26.6  0.351  31  NaN
url='https://raw.githubusercontent.com/jbrownlee/             ������     2  8.0  183.0  64.0   NaN    NaN  23.3  0.672  32  1.0
                                                                    3  1.0   89.0  66.0  23.0   94.0  28.1  0.167  21  NaN
Datasets/master/pima-indians-diabetes.csv'                           4  NaN  137.0  40.0  35.0  168.0  43.1  2.288  33  1.0
df= pd.read_csv(url,sep=',', header=None)
print(df.shape)
# reemplazar con la media                                            Missing: 0
                                                                          0      1     2      3       4     5     6     7    8
valores = df.values #numpy array con los valores              ������     0  6.00  148.0  72.0  35.00  155.55  33.6  0.63  50.0  1.0
imputador = SimpleImputer(missing_values=np.nan,                     1  1.00   85.0  66.0  29.00  155.55  26.6  0.35  31.0  1.0
                                                                    2  8.00  183.0  64.0  29.15  155.55  23.3  0.67  32.0  1.0
strategy='mean') #definir el imputador                               3  1.00   89.0  66.0  23.00   94.00  28.1  0.17  21.0  1.0
# transformar el dataset                                             4  4.49  137.0  40.0  35.00  168.00  43.1  2.29  33.0  1.0
transformados = imputador.fit_transform(valores)
transformados=pd.DataFrame(transformados)
print(transformados.head().round(2))
Manejo de datos nulos: 
SimpleImputer
Si  queremos  reemplazar  las  columnas 
numéricas  por  mediana  podemos  hacer 
esto
url='https://raw.githubusercontent.com/jbrownlee/       0      1     2     3      4     5      6   7    8
Datasets/master/pima-indians-diabetes.csv'          0  6.0  148.0  72.0  35.0    NaN  33.6  0.627  50  1.0
df= pd.read_csv(url,sep=',', header=None)      ������   1  1.0   85.0  66.0  29.0    NaN  26.6  0.351  31  NaN
                                                 2  8.0  183.0  64.0   NaN    NaN  23.3  0.672  32  1.0
print(df.shape)                                     3  1.0   89.0  66.0  23.0   94.0  28.1  0.167  21  NaN
                                                 4  NaN  137.0  40.0  35.0  168.0  43.1  2.288  33  1.0
# reemplazar con la mediana
valores = df.values #numpy array con los valores
imputador = SimpleImputer(missing_values=np.nan,        0      1     2     3      4     5     6     7    8
strategy='median') #definir el imputador            0  6.0  148.0  72.0  35.0  125.0  33.6  0.63  50.0  1.0
                                                 1  1.0   85.0  66.0  29.0  125.0  26.6  0.35  31.0  1.0
# transformar el dataset                       ������   2  8.0  183.0  64.0  29.0  125.0  23.3  0.67  32.0  1.0
transformados = imputador.fit_transform(valores)    3  1.0   89.0  66.0  23.0   94.0  28.1  0.17  21.0  1.0
transformados=pd.DataFrame(transformados)           4  4.0  137.0  40.0  35.0  168.0  43.1  2.29  33.0  1.0
print(transformados.head().round(2))
  Manejo de datos nulos: 
  SimpleImputer
   Si  queremos  reemplazar  las  columnas 
   numéricas por moda podemos hacer esto
url='https://raw.githubusercontent.com/jbrownlee/                        0      1     2     3      4     5      6   7    8
                                                                    0  6.0  148.0  72.0  35.0    NaN  33.6  0.627  50  1.0
Datasets/master/pima-indians-diabetes.csv'                    ������     1  1.0   85.0  66.0  29.0    NaN  26.6  0.351  31  NaN
df= pd.read_csv(url,sep=',', header=None)                            2  8.0  183.0  64.0   NaN    NaN  23.3  0.672  32  1.0
print(df.shape)                                                      3  1.0   89.0  66.0  23.0   94.0  28.1  0.167  21  NaN
                                                                    4  NaN  137.0  40.0  35.0  168.0  43.1  2.288  33  1.0
# reemplazar con la moda
valores = df.values #numpy array con los valores                         0      1     2     3      4     5     6     7    8
imputador = SimpleImputer(missing_values=np.nan,                     0  6.0  148.0  72.0  35.0  105.0  33.6  0.63  50.0  1.0
strategy='most_frequent') #definir el imputador               ������     1  1.0   85.0  66.0  29.0  105.0  26.6  0.35  31.0  1.0
                                                                    2  8.0  183.0  64.0  32.0  105.0  23.3  0.67  32.0  1.0
# transformar el dataset                                             3  1.0   89.0  66.0  23.0   94.0  28.1  0.17  21.0  1.0
transformados = imputador.fit_transform(valores)                     4  1.0  137.0  40.0  35.0  168.0  43.1  2.29  33.0  1.0
transformados=pd.DataFrame(transformados)
print(transformados.head().round(2))
Agregaciones con
Pandas
Recall: Agregaciones
De la clase anterior recordemos que ...           ✔ Las agregaciones son un tipo de 
                                                    operación.
                                                 ✔ Se realizan sobre un conjunto de 
                                                    datos.
                                                 ✔ Retornan un resultado que es una 
                                                    medida resumen del conjunto de 
                                                    datos
                                                 ✔ Las principales agregaciones de 
                                                    Numpy son:  
                                                 ✔ np.sum, np.mean, np.max, np.std, 
                                                    np.var
Agregaciones en Pandas
Pandas permite realizar agregaciones 
sobre Data Frames enteros o porciones 
del mismo.                                   df_lluvias_archivo = 
En primer lugar, importemos nuestro          pd.read_csv('<ruta>/pune_1965_to_2002.csv')
dataset de prueba:
1. Descargue el archivo con 
    formato .csv desde este enlace.
2. Copie la ruta del archivo y pásela al 
    método read_csv de Pandas:
Lectura de Datasets
Hasta el momento, nuestro dataset luce         ✔ Se trata de mediciones de 
así:                                              precipitaciones (en milímetros)
                                              ✔ Existe un total de trece columnas, 
                                                 una para el año y otras doce para 
                                                 cada uno de los meses
                                              ✔ Tiene un índice numérico.
                                            Sería conveniente que el índice sea la 
                                            columna Year ������ 
                                            Construyamos un nuevo Data Frame con 
                                            este índice
Lectura de Datasets
indice = list(df_lluvias_archivo.Year)                  ������
indice
columnas = df_lluvias_archivo.columns[1:]               ������
columnas
Guardamos en un arreglo todos los 
valores, excepto los de la primera              ������          valores = df_lluvias_archivo.values[:,1:]
columna
Lectura de Datasets
Ensamblamos las partes… ������������
                                      ¡Y listo!
df_lluvias = pd.DataFrame(valores,index=indice,columns=columnas)
df_lluvias
                                      Ya tenemos 
                                      preparado nuestro 
                                      Data Frame
Agregaciones en Pandas
✔ Suma de las precipitaciones para cada mes.     df_lluvias.sum()
                                          ������
✔ Promedio de precipitaciones de cada año.
   df_lluvias.mean(axis='columns')   ������
El método Describe
El método describe
Este método nos ayuda a…   ������ df_lluvias.describe().round(1)
✔ Obtener un breve resumen del Data 
Frame con describe()
✔ Redondear los valores de un Data 
Frame con el método round()
El método describe
✔ Si transponemos el Data Frame       ������    df_lluvias.T.describe().round(1)
   antes de aplicar describe, obtenemos 
   el resumen según el año
El método Groupby
El método groupby
                    ✔ La función groupby () se utiliza para 
                      dividir  los  datos  en  grupos  según 
                      algunos criterios. 
                    ✔ Los objetos pandas se pueden dividir 
                      en cualquiera de sus ejes. 
                    ✔ Se puede realizar cualquier operación 
                      con  las  agrupaciones  siempre  y 
                      cuando el tipo de dato lo permita
El método groupby
                                                                             Jan    Feb    Mar
                                                                  Year                     
 El  método  groupby  nos  permite  agrupar                       1965  0.029  0.069  0.000
 por    una    o    más     categorías    y                       1966  0.905  0.000  0.000
 posteriormente  aplicar  una  función  de                        1967  0.248  3.390  1.320
 resumen  por  ejemplo  (media,  mediana,                         1968  0.318  3.035  1.704
 moda, max, min).                                                 1969  0.248  2.524  0.334
                                                                  1970  0.070  0.000  0.001
                                                                  1971  0.000  0.000  0.000
                                                                  1972  0.000  0.029  0.000
                                                                  1973  0.000  2.969  0.234
print(df_lluvias_archivo.groupby('Year')                   ������     1974  0.000  0.000  6.427
['Jan','Feb','Mar'].mean().head(15))                              1975  0.333  0.585  0.000
                                                                  1976  0.000  0.000  5.993
                                                                  1977  0.000  2.981  3.289
                                                                  1978  0.061  4.197  4.004
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Operaciones con
Strings
Operaciones con 
Strings
✔ A menudo, tendremos que trabajar con 
datos en forma de Strings, es decir 
cadenas de caracteres o texto. 
✔ Es muy probable que no tengan el 
formato requerido
✔ Pandas provee métodos para 
manipular Strings masivamente
Operaciones con 
Para estos ejemplos, usaremos el dataset de presidentes de EEUU, ¡trabajaremos juntos en 
Strings
vivo! 
1) Descargue el archivo .csv en este enlace.
   Presidentes_archivo = pd.read_csv('<ruta>/us_presidents 
   2.csv')
2) Seleccione la columna president
   Presidentes_nombres = 
   pd.Series(Presidentes_archivo['president'])
   Presidentes_nombres
Operaciones con 
Strings
 ✔ Convertir a mayúsculas
Veamos algunos ejemplos...
       Presidentes_nombres.str.uppe       ������
       r()
 ✔ Longitud total, incluyendo espacios y otros caracteres que puedan aparecer
      Presidentes_nombres.str.len(        ������
      )
Operaciones con 
Strings
  ✔ Evaluar si comienzan con una determinada letra
Veamos algunos ejemplos...
      Presidentes_nombres.str.startswith('J     ������
      ')
  ✔ Separar en una lista
      usando el espacio como separador
      Presidentes_nombres.str.split())          ������
Introducción a 
Series de Tiempo
Series de Tiempo
… datos, ligados al tiempo  
✔ Son tipos de datos especiales donde el tiempo toma un rol fundamental.
✔ Observamos cambios en los valores de la variable a lo largo del tiempo.
✔ Si ignoramos esa dimensión temporal, los valores pierden contexto.
✔ Son variables aleatorias indexadas por el tiempo
Series de Tiempo
… datos, ligados al tiempo ������ 
Python provee tres tipos de datos relacionados al tiempo:
 ✔ Time stamp o marca de tiempo: representan un punto en el tiempo. 
   Por ejemplo, fecha y hora.
 ✔ Período: representan un intervalo de tiempo. Por ejemplo, los minutos 
   transcurridos desde que comenzó la clase hasta ahora.
 ✔ Duración: representa una duración medida en tiempo, pero 
   independientemente del momento en que sucede. Por ejemplo, 15 
   minutos.
Series de Tiempo
… datos, ligados al tiempo.
Por su parte, Pandas provee un objeto índice para cada uno de esos objetos temporales:
      Tipo de dato        Objeto en Python      Índice en Pandas
  Time stamp            Timestamp             DateTimeIndex
  Período               Period                PeriodIndex
  Duración              Timedelta             TimeDeltaIndex
Ejemplo en vivo
Comprendamos el uso y manipulación de 
objetos tipo serie de Tiempo en Python
Operando objetos de 
tiempo
Operando objetos de 
tiempo
✔ Convertir String a Timestamp:
Veamos algunos ejemplos... 
fecha = pd.to_datetime('03/01/2020',dayfirst=True)
fecha                           ������
✔ Días desde el 3 de enero al del 2020 al 10 de enero del 2020:
fin = pd.to_datetime('10/01/2020',dayfirst=True)
fechas_1 = pd.date_range(start=fecha, end=fin)
Operando objetos de 
tiempo
✔ Ocho fechas desde el 3 de enero de 2020, con períodos:
Veamos algunos ejemplos... 
fechas_2 = pd.date_range(start=fecha, periods=8)
fechas_2
✔ La frecuencia por defecto es de un día. Por lo tanto, ocho períodos representan ocho 
días.
                                            ������
Operando objetos de 
tiempo
✔ Cambiando la frecuencia a meses en lugar de días:
Veamos algunos ejemplos... 
fechas_3 = pd.date_range(start= fecha, periods= 8, freq='M')
fechas_3
������
✔ Notar que como día se toma el último de cada período
Operando objetos de 
tiempo
✔ Ocho meses consecutivos, a partir del mes de inicio:
Veamos algunos ejemplos... 
     mes_inicio = fecha.strftime('%Y-%m')           ������
     mes_inicio
     fechas_4 = pd.period_range(start=mes_inicio, periods=8, freq='M')
     fechas_4
 ������
Operando objetos de 
tiempo
Veamos algunos ejemplos... 
 ✔ ¿Cuánto tiempo pasó desde el primer periodo al último?
    cuanto_tiempo = fechas_3[7] - fechas_3[0]           ������
    cuanto_tiempo
¡Al utilizar operadores normales sobre objetos de tiempo, obtenemos como resultado objetos 
                                            de tiempo! 
Operando objetos de 
tiempo
Veamos algunos ejemplos... 
✔ ¿Cuántos meses pasaron desde el primer periodo al último?
                          ������
cuanto_tiempo_meses = fechas_3[7].to_period('M') - fechas_3[0].to_period('M') 
cuanto_tiempo_meses
                 ������
Conversión a 
DateTimeIndex
Conversión a 
DateTimeIndex
Ahora que sabemos manipular objetos de tiempo, retomemos el Data 
Veamos algunos ejemplos...
Frame de presidentes. Seleccionamos las fechas de asunción
 fechas_presidentes_orig = Presidentes_archivo['start']               ������
 fechas_presidentes_orig
 type(fechas_presidentes_orig)         ������                                        ������
Conversión a 
DateTimeIndex
Transformemos las fechas en formato string a índices de tiempo
Veamos algunos ejemplos...
fechas_presidentes = pd.DatetimeIndex(fechas_presidentes_orig)
fechas_presidentes
������                                               ✔ 
Conversión a 
DateTimeIndex
Ahora que tenemos las fechas en el tipo de dato correcto, construyamos la Serie
Veamos algunos ejemplos...
Serie_presidentes = pd.Series(Presidentes_nombres.values,index=fechas_presidentes)
Serie_presidentes
                        ¡Listo!
                        Ya podemos ejecutar operaciones con 
                        objetos de tiempo.
 Manipulación de 
  Dataframes con 
      Pandas
     Duración: 15 minutos
 ACTIVIDAD EN CLASE
Manipulación de  Trabajaremos de forma individual. 
Dataframes con                                  Tiempo estimado: 15 minutos.
Pandas
✔ Ir al siguiente repositorio y descargar el siguiente archivo (BTCUSD_1hr.csv): Bitcoin
✔ Cargar el archivo usando la función pd.read_csv() 
✔ Utilizar el método .describe() para obtener un resumen numérico rápido de las variables
✔ Obtenga la cantidad de nulos por medio del atributo .isna()
✔ Extraer el mes de la columna Date y utilizar el método .groupby() para calcular la media mensual para cada 
variable
✔ Hacer un gráfico de los precios de Bitcoin y analizar tendencias. Que medida se les ocurre pueda establecer la 
volatilidad cada día?
CLASE N°6
Glosario
Manejo de datos nulos: conjunto de técnicas que nos permiten manipular y trabajar 
con los datos nulos (e.g Imputación, eliminación, reemplazo por un valor representativo 
como la mediana)
Método Describe: método incorporado en los dataframes para obtener un resumen 
numérico básico de las variables numéricas presentes (conteo, min, max, media, 
cuartiles, desviación estándar)
Método Groupby: método incorporado en los dataframes que permite agrupar los 
datos por agluna categoría específica y aplicar alguna función de agregación 
Operaciones con strings: son todas aquellas herramientas que nos permiten 
manipular datos con formato str, las librerías más comunes para esto son re y los 
métodos incorporados de str.
CLASE N°6
Glosario
Serie de tiempo: cualquier variable aleatoria indexada por el tiempo que se 
caracterizan por tener métodos especiales de análisis (e.g Modelos ARIMA, SARIMAX) 
Timestamp: tipo de dato ligado a series de tiempo, bastante común y se reconoce 
con formato DateTimeIndex en Python
Period: tipo de dato ligado a series de tiempo que se reconoce con formato 
PeriodIndex en Python
TimeDelta: tipo de dato ligado a series de tiempo que se reconoce con formato 
TimeDeltaIndex en Python
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Operaciones con datos en Pandas
      ✓ Agregaciones con Pandas
      ✓ Manejo de datos ausentes
      ✓ Operaciones con Strings
      ✓ Introducción a Series de Tiempo
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 16. DATA SCIENCE
Estudios de casos de 
Modelos Analíticos I
Temario
                   15                         16                         17
               Modelo                   Estudios de                 Estudios de 
           analítico para                casos de                     casos de 
                 DS II                    modelos                     modelos 
                                        analíticos I                analíticos II
            ✓ Modelo analítico         ✓ Casos de 
                                           éxitos con             ✓ Introducción
            ✓ Reglas de                    ciencias de 
                asociación                 datos                  ✓ Casos de 
            ✓ Reducción de             ✓ Armado de                    accidentes
                dimensionalidad            presentación 
                                           ejecutiva
Objetivos de la clase
        Analizar casos de éxito de la aplicación de la 
        Ciencia de Datos en diferentes industrias.
MAPA DE CONCEPTOS
                           Mazda              Clustering
                         San Cristóbal       Detección de 
                                               Fraudes
    Casos de Éxito con 
    Modelos Analíticos
                          Starbucks            IA e IoT
                         Como generar 
                         presentación 
                           ejecutiva
                         Caso Aplicado 
                          Empresa de 
                           Seguros
Casos de éxito con 
ciencia de datos
  ¿De qué trata la ciencia de 
  datos?  
  Sin dudas, la Ciencia de Datos es una            Existen múltiples definiciones al 
                                                   respecto. Sin embargo, elegimos esta 
  disciplina transversal a múltiples industrias,   definición para el desarrollo del 
  como ser por ejemplo: Finanzas, Retail,          curso ������
  Automotriz, Logística, Turismo, etc. 
  A lo largo de esta sesión, indagaremos 
  sobre algunos Casos de Éxitos basados en 
  diversas industrias de aplicación. 
Para pensar
¿Cómo usarían modelos analíticos (aprendizaje 
supervisado y no supervisado) en empresas para 
la resolución de problemas de una compañía 
cervecera?
Contesta mediante el chat de Zoom 
Caso Mazda
Mazda Motor 
Corporation
Mazda Motor Corporation es un 
fabricante de automóviles japonés, 
fundado en 1920, con sede principal en 
Hiroshima, y con plantas en las 
localidades de Hiroshima, Nishinoura, 
Nakanoseki y Miyoshi, Japón.
La organización implementó un 
algoritmo de Segmentación de Clientes, 
dentro de la aplicación del ámbito de 
estudio de la Ciencia de Datos. 
  ¿De que trata la ciencia de 
  datos?  
  Sin dudas, la Ciencia de Datos es una            Existen múltiples definiciones al 
                                                   respecto. Sin embargo, elegimos esta 
  disciplina transversal a múltiples industrias,   definición para el desarrollo del 
  como ser por ejemplo: Finanzas, Retail,          curso ������
  Automotriz, Logística, Turismo, etc. 
  A lo largo de esta sesión, indagaremos 
  sobre algunos Casos de Éxitos basados en 
  diversas industrias de aplicación. 
     Segmentación de clientes
     Este  tipo  de  algoritmo,  en  términos 
     generales,  busca  encontrar  grupos 
     homogéneos de clientes que respondan 
     de  modo  similar  a  determinadas 
     estrategias de marketing. 
     Tradicionalmente  la  forma  de  crear 
     estos    segmentos,     se    basa    en 
     seleccionar  atributos  del  perfil  del 
     cliente  (geográficas,  demográficas  y 
     socio-económicas  habitualmente)  para 
     establecer   ciertos   parámetros     de 
     similitud. 
    Algoritmos de Clustering
     Los    algoritmos    de   Clustering,       Este tipo de algoritmo pertenece al 
     permiten     analizar   cientos    de       Aprendizaje No Supervisado dentro 
     variables  de  cualquier  dimensión         del mundo del Machine Learning.
     del buyer persona e incluirlos como 
     atributos  válidos  para  encontrar 
     agrupaciones naturales de clientes.
     En consecuencia, se busca que los 
     grupos  resultantes  del  análisis  de 
     clusterización  sean  homogéneos 
     entre sí y diferenciados claramente 
     de  los  otros  clústeres  que  se 
     generen. 
BigML
Mazda  venía  utilizando  la  Analítica 
Avanzada desde hace años, y tenía muy 
claro que su estrategia debía estar basada 
en  una  plataforma  colaborativa  que 
cubriera todas sus necesidades. 
La  herramienta  que  se  utilizó  fue  BigML, 
que  permite  la  creación  de  Modelos  de 
Machine Learning auto - administrados o 
también conocidos como AutoML.
Segmentos Mazda
Una vez aplicados los más de 30 
atributos para obtener la 
segmentación de los clientes de 
Mazda, el algoritmo identificó 
claramente 5 segmentos de 
clientes bien diferenciados.
        Segmentos 
        Mazda
          1                                   2                                 3
  Compradores jóvenes con        Clientes con preferencia por la      Clientes más jóvenes con 
  poder adquisitivo medio        gama alta de la marca                coches del segmento 
  con preferencias por el        mostraban más interacción con        más accesible de los 
  segmento sub medio.            el servicio de Posventa, siendo      vehículos de la marca.  
                                 más exigentes en sus 
                                 interacciones. 
                                   
    Segmentos 
    Mazda
                           4                                  5
                    Clientes de un poder              Es el menos numeroso. 
                    adquisitivo menor y con           Son clientes jóvenes de 
                    vehículos más antiguos            poder adquisitivo medio 
                    de precio más                     pero con gran compromiso 
                    asequible.                        con la marca.
Resultados
Con esta información disponible, el 
Departamento de  Marketing de la empresa 
pudo orientar las campañas futuras de 
lanzamiento, adaptando las actividades de 
Marketing a cada uno de los segmentos y en 
especial a su público objetivo.
Para pensar
¿Qué otros casos exitosos de clustering 
conocen?¿Qué otros casos de éxito se les 
ocurren?
Contesta mediante el chat de Zoom 
Caso San Cristóbal
  Caso San Cristóbal
  San    Cristóbal    Seguros,    es    una 
  empresa  Argentina  nacida  hace  más            A su vez, brinda un amplio portfolio 
  de 80 años, con un origen mutualista.            de     coberturas    para     individuos, 
  Hoy el Grupo San Cristóbal, asegura a            productores              agropecuarios, 
  más  de  725.000  personas  y  cuenta            comercios,      pymes      y    grandes 
  con  presencia  física  en  Argentina  y         empresas.
  Uruguay.
  La  organización  destaca  por  ser  en 
  una empresa líder, caracterizada por 
  honrar  sus  acuerdos  y  generar  un 
  impacto positivo en la comunidad. 
         Detección de fraudes
         La empresa desarrolló su propia plataforma de 
         detección  de  fraudes  para  el  mundo  del  seguro, 
         estructurando  una  nueva  unidad  de  Prevención, 
         que emplea tecnología como Inteligencia Artificial, 
         Machine Learning y Ciencia de Datos.
         Como el campo de aplicación  es  tan  amplio,  se 
         incorporaron  especialistas  en  criminalística,  agro, 
         peritos,   científicos    de    datos,    entre    otros, 
         formándose un equipo multidisciplinario.
Para pensar
El Caso San Cristóbal ¿Se trata de aprendizaje 
supervisado o no supervisado? ¿Se trata de 
problemas de clasificación o de regresión?
Contesta mediante el chat de Zoom 
   ¿Qué se busca con el ML?
                                    ✓ Discernir entre los casos reales y los posibles 
                                        fraudes, lo cual genera un impacto directo en 
                                        el negocio central de la compañía. 
                                    ✓ Resulta importante mencionar, que este tipo 
                                        de  algoritmo  pertenece  al  Aprendizaje 
                                        Supervisado  modelos  de  Clasificación, 
                                        donde  se  trata  de  clasificar  la  variable 
                                        target:         Fraude/No           Fraude.
 Pasos para detectar 
 fraudes
         1                                         2
    Detección                                  Análisis
    Puede ser denunciado de forma              El equipo de analistas recibe los posibles 
    manual por cualquier agente de la          casos y separa "la paja del trigo", para 
    compañía o puede detectarlo                devolver al circuito de la compañía los falsos 
    directamente la herramienta de IA,         positivos o caso contrario, continuar con el 
    gracias a sus "reglas" predefinidas y      proceso de investigación. (Este concepto se 
    modelos predictivos.                       asocia a la Matriz de Confusión)
Pasos para detectar 
fraudes
       3                                     4
Investigación                             Resolución
La compañía conformó un equipo            Luego de realizar el análisis 
interno de investigación eficiente y      correspondiente, el equipo toma la 
profesional que se encarga de analizar    decisión de rechazar la denuncia y 
a fondo los casos. Cabe destacar que      evaluar la factibilidad de iniciar la 
cualquiera de las verticales de la        instancia judicial en caso de ser 
compañía puede pasar por el proceso       necesario.
de análisis. El tiempo estimado de 
demora es de 24/48 hs. 
    Análisis automático de 
    imágenes
    Como  parte  de  estos  modelos  de 
    detección,  San  Cristóbal  cuenta  con 
    una     plataforma     de     análisis   de 
    imágenes para vehículos siniestrados. 
    De  forma  independiente,  se  encarga 
    de catalogar las imágenes, la marca, 
    color y patente, para luego identificar 
    los daños. 
    Volviendo  a  la  plataforma  de  San 
    Cristóbal, en base a lo visto podríamos 
    decir  que se aplica el Deep Learning 
    para el análisis de imágenes.
Para pensar
En el Análisis automático de Imágenes ¿Qué 
modelo de Análisis de Datos se 
implementa?
Contesta mediante el chat de Zoom 
Resultados
Gracias a la implementación del modelo de 
predicción de fraude y de cara al futuro, la 
compañía está explorando nuevas alternativas 
de uso para su plataforma, tal como el análisis 
de los vehículos a la hora de tomar un seguro, 
así como también para la liquidación de 
siniestros, optimizando notablemente la 
experiencia del usuario.
Caso Starbucks
     Caso Starbucks
     Starbucks  recopila  enormes  cantidades  de 
     datos   de   más  de  100  millones  de 
     transacciones por semana. Cuenta con más de 
     30,000  tiendas  en  todo  el  mundo  y  realiza 
     cerca  de  100  millones  de  transacciones  por 
     semana. Esto le da una visión integral de lo 
     que consumen y disfrutan sus clientes. 
PARA RECORDAR
Contenido destacado
¿Cómo crees que se usan los datos de 
consumo? ¿Cómo impactan en la estrategia 
de posicionamiento en el mercado?
Veamos algunas tecnologías utilizadas para 
crear ventajas competitivas. ������
   Uso de IA e IoT 
   La forma en que Starbucks utiliza los 
   datos y la tecnología moderna para          Todos los ejemplos que 
   obtener una ventaja competitiva es          mencionaremos a continuación, 
   analizada por diversas empresas del         junto con tecnologías como AI, Data 
   mundo independiente del rubro de            Science, IoT y la nube, le permiten a 
   aplicación. Por ejemplo, la compañía es     la organización alcanzar los 
   pionera en combinar sistemas de             siguientes objetivos:
   fidelización, tarjetas de pago y 
   aplicaciones móviles, etc. 
Para pensar
En el caso Starbucks ¿Se podría implementar K-
Nearest Neighbor? ¿Se pueden aplicar reglas de 
asociación? ¿Qué algoritmo usarías?
Contesta mediante el chat de Zoom 
 Objetivos de IA e IoT 
   1. Dirigirse  a  clientes  con  promociones  y 
      ofertas personalizadas.
   2. Desarrollo  de  productos  orientados  a  las 
      necesidades de los clientes y usuarios.
   3. Planificación inmobiliaria sofisticada.
   4. Creación dinámica de menú y ajustes.
   5. Mantenimiento    optimizado   de   las 
      maquinarias de la compañía.
   1.Promociones 
   Conocer las preferencias individuales de los 
         Personalizadas
   pedidos de los clientes y los patrones de          Estos pueden incluir bebidas frías en 
   compra le permite a Starbucks enviar ofertas       días calurosos, lanzamientos de 
   personalizadas con mayor probabilidad de           productos o menús de temporada.
   ser relevantes. 
   Se siguen ofreciendo campañas masivas 
   convencionales, pero directamente a cada 
   consumidor en el segmento objetivo. 
2.  Productos orientados a 
las necesidades 
 El uso de los datos de clientes en el desarrollo de 
 su gama de productos le permite a la compañía      Esto se ha convertido en una gama 
 crear productos y servicios que sus “Clientes      completa de productos globales 
 amen”                                              inspirados en calabazas.
 Por ejemplo, hace más de 15 años surgió la idea 
 de introducir bebidas con sabor a calabaza en 
 Halloween. 
 3. Planificación inmobiliaria 
 sofisticada
 Planear dónde abrir una tienda Starbucks es     El sistema también considera la ubicación 
 ahora un complejo análisis de datos, donde se  de las tiendas existentes de Starbucks y se 
 incluyen factores asociados a la población,     basa en gran parte en el uso de datos 
 niveles de ingresos, tráfico, presencia de      geo-referenciados a través de los 
 competidores, etc.                              sistemas SIG (Sistemas de 
                                                 Información Geoespacial).
4. Menús 
dinámicos
La forma en que Starbucks utiliza los datos le permite 
que pueda realizar revisiones basadas en el 
cliente, la ubicación y la hora. Esto en 
consecuencia, afecta a productos, promociones y 
precios. Incluso, también es factible impulsar 
productos seleccionados según las circunstancias 
locales, como el clima o la hora del día.
5. Mantenimiento 
optimizado 
Gracias a la analítica de datos, es 
posible identificar de manera proactiva 
potenciales averías en las maquinarias de 
la empresa, de tal forma de realizar 
mantenimientos preventivos evitando 
costos y gastos para la organización.
         ☕
       Break
       ¡10 minutos y 
        volvemos!
          ¡Lanzamos la
          Bolsa de 
          Empleos!
         Un espacio para seguir potenciando tu carrera y 
         que tengas más oportunidades de inserción 
         laboral.
         Podrás encontrar la Bolsa de Empleos en el menú 
         izquierdo de la plataforma.
         Te invitamos a conocerla y ¡postularte a tu futuro 
         trabajo!
           Conócela
Caso Empresa 
Seguros
Ejemplo en vivo
¿Alguna vez han imaginado como una compañía de 
seguros establece si una reclamación es real o 
fraude?
Estudiaremos un caso real aplicado donde 
podremos ver el uso de del Aprendizaje No 
Supervisado (K-means)
Utilizaremos el notebook  Clase_16.ipynb dentro de 
la carpeta de clase.
Fases importantes para la 
resolución de Casos en DS
  1                      2                      3                    4                   5
Definición de            Contexto              Problema               Contexto            Exploración 
objetivo              comercial             comercial               analítico        de datos (EDA)
Fases importantes para la 
resolución de Casos en DS
  6                      7                      8                    9                  10
 Data              Selección del          Desarrollo de        Interpretación      Conclusiones
Engineering            algoritmo             algoritmo
                     apropiado
1.Definición de objetivo
¿Podemos detectar patrones entre 
consumidores específicos para identificar 
posibles fraudes?
Definir el objetivo es una de las 
etapas más importantes porque es 
donde decidimos que queremos 
hacer
2.  Contexto 
comercial
  Trabaja para el equipo de fraude en una         Sin embargo, muchas personas intentan 
  gran compañía de seguros que se ocupa           aprovecharse, presentando reclamos con 
  de emitir pólizas de seguro para varios         falsos pretextos para obtener ingresos 
  tipos de reclamos, tanto para individuos        adicionales. Es función del equipo de 
  como para empresas.                             fraude determinar qué reclamos 
  Las pólizas que se emiten se controlan y        presentados deben aprobarse y 
  cualquier reclamo presentado se examina         cuáles deben rechazarse.
  y evalúa para determinar la legitimidad y 
  la aprobación final para el pago por parte 
  de la compañía de seguros. 
3.  Problema 
comercial Su tarea es responder la siguiente 
                       pregunta: 
                       ¿Existen patrones particulares en los 
                       grupos de reclamos presentados que 
                       puedan ser indicativos de fraude?
4.  Contexto 
analítico
El equipo de contabilidad le ha 
proporcionado datos sobre todas las 
reclamaciones recientes realizadas por 
1000 personas. 
Los siguientes datos no estarán 
etiquetados; es decir, no hay una 
variable que nos diga cuáles de estas 
afirmaciones son fraudulentas o no. En su 
lugar, debemos utilizar modelos de 
agrupamiento para abordar este 
problema de aprendizaje no 
supervisado.
Ejemplo
5.  Exploratory Data 
Analysis (EDA)
                    Parece que hay tres conjuntos de picos 
                    para los montos de las reclamaciones.
                    Intentemos ver algunos diagramas de 
                    dispersión en 2D de claims frente a 
                    income e income frente a age para la 
                    población de muestra, para obtener más 
                    información sobre lo que está 
                    sucediendo.
Ejemplo
También podemos estratificar 
nuestros datos por género 
antes de realizar las 
visualizaciones anteriores:
Ejemplo
No parece haber grandes diferencias     1. Grupo gigante de reclamos alrededor del rango 
en las distribuciones basadas en el        de ingresos de $30,000-$40,000 (Valores 
género; sin embargo, las gráficas          típicos de ingreso)
muestran algunos grupos claros.         2. Una franja de reclamaciones entre $50.000- 
                                           $100,000 que valen $5.000 aproximadamente. 
                                           No está claro exactamente qué son, pero podrían 
                                           ser cosas cotidianas con las que las personas 
                                           más pudientes pueden lidiar (por ejemplo, 
                                           reclamos por accidentes automovilísticos). 
                                        3. Franja de reclamos por al menos $20,000 entre 
                                           las personas que ganan solo $ 10,000, lo cual es 
                                           inusual y bien puede consistir en reclamos 
                                           fraudulentos.
6.  Data Engineering (Min 
Max Scaling)
6.  Data Engineering (Min 
Max Scaling)
Ejemplo
El método usado transforma los datos de 
tal manera que la magnitud de todas las 
características sea aproximadamente 
similar. Sin embargo, puede haber un 
método de escalado diferente que 
permita que el clustering de k-means 
funcione mejor. La mejor elección de 
método de escala depende de los datos 
específicos proporcionados.
Ejemplo
7.  Selección del 
algoritmo apropiado (k-
means)
Este algoritmo separa el conjunto de 
datos dado en k grupos que 
minimizan la suma de los cuadrados 
de distancias entre cada par de 
puntos en el grupo. 
El científico de datos debe 
preseleccionar el valor de k, que 
puede requerir un análisis previo 
para determinarlo.
   Pasos
    1. Inicialización: Se debe seleccionar k           3 Actualización: Una vez que se 
        puntos (no necesariamente ninguno de               han asignado todos los puntos 
        los puntos de datos, solo puntos en el             de datos a sus respectivos 
        mismo espacio dimensional) como los                grupos, se calcula un nuevo 
        centroides "iniciales".                            centroide para cada grupo 
    2. Asignación: Cada punto de datos se                  tomando la media de todos los 
        asigna al grupo correspondiente al                 puntos en ese grupo.
        centroide más cercano a sí mismo              Luego, se repiten los pasos 2 y 3 
        (generalmente basado en la distancia          hasta que los grupos ya no 
        euclidiana estándar).                         cambien.
8.  
Desarrollo 
del 
algoritmo
  Ejemplo
  Aunque el análisis visual sugiere que 
  k=4 es el valor óptimo, es una buena 
  práctica          verificar          esto 
  cuantitativamente.         Esto        es 
  especialmente  importante  cuando  se 
  agrupan  datos  que  no  se  pueden 
  visualizar fácilmente, un hecho común 
  con       datos      multidimensionales 
  complejos.
Ejemplo
9.  Interpretación de 
resultados
1. El primero son aquellos con ingresos 
altos y reclamos bajos, que 
probablemente sean reclamos ordinarios 
hechos por familias acomodadas. Es muy 
probable que estos no sean fraudulentos 
y que la empresa los acepte.
2. El segundo son los ingresos 
moderados con valores de 
reclamación moderados. Estos son 
bastante abundantes y podrían ser 
elementos cotidianos como reclamos de 
automóviles. Lo más probable es que 
debamos aceptarlos.
 9.  Interpretación de 
 resultados
  3 El tercero son los ingresos moderados y     4 La última categoría es la de ingresos 
      las altas reclamaciones. Esto podría ser     bajos pero reclamaciones muy 
      plausible si es algo que las personas de     elevadas. Estos claramente no son 
      ingresos medios necesitan pero que no        asequibles y, con la excepción de algo 
      siempre pueden pagar, como ciertas           como las declaraciones de propiedades 
      declaraciones de propiedades                 saludables, bien podrían ser intentos 
      saludables. Así que probablemente            de obtener efectivo gratis. Lo más 
      deberíamos investigar esto más a fondo.      probable es que los rechacemos.
¿Algunas desventajas del k-means?
Deficiencias potenciales
1. El algoritmo k-means da más peso a los 
clústeres más grandes, ya que intenta 
minimizar la variación intra-clúster, por lo que 
en los casos en los que los clústeres no tienen 
naturalmente el mismo tamaño, esto podría 
resultar en resultados sesgados.
2. No necesariamente converge de forma uniforme 
o rápida, por lo que la elección de los puntos 
iniciales es sumamente importante. Este grado 
de dependencia no es necesariamente el ideal.
3. Los valores atípicos pueden influir en gran 
medida en los datos y las medias del grupo, a 
menos que se cumpla una alternativa.
10.  Conclusiones
En este caso, dividimos con éxito el conjunto de datos en 
4 grupos separados y validamos nuestra intuición basada 
en la agrupación de  kmeans. 
Aprendimos que hay muchos supuestos incorporados en 
la  agrupación  de  k-means  (es  decir,  la  elección  de  la 
métrica de distancia, la normalización y k) y que esto no 
se puede automatizar fácilmente, ya que estas opciones 
dependen del conjunto de datos particular en mano. 
Una vez que se han identificado los grupos individuales, 
la  compañía  de  seguros  ahora  puede  determinar  qué 
reclamos  investigar  más  a  fondo  o  considerar  de 
inmediato  como  fraudulentos  en  función  de  algunos 
criterios adicionales.
¡Para llevar!
En este caso, presentamos los conceptos de          La agrupación es una poderosa 
técnicas de aprendizaje no supervisado y            herramienta de aprendizaje no 
agrupación. En particular, discutimos el algoritmo  supervisada para investigar datos no 
k-means y su funcionamiento interno. Hicimos        etiquetados, particularmente para 
esto para:                                          determinar patrones que no se conocían o 
                                                  entendían previamente. En el caso de 
1. Visualización de un conjunto de datos para      grandes conjuntos de datos, se pueden 
   generar posibles agrupaciones en nuestros      identificar relaciones muy complejas 
   datos                                          entre grupos de puntos de datos 
2. Presentamos el concepto de k-means y cómo       individuales que posiblemente no se 
   funciona el algoritmo                          podrían hacer a través del ojo 
3. Alterar los datos para que se ajusten a los     humano.
   errores del algoritmo
4. Validamos nuestra hipótesis inicial de los 
   clusters en los datos mediante la 
   implementación del algoritmo
¡Para llevar!
Si bien esta es una buena técnica para datos de 
baja dimensión, k-means es susceptible a un 
rendimiento deficiente en dimensiones más altas, 
como muchos algoritmos de aprendizaje 
automático. En dimensiones más altas, primero se 
realizan técnicas más avanzadas, como t-SNE, 
para reducir el número de dimensiones antes de 
agrupar. Finalmente, k-means es solo uno de los 
muchos algoritmos de agrupamiento. Los 
desarrollos modernos incluyen los algoritmos 
DBSCAN y OPTICS.
Armado de la 
presentación 
ejecutiva
¡A tomar nota!
 Puntos importantes
                                               Hook  (Punto  de  enlace):  Presente  su 
 Introducción: Simple, quién es y por qué  propuesta  de  valor  y  comience  con  su 
 está  allí.  Dedicar  la  menor  cantidad  de  conclusión.  Hágales  saber  el  retorno  de  la 
 tiempo posible a presentarse a uno mismo y  inversión  (ROI)  y  el  impacto  en  el  resultado 
 sumar la información relevante que agregue  final desde el principio, para que se sienten y 
 contexto.                                     se  den  cuenta.  Tenemos  30  segundos  de 
                                               atención. Algunas sugerencias:
                                                ✓ Sorprender con estadísticas interesantes.
                                                ✓ Usa el humor. Aligera el estado de ánimo.
                                                ✓ Haz  preguntas  retóricas.  Hace  que  la 
                                                    audiencia   piense   sin  necesidad  de 
                                                    responder.
                                                ✓ Utilizar una cita inspiradora.
   Puntos importantes
   Situación actual: Explique dónde está el               Evidencia:  Sea  selectivo  y  elija  un 
   cliente.   Presentar  desafíos,  por  qué              conjunto  de  datos  que  llame  su 
   ocurren y el impacto que tienen. Hablar                atención.  ¿Qué  has  descubierto  que 
   en torno a métricas e indicadores clave de             necesitan  saber?  ¿Qué  significan  los 
   rendimiento que sean importantes para la               datos?
   audiencia.                                             Pensar en preguntas como: ¿Qué datos 
   Nuevas  oportunidades:  ¿Cómo  se                      usaste? ¿Cómo lo calculaste? ¿Cómo lo 
   puede resolver el problema o ayudarlos a               medimos? ¿Tiene  estudios  de  caso  o 
   hacerlo      aún     mejor?      Analice      las      testimonios  para  demostrar  el  éxito 
   oportunidades       que     ha     identificado,       anterior?
   incluidas las posibles ventajas. Considerar 
   el  riesgo  potencial  y  cualquier  estrategia 
   que tenga para mitigarlos.
  Puntos importantes
   Alternativas:     Hacer    saber    que    ha    Próximos pasos:  Dé una explicación 
   considerado otras opciones, cuáles eran y        sobre  cuándo  y  cómo  hará  que  esto 
   por  qué  no  son  la  opción  correcta.  Dos    suceda  y  los  próximos  pasos,  incluido 
   puntos  importantes:  a)  Qué  opciones          todo  lo  que  necesite  de  su  audiencia. 
   consideraron en el análisis, b) Cuál opción      Guardar los detalles.
   se eligió justificando el por qué.               El  gran  final:  Cerrar  la  historia  y 
   ¿Podría ser un buen lugar para hablar sobre      dejarlos inspirados y obligados a actuar 
   lo que sucede si no hacen nada?                  e interesarse por la idea transmitida.
   Recomendaciones: ¿Cuál de sus opciones 
   sugiere que sea el mejor curso de acción y 
   por qué? Sea definitivo y tenga un punto de 
   vista. Ser claro en los plazos para los que 
   verán un retorno.
Actividad colaborativa
Checkpoint Proyectos Finales
Analizaremos el siguiente proyecto en el siguiente 
link: Aerolíneas DS, con el fin de determinar la 
validez de Abstracto, Introducción, Contexto Político 
y empresarial
Discutiremos en el final de la actividad los 
resultados obtenidos
Duración: 30 minutos
Realizaremos la actividad en grupos de 2-3 personas
     ACTIVIDAD COLABORATIVA
Acuerdos
Presencia                                       Apertura al aprendizaje
✓ Participar y “estar” en la clase, que          ✓ Siempre, pero siempre puedes 
    tu alrededor no te distraiga                    seguir aprendiendo. Compartir el 
                                                    conocimiento es válido, la 
Escucha activa                                       construcción colaborativa es la 
                                                    propuesta.
✓ Escuchar más allá de lo que la 
    persona está expresando 
    directamente                               Todas las voces
                                                 ✓ Escuchar a todos, todos podemos 
                                                    reflexionar. Dejar el espacio para 
                                                    que todos podamos participar.
     ACTIVIDAD COLABORATIVA
Checkpoint Proyectos 
Finales
Consigna: Analizaremos el siguiente                Luego de esto discutiremos sobre los 
proyecto disponible en el siguiente link:          proyectos finales de cada grupo en el 
Aerolíneas DS                                      break out rooms, resolveremos dudas en 
Analizar las secciones: Contexto                   el desarrollo de las etapas: Delimitación y 
empresarial, Problema comercial,                   definición de problema, Data Acquisition, 
Contexto analítico e identificar fortalezas        Data Wrangling, Exploratory Data Analysis
y debilidades
Verificar si las gráficas generadas 
permiten dar respuesta parcial o total a 
las preguntas generadas
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
CLASE N°16
Glosario
Definición de objetivo: fase 1 en la             Data Engineering: fase 6 en la 
resolución de Casos en DS donde se establece     resolución de casos en DS donde se 
los límites del problema y el alcance del        resuelven problemas asociados a outliers, 
proyecto                                         nulos y selección de variables
Contexto comercial: fase 2 en la resolución      Selección y desarrollo de algoritmo: 
de casos en DS donde se hace la introducción     fases 7, 8 y 9 donde se elige el algoritmo 
al problema                                      para resolver el problema y 
Problema comercial: fase 3 en la resolución      posteriormente se implementa
de casos en DS donde se delimita el problema     Despliegue de algoritmo: fases final 
a resolver                                       en casos de DS donde se entrega un 
Contexto analitico: fase 2 en la resolución      producto utilizable para el beneficio de un 
de casos en DS donde se establece que datos      consumidor final (e.g aplicación, 
ayudaran a resolver el problema comercial        Dashboard, reportes automáticos)
EDA: fase 5 en la resolución de casos en DS 
donde se obtienen insights por medio de 
visualizaciones
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
 MATERIAL AMPLIADO
Recursos multimedia
✓ Machine Learning made beautifully simple for everyone  | BigML.com
✓ Clustering, análisis de segmentos de clientes: Caso de éxito para Mazda  | 
 cleverdata.io
✓ Se acabaron las estafas: así funciona la herramienta para la detección te
 mprana de fraudes de San Cristóbal
  | iprofesional.com
✓ Starbucks no es un negocio de café: es una empresa de tecnología de dat
 os
  | brita.mx
Disponible en nuestro repositorio.
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Casos de Éxitos: Mazda, San Cristóbal Seguros y 
        Starbucks. Caso Empresa de Seguros. Creación 
        de presentación ejecutiva
Muchas 
gracias.
Opina y valora 
esta clase
               Encuesta
               sobre esta clase
               Por encuestas de Zoom
               ¡Terminamos la clase! 
               Cuéntanos qué temas te resultaron más complejos de 
               entender. Puedes elegir más de uno. Vamos a 
               retomar aquellos temas que resultaron de mayor 
               dificultad en el próximo AfterClass.
Esta clase va a ser
grabad
  a
      Clase 05. DATA SCIENCE
 Programación con 
arrays: introducción 
       a NumPy
Temario
                      04                             05                             06
            Introducción a la                Programación                  Introducción a la 
            librería científica                con arrays:                 manipulación de 
                con Python                   Introducción a               datos con Pandas 
                                                  Numpy                         (Parte II)
                  (parte I)               ✓ Estructura de datos
               ✓ Estructura de                                             ✓ Fuentes de datos 
                   datos en Pandas        ✓ Numpy y ndarrays               ✓ Series y data frame
               ✓ Manipulación de          ✓ Tipos de datos                 ✓ Selección de datos
                   datos en Pandas        ✓
                                              Indexado y acceso            ✓ Operaciones
               ✓ Lecturas de              ✓ Operaciones básicas
                   archivos con                                            ✓ Agregaciones
                   Python                 ✓ Operaciones vectorizadas       ✓ Strings
Objetivos de la clase
         Conocer las estructuras de datos y su 
         implementación en Python
         Entender el uso básico del paquete NumPy 
MAPA DE CONCEPTOS
                      Recall: list, tuple, 
                      dict, set
Estructuras de 
datos en Python        Nueva estructura: 
                      Array               NumPy y ndarrays
                      Acceso a 
                      elementos
Acceso a                                   Operaciones 
ndarrays                                   básicas
                      Acceso a 
                      subarrays
                                          Agregaciones
Operaciones con                            Operaciones 
NumPy                                      vectorizadas
¡Vamos a Kahoot!
Estructuras de datos
Estructuras de 
control:
FOR, WHILE, IF
¿Qué son y para qué 
sirven?
  ✓ Las estructuras de control sirven           ✓ Las estructuras de control más 
     para dar claridad y orden al                  comunes son: 
     código. 
                                                   ������ For
  ✓ Si hay que hacer operaciones 
     repetitivas, estas estructuras nos            ������ While
     ayudan a organizarlas.                        ������ If
                                                   ������ Switch (Otros lenguajes e.g C)
   Recall: estructuras de 
   datos
      ✓ Anteriormente vimos las estructuras list, tuple, dict y set.
                     Tipo                Ejemplo                                           Definición
                                 list              [1, 2, 3]                                                      Lista ordenada
                               tuple               (1, 2, 3)                                     Lista ordenada inmutable
                                 dict   {'a':1, 'b':2, 'c':3}         Diccionario: conjunto de pares clave:valor
                                                                             Conjunto, a la manera de un conjunto 
                                 set              {1, 2, 3}                                                            matemático
Recall: estructura 
list
✓ Anteriormente trabajamos con 
estructuras list, que nos permitían 
almacenar datos ordenados de 
distinto tipo.
✓ Siempre mantenían el orden de sus 
elementos
✓ Eran mutables             L = list(range(10))
                         L
                 ������
Recall: estructura 
tuple
✓ Trabajamos también con las 
estructuras tuple.           T = tuple(range(10))
✓ Al igual que las listas,     T
siempre mantenían el 
orden de sus elementos
                                  ������
✓ Eran inmutables. Una vez 
inicializadas, no era posible 
reasignar elementos.
 Sin embargo… ������
 ¡Estas estructuras no llegan a cubrir 
 las necesidades del Data Scientist!  
Numpy y ndarrays
Actividad colaborativa
Programando estructuras en Python
Deberán resolver en grupo dos problemas 
reales, utilizando las estructuras 
aprendidas de programación en Python en 
una notebook.
Duración: 15 minutos
  Introducción a 
  NumPy
                                      NumPy es un proyecto de código abierto 
                                      que tiene como objetivo permitir la 
            NUMerica   PYthon         computación numérica con Python. Fue 
            l                         creado en 2005, basándose en el trabajo 
                                      inicial de las bibliotecas Numeric y 
           Potente estructura de      Numarray. 
                 datos
                                      NumPy siempre será un software 100% 
          Implementa matrices y       de código abierto, de uso gratuito para 
                matrices              todos y publicado bajo los términos 
           multidimensionales         liberales de la licencia BSD modificada
             Estructuras que         Equipo creador:
            garantizan cálculos      https://numpy.org/gallery/team.ht
          eficientes con matrices    ml
 El array como 
 estructura de datos
 ✓ Extenderemos la aplicación de estos       ✓ Mientras que el tipo de dato list 
    tipos de estructura de datos,               puede guardar datos de 
    agregando el tipo de dato array.            diferentes tipos, el tipo de dato 
 ✓ Tanto array como list sirven para            array guarda datos de un único 
    guardar conjuntos de datos                  tipo. 
    ordenados en memoria.                    ✓ Esto le permite ser más 
                                                eficiente, especialmente al 
                                                trabajar con conjuntos de datos 
                                                grandes. 
El array como 
estructura de datos
Los np.arrays pueden ser de diferentes 
dimensiones : 1D (vectores), 2D 
(matrices), 3D (tensores)
Creación de 
ndarrays
Creación de  ndarrays
✓ La librería Numpy provee una forma               import numpy as np
    particular de array llamada ndarray o 
    Numpy Array.                                   Npa = np.array(range(10))
✓ Recordar: los ndarrays, al ser un tipo           Npa
    de array, sólo pueden almacenar 
    datos de un mismo tipo.                                    ������
  Veamos ejemplos
  Np_cero = np.zeros(10)                          ������
  Np_cero
  Np_cero_int = np.zeros(10, dtype=int)           ������
  Np_cero_int
  Np_uno = np.ones(10)                            ������
  Np_uno
  Np_relleno = np.full(10,256)                    ������
  Np_relleno
Veamos ejemplos
✓ Numpy provee objetos rango:
Np_rango = np.arange(10)  ������
Np_rango
✓ Ndarrays con valores aleatorios y de dos dimensiones: 
Np_random_dimensiones = np.random.randint(10, size=(3, 4))
Np_random_dimensiones
Tipos de datos y 
atributos de arrays
Tipos de datos 
arrays
        Tipos de datos en numpy            Tipos de datos en numpy
        i   integer  -                     M datetime
        b   boolean                        O   object
        u   unsigned integer               S   string
        f   float                          U   unicode string
        c   complex float                  V   fixed chunk of memory for 
                                               other type ( void )
        m timedelta
  Tipos de datos 
  arraysTipos de datos en Python
                           strings    para representar datos textuales
                           integer    para representar números 
                                      enteros. e.g. -1, -2, -3
                           float      para representar números reales. 
                                      e.g. 1.2, 42.42
                           boolean  para representar True o False.
                           comple     para representar números 
                           x          complejos. e.g. 1.0 + 2.0j, 1.5 + 
                                      2.5j
Verificando el tipo de 
dato de un array
   # Verificando el tipo de dato de array
   arr = np.array([1, 2, 3, 4])                                int64
                                                     ������
   print(arr.dtype)
   arr = np.array(['apple', 'banana', 'cherry'])     ������        <U6
   print(arr.dtype)
                                    Creando arrays con formato específico
   arr = np.array([1, 2, 3, 4], dtype='S')           ������        [b'1' b'2' b'3' b'4']
   print(arr);print(arr.dtype)                                 |S1
   arr = np.array([1, 2, 3, 4], dtype='S')           ������        [b'1' b'2' b'3' b'4']
   print(arr);print(arr.dtype)                                 |S1
Convertir el tipo de 
dato de un array
 arr = np.array([1.1, 2.1, 3.1])
                                    [1 2 3]
 newarr = arr.astype('i')      ������
                                    int32
 print(newarr)
 print(newarr.dtype)
 arr = np.array([1.1, 2.1, 3.1])
                                    [ True False  True]
 newarr = arr.astype('i')      ������
                                    bool
 print(newarr)
 print(newarr.dtype)
Atributos de los 
arrays
Veamos ejemplos
Np_rango = np.arange(10)  ������
Np_rango
✓ Ndarrays con valores aleatorios y de dos dimensiones: 
Np_random_dimensiones = np.random.randint(10, size=(3, 4))
Np_random_dimensiones
        Atributos de los 
  Inspeccionemos un poco nuestros Numpy arrays ������
                       Arrays
  Podemos acceder a distintas propiedades de los arreglos:
                    Np_cero.ndim                        ������
    ✓ Dimensión: 
                                                        ������
                     Np_random_dimensiones.ndim
    ✓ Forma:        Np_random_dimensiones.shape         ������
    ✓ Tamaño:       Np_random_dimensiones.size          ������
Inspeccionemos nuestros 
Numpy arrays ������
Podemos acceder a distintas propiedades de los arreglos:
                                   Np_cero.dtype                       ������
  ✓ Tipo de dato:                                                      ������
                                   Np_cero_int.dtype
  ✓ Tamaño de elemento:            Np_random_dimensiones.itemsize      ������
  ✓ Tamaño total:                  Np_cero.nbytes
                                                                       ������
                                   Np_cero_int.nbytes
Resumen Tipos de Datos y 
Propiedades de Arrays
Tipos de datos posibles en numpy array
Resumen Tipos de Datos y 
Propiedades de Arrays
Propiedades de los numpy array
Indexado y acceso
Accediendo a 
elementos
  Veamos cómo consultar los 
  arreglos
✓ Al igual que las listas, los elementos del arreglo se acceden mediante su índice, 
    comenzando desde 0.
    rango = range(1,11)
    Np_diez_numeros = np.array(rango)        ������
    Np_diez_numeros
                                                     ������
✓ Primer elemento:       Np_diez_numeros[0]
                         Np_diez_numeros[4]          ������
✓ Quinto elemento:
  Veamos cómo consultar los 
  arreglos
 ✓ Podemos seleccionar elementos desde atrás para adelante mediante índices negativos, 
    comenzando desde -1.
 ✓ Último elemento:                                        ������
                                 Np_diez_numeros[-1]
 ✓ Penúltimo elemento:           Np_diez_numeros[-2]       ������
 ✓ Para acceder a un elemento de una matriz, indicar fila y columna:
     Np_random_dimensiones     ������
     Np_random_dimensiones[2, 1]     ������
Accediendo a 
subarrays
 El array como 
 estructura de datos
    Podemos seleccionar una                      ✓ El parámetro tamaño_de_paso 
   rebanadas del arreglo de la                       permite, por ejemplo, 
       siguiente manera:                             seleccionar elementos de dos en 
                                                     dos
Objeto[desde:hasta:tamaño_de_paso]                ✓ Atención a estos detalles
                                                      ○ El índice "desde" es 
                                                          inclusivo.
                                                      ○ El índice "hasta" es 
                                        ⚠                 exclusivo.
Veamos algunos 
ejemplos
    ✓ Primeros cuatro:                 Np_diez_numeros[:4]
    ✓ Desde el cuarto:                 Np_diez_numeros[3:]
    ✓ Desde el quinto al séptimo:      Np_diez_numeros[4:7]
    ✓ De dos en dos:                   Np_diez_numeros[::2]
    ✓ Desde atrás, de dos en dos:      Np_diez_numeros[::-2]
Veamos algunos 
ejemplos
     Para arreglos multidimensionales, especificar los índices de manera ordenada:
                                Objeto[dimensión1, dimensión2,…]
Veamos algunos ejemplos...
  ● Tercera fila, todas las columnas:                      Np_random_dimensiones[2,]
  ● Primeras dos filas, primeras dos columnas:             Np_random_dimensiones[:2, :2]
  ● Tercera fila, cuarta columna:                          Np_random_dimensiones[2, 3]
         ☕
       Break
       ¡10 minutos y 
        volvemos!
Operaciones básicas: 
reshape, 
concatenación, 
splitting
               Operaciones básicas
   Reshape       Concatenación    Splitting
Reshape
Para pensar
Pensando en ajedrez 
Si tuviésemos que rellenar una grilla de 
8x8 
con números desde 1 a 64, 
¿Cómo lo haríamos?
Reshape
Permite modificar la dimensión 
de un arreglo (siempre y 
cuando las dimensiones de 
salida están relacionadas con 
las de entrada)
¿Que patrón curioso 
observan? 
Reshape
Permite modificar la dimensión de un arreglo, retornando otro con distinta 
dimensión y forma pero manteniendo los mismos elementos.
  np.arange(1,65)                        ������
  Ajedrez_64 = np.arange(1,65).reshape(8,8)       ������
  Ajedrez_64
Concatenación
Concatenación
Permite modificar concatenar arrays siempre y cuando las dimensiones lo permitan.
 axis= 1 concatena por columnas                         axis= 0 concatena por filas
Concatenación
Concatenación
Consiste en formar un nuevo arreglo a partir de 
“enganchar” o “apilar” otros.
✓ Python ofrece dos métodos:
○ Con la operación concatenate. 
○ Con las operaciones vstack y hstack 
       Array_1 = np.random.randint(10, size=5)
       Array_2 = np.random.randint(10, size=5)
       Arrays_concatenados = np.concatenate([Array_1, Array_2])
        ������
  Concatenación
 ✓ El método vstack apila verticalmente:
  Array_extra = np.array([[10],[20]])             ������
  Array_extra
  Array_apilados_v = np.vstack([Array_extra, Array_extra])                ������
  Array_apilados_v
   ✓ El método hstack apila horizontalmente:
                                                      ������
  Array_apilados_h = np.hstack([Array_extra, Array_extra])                ������
  Array_apilados_h
Splitting
Splitting
 ✓ Consiste en desarmar o partir los arreglos.
 ✓ Puede pensarse como la operación inversa a la concatenación
Arrays_concatenados      ������
                                                            Especificamos los puntos de corte 
Array_partido = np.split(Arrays_concatenados, [2])        con un arreglo. En este caso queremos 
Array_partido                                                un único corte entre el segundo y 
                                                                      tercer elemento
                       ������
Splitting
✓ Dos puntos de corte          Array_partido_2 = np.split(Arrays_concatenados, [2, 8])
                              Array_partido_2
                                                       ������
✓ Podemos desarmar el arreglo y 
                              ������     Parte_1, Parte_2, Parte_3 = Array_partido_2
  guardarlo en variables distintas
    Parte_1                   Parte_2                  Parte_3
Splitting
✓  hsplit realiza cortes verticales:
 Ajedrez_partido_1 = np.hsplit(Ajedrez_64, [4])             ������
 Ajedrez_partido_1
✓ vsplit realiza cortes horizontales:
Ajedrez_partido_2 = np.vsplit(Ajedrez_64, [4])              ������
Ajedrez_partido_2
Agregaciones
   Cálculos sobre Numpy 
Como futuros Data Scientists, cotidianamente nos encontraremos con la tarea de efectuar 
                arrays
            cálculos a partir de arrays  
       Numpy está para darnos una mano en esto
Calculando el 
promedio
Una solución tradicional al 
problema de calcular la media es        Array_aleatorio = np.random.randint(10, size=10)
la siguiente:                           print(Array_aleatorio)
Si bien esta resolución es elegante      suma = 0
y cumple con su tarea, Numpy nos         for i in Array_aleatorio:
provee de opciones más cómodas y 
eficientes ������                                suma += i
                                      promedio = suma / np.size(Array_aleatorio)
Agregaciones
  ✓ Suma:                          Array_aleatorio.sum()
                                   Array_aleatorio.mean()                 Estas funciones están 
  ✓ Promedio:                                                             optimizadas para grandes 
                                                                          volúmenes de datos y 
  ✓ Valor máximo:                  Array_aleatorio.max()                  además nos ahorran 
                                                                          mucho código… ������
  ✓ Mediana:                       np.median(Array_aleatorio)
  ✓ Desvío estándar:               np.std(Array_aleatorio)
  ✓ Varianza:                      np.var(Array_aleatorio)
Operaciones 
aritméticas 
Operaciones 
estadísticas
Operaciones 
vectorizadas
Operaciones vectorizadas
¿Por qué son tan importantes?
✓ Incluso las operaciones más             ✓ Las operaciones 
   sencillas pueden resultar muy              vectorizadas o funciones 
   lentas si las llevamos a cabo              universales (ufuncs) nos 
   elemento a elemento.                       permiten operar entre 
✓ Las computadoras son                        arreglos de la manera más 
   especialmente buenas para                  rápida posible.
   realizar cálculos en paralelo ������ 
Operemos arreglos, 
pero de manera 
Recordemos los arreglos de prueba:
eficiente 
 Array_1   ������                                      Array_2   ������
 ✓ Sumas vectorizadas:
        Array_1 + 5     ������
        Array_1 + Array_2
                                       ������
        np.add(Array_1, Array_2)         ¡Ambas formas son equivalentes!
 Producto Vectorial
  ✓ El producto vectorial sobre 
      arreglos unidimensionales se 
      calcula sumando los resultados de 
      multiplicar los elementos que                                                ������
      tienen la misma posición.               np.matmul(Array_1, Array_2)
  ✓ En Numpy, la versión vectorizada 
      se implementa en el método 
      np.matmul
                    1
  Elección de Datasets 
          potenciales
Deberás entregar el primer avance de tu proyecto final. Identificarás 3 datasets 
potenciales con las siguientes características: i) al menos 2000 filas, ii) al menos 
15 columnas. Posterior a esto crearás un notebook donde cargarás los datos 
utilizando la librería pandas y finalmente describirás las variables que sean más 
interesantes teniendo en cuenta el contexto comercial y analítico del problema 
             que se quiera resolver.
    DESAFÍO 
    ENTREGABLE
Datasets con la librería 
Pandas
Consigna
✓ Identificar 3 datasets que cumplan con         ✓ Cargar los archivos correspondientes 
    las siguientes condiciones: a) al menos         por medio de la librería pandas
    2000 filas y b) al menos 15 columnas.        ✓ Describir las variables 
    Pueden buscar en las siguientes                 potencialmente interesantes en cada 
    fuentes: GitLab, Github, Kaggle, Google         archivo teniendo en cuenta el 
    Dataset Search (Si desean trabajar con          contexto comercial y analítico 
    un archivo propio se puede también)             involucrado
✓ Algunas API recomendadas para                 Aspectos a incluir
    obtener información:                          ✓ El código debe estar hecho 
    Marvel,PokeApi,CovidTracking,Nomics              en un notebook y debe 
    (Criptomonedas),Wheater API                      estar probado.
    DESAFÍO 
    ENTREGABLE
Datasets con la librería 
Pandas
Formato
  ✓ Entregar un archivo con                      Explicación en video
      formato .ipynb. Debe tener el               ✓ ¡Clickea aquí!
      nombre “Datasets+Apellido.ipynb”.
Sugerencias
  ✓ Preparar el código y probar los 
      resultados con distintas entradas
CLASE N°5
                                                    Indexación: forma de extraer elementos de un 
Glosario                                            objeto en Python. Importante recordar que el 
                                                    primer índice es el 0 de izquierda a derecha y de 
                                                    derecha a izquierda es -1.
                                                    Reshape: modificar la forma de un array 
Numpy: librería de Python que nos permite           siempre y cuando las dimensiones de entrada y 
trabajar con matrices y vectores de forma           salida sean compatibles
sencilla y potente
                                                    Concatenación: apilamiento de arrays siempre 
Array: estructura fundamental en Numpy que          y cuando las dimensiones sean compatibles
solo permite un tipo de dato haciéndolo eficiente 
para operaciones de alta complejidad, pueden        Splitting: desarmado de un array (operación 
ser de 1D (vectores), 2D (matrices) o 3D            inversa de la concatenación)
(tensores)                                          Agregaciones: todas aquellas funciones 
Atributos de arrays: son las propiedades de         preestablecidas que nos permiten calcular 
los arrays creados, podemos extraer propiedades     medidas de tendencia central (e.g media, 
como: dimensión (.ndim), forma (.shape),            mediana) o dispersión (e.g. varianza ,desviación 
tamaño (.size) entre otros                          estándar) de manera eficiente
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Introducción a NumPy y ndarrays, acceso e 
        indexado.
      ✓ Operaciones básicas con ndarrays.
      ✓ Agregaciones
      ✓ Operaciones vectorizada
Muchas 
gracias.
Opina y valora 
esta clase
Esta clase va a ser
grabad
  a
      Clase 17. DATA SCIENCE 
Estudios de casos de 
Modelos Analíticos II
Temario
           16                17                 18
       Estudio de         Estudio de       Introducción al 
        casos de           casos de          ML y la IA
        modelos            modelos 
       analíticos I      analíticos II
      ✓ Casos de                            ✓ Introducción
         éxitos con      ✓ Introducción     ✓ Tipos de IA
         ciencias de     ✓ Casos de 
         datos              accidentes      ✓ Aplicaciones en 
      ✓ Armado de                              la industria
         presentación 
         ejecutiva                          ✓ Riesgos 
                                               asociados
Objetivos de la clase
         Identificar formas de aplicación de Ciencia de 
         Datos en diferentes industrias
         Comparar las potencialidades de aplicación en 
         cada caso.
MAPA DE CONCEPTOS
                                 Andreani      Future 
                                              Engineering
                                Medplaya      Analítica 
  Casos de éxito                              predictiva
  con Modelos 
   Analíticos                    Amazon       Clicksteam
                               Caso accidente 
                                 New York
Repaso en Kahoot
Introducción
PARA RECORDAR
Empecemos
Continuando con la clase de Estudios de Casos de 
Modelos Analíticos I en esta sesión veremos 
algunas aplicaciones adicionales del mundo de la 
Ciencia de Datos, en diferentes industrias. 
Andreani
Caso Andreani
Andreani es una empresa de servicios logísticos 
orientada a crear una red de logística social. Cuenta 
con más de 4734 colaboradores, 165 puntos de 
ventas, 103 sucursales en 86 localidades, etc.
Arquitectura de Datos actual
ML para Andreani
Desafío: Generar un Modelo de                    ¿Esto qué permite?
Predicción de tiempos de entrega.                ✓
                                                   Avisarle al usuario cuando va a estar 
Objetivo: Predecir qué día va a llegar el           llegando su paquete. ������
paquete y en qué franja horaria                  ✓ Optimizar la logística de la empresa. ������
(mañana/tarde). Tanto para entrega por 
Sucursal como para entrega en Domicilio.         ✓ Incrementar la satisfacción y la 
                                                   experiencia de usuario. ������
Para pensar
Si los objetivos se orientan a:
✓ Predecir qué día va a llegar el paquete y en qué 
franja horaria. Tanto para entrega por Sucursal 
como para entrega en Domicilio.
✓ Avisarle al usuario cuando va a estar llegando su 
paquete.
✓ Optimizar la logística de la empresa.
✓ Incrementar la satisfacción y la experiencia de 
usuario.  
¿Qué solución se podría plantear desde el ML y 
por qué?
Contesta en el chat de Zoom 
Feature 
Engineering
✓ Se entrenó el modelo usando 12 meses de 
historia.
✓ Se consideró a la hora del diseño del algoritmo 
el escenario generado como contexto de la 
pandemia del Covid – 19.
✓ Se realizó un proceso de Feature 
engineering. Ej: Tratamiento de outliers, 
valores missings, sampling, etc.
Medplaya
Caso Hoteles 
Medplaya
MedPlaya es una cadena hotelera 
especializada en vacaciones en la costa. 
Desde su página de internet se pueden 
alquilar excursiones, hoteles y atractivos 
turísticos en múltiples destinos del mundo 
como así también, contratar servicios de 
viajes.
Caso Hoteles Medplaya
Retos:                                         Objetivo:
La aparición de nuevos actores en el           Conocer exactamente qué reservas se 
sector hotelero que actúan como                cancelarán para maximizar la ocupación 
intermediarios ha supuesto al mismo            de la cadena de hoteles.
tiempo una oportunidad y un reto para el 
sector. Ej. Airbnb.
Para pensar
¿Por qué es importante realizar este tipo de 
predicciones? 
Debido a que las habitaciones con mayor probabilidad 
de cancelación, se pueden poner a la venta o lanzar 
una oferta al cliente para maximizar los ingresos de la 
compañía.
¿Cómo podríamos utilizar el ML para estas 
predicciones?
Contesta en el chat de Zoom 
Analítica Predictiva
✓ Conocer con claridad qué reservas 
se cancelarán aumenta la eficiencia 
de las previsiones de ocupación. 
✓ Permite tomar decisiones de 
negocio basadas en datos de la 
propia organización, no en 
intuiciones o estadísticas globales.
✓ Al poner a la venta las habitaciones 
con probabilidad de cancelación se 
logra maximizar los ingresos de la 
compañía.
Modelo predictivo
Resulta importante mencionar que los         ¿Cómo se desarrolló y se implementó el 
riesgos se mitigan ya que utilizamos la      Modelo Predictivo?
confianza matemática de la previsión         En una primera fase se recogió toda la 
para gestionar solo aquellas reservas con    información histórica de las reservas para 
mayor probabilidad de ser canceladas.        analizar los perfiles de clientes y los 
                                            patrones de comportamiento de las 
                                            mismas: fechas de antelación, ocupantes, 
                                            régimen, tarifa, etc.
Con la data histórica de la compañía, se realizó 
un análisis de las reservas entrantes para poder 
determinar la predicción de cancelación. Dentro de 
este contexto, es importante destacar que la                          REEMPLAZAR 
predicción, se acompaña del % de probabilidad de                      POR IMAGEN
dicha  predicción. Lo que permite, tomar todas las 
decisiones de reventa en base a datos y criterios 
estrictamente de negocio balanceando riesgo y 
rentabilidad de forma personalizada.
Resultados y planes 
a futuro
Gracias a esta solución Medplaya recibió el 
premio de la Innovación Turística en la pasada 
Feria de Fitur ’19. Los resultados han 
demostrado que el % de acierto sobre el total 
de reservas es de un 80% aproximadamente 
y el porcentaje de aciertos de las 
cancelaciones es de un 67%. 
Los próximos planes pasan por incluir datos 
de pronóstico del tiempo e información de la 
competencia para mejorar los modelos 
predictivos.
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Amazon
Para pensar
Cuando Amazon recomienda un producto en su sitio, 
claramente no es coincidencia. Este gigante del e-
commerce conoce tan bien a sus clientes que puede 
realizar envíos incluso antes de que efectúen una 
compra. 
¿Crees que se usa el análisis predictivo? 
¿Con qué otras empresas se detectan 
Contesta en el chat de Zoom 
similitudes?
Caso Amazon
Amazon es una de las 500 mayores empresas de 
EE.UU. Desde que Jeff Bezos lanzó Amazon.com en 
1995, se ha hecho un progreso significativo en la 
oferta, en los sitios web y en la red internacional de 
distribución y servicio al cliente. En la actualidad, 
Amazon ofrece gran variedad de productos, desde 
libros o productos electrónicos, hasta raquetas de 
tenis, servicios de Cloud e incluso de Streaming como 
Amazon Prime. 
Clicksteams
 Entonces, Amazon utiliza el análisis           Los sistemas de recomendación son muy 
 predictivo para construir un sistema           conocidos por su uso en los entornos de 
 de recomendación que sugiere                   sitios web de e-commerce, en los cuales 
 productos a las personas que visitan el        utilizan las entradas acerca de los interés 
 sitio. Para esto, utiliza datos de             de sus clientes para generar una lista de 
 secuencias de clics (clicksteams) de           recomendaciones de items. 
 clientes y los datos históricos de compras 
 de sus clientes para mostrar a cada 
 usuario, resultados personalizados en sus 
 páginas web personalizadas.
Clicksteams en Amazon
Para crear una lista de recomendaciones,       El sistema de recomendaciones de 
muchos websites utilizan solo los items        Amazon se basa en una serie de 
que compran los clientes y explícitamente      elementos tales como: las compras de un 
el promedio en que éstos representan sus       usuario en tiempos pasados, los items 
interés, pero también pueden utilizar          que tiene en su carrito de compra virtual, 
otros atributos, incluyendo items vistos,      la clasificación de los items según su 
datos demográficos, asuntos de intereses       interés, y el top de los items que han 
y artistas favoritos.                          comprado o visitado. 
Algoritmos de 
recomendación
La mayoría de los algoritmos de recomendación                     REEMPLAZAR 
comienzan por encontrar un conjunto de clientes                    POR IMAGEN
cuyas compras y artículos clasificados coinciden 
con las compras y clasificación de  artículos de otros 
clientes. El algoritmo agrega artículos de estos 
clientes similares, elimina los elementos que el 
usuario ya ha comprado o clasificado y recomienda 
los productos que queden al usuario.
Enfoques de los 
sistemas de 
recomendación
Filtrado colaborativo
Si dos usuarios compartieron los mismos       En este tipo de recomendación, los ítems 
intereses en el pasado, ellos tendrán         son filtrados desde un gran conjunto de 
gustos similares en el futuro. Si, por        alternativas, que es hecho en 
ejemplo, el usuario A y el usuario B tienen   colaboración entre las preferencias 
el mismos historial de compras y el           de los usuarios y de allí, justamente se 
usuario A compró un libro que el usuario B    obtiene su nombre.
no ha comprado aún, la idea básica es 
proponerle el libro al usuario B.
Filtrado basado en 
Contenido
Este tipo de RS es empleado en                Por ejemplo, un escenario en el cual se 
dominios específicos donde el historial       está construyendo un sistema de 
de compras del usuario es muy pequeño.        recomendación para recomendar 
El algoritmo considera el conocimiento        artefactos eléctricos para el hogar, donde 
acerca de los ítems, tales como               muchos de los usuarios son nuevos. En 
características, preferencias de los          este caso, el sistema considera 
usuarios consultados explícitamente y         características de los ítems, y se 
criterios de recomendación, antes de dar      generan perfiles de los usuarios para 
una recomendación. La certeza del             obtener información adicional de los 
modelo es juzgado en lo útil que fueron       mismos, tales como especificaciones, 
para el usuario las recomendaciones           para luego realizar las recomendaciones. 
realizadas.
Para pensar
¿Qué ejemplos de filtrado colaborativo conocen? Y, 
¿Qué ejemplos de filtrado basado en contenidos se les 
ocurren?
Pensemos en qué estrategias permiten 
diferenciar uno del otro.
Contesta en el chat de Zoom 
Caso accidentes
en Nueva York
Ejemplo en vivo
¿Cómo se imaginan diseñar una estrategia para 
reducir la cantidad de accidentes en las vías de una 
gran ciudad como New York? Estudiaremos un caso 
real aplicado donde podremos ver el uso de 
Feature Engineering
Utilizaremos el notebook  Clase_17.ipynb disponible 
en la carpeta de clase.
Fases importantes para la 
resolución de Casos en DS
 1                2                  3                   4                 5
Definición de        Contexto          Problema            Contexto         Exploración
objetivo          comercial         comercial           analítico      de datos (EDA)
Fases importantes para la 
resolución de Casos en DS
 6                7                 8                  9                 1
                                                                         0
Data         Selección del     Desarrollo de      Interpretación     Conclusiones
Engineering        algoritmo         algoritmo
              adecuado
Veamos cada fase 
aplicada a este caso
1.Definición de objetivo
¿Cómo podemos controlar el creciente 
número de accidentes en Nueva York?
Definir el objetivo es una de las etapas 
más importantes porque es donde 
decidimos que queremos hacer
2.  Contexto comercial
La ciudad de Nueva York ha                   La ciudad te ha contratado para que 
experimentado un aumento en el               construyas visualizaciones que les ayuden 
número de accidentes en las carreteras       a identificar patrones en accidentes, lo 
de la ciudad. Quieren saber si el número     que les ayudaría a tomar acciones 
de accidentes ha aumentado en las            preventivas para reducir la cantidad de 
últimas semanas. Para todos los              accidentes en el futuro. Tienen ciertos 
accidentes reportados, han recopilado        parámetros como municipio, hora del día, 
detalles para cada accidente y han estado    motivo del accidente, etc. De los que se 
manteniendo registros durante el último      preocupan y de los que les gustaría 
año y medio (desde enero de 2018 hasta       obtener información específica.
agosto de 2019).
3.  Problema comercial
Tu tarea es manipular los datos                  ✓ ¿Cuál es la proporción de recuento 
disponibles y proporcionar visualizaciones           de accidentes por área por 
que respondan a las preguntas                        municipio? ¿Qué distritos tienen un 
específicas que tiene el cliente:                    número desproporcionadamente 
 ✓ ¿Cómo ha fluctuado el número de                  grande de accidentes para su 
    accidentes durante el último año y              tamaño?
    medio? ¿Han aumentado con el                ✓ Para cada municipio, ¿Durante qué 
    tiempo?                                         horas es más probable que ocurran 
 ✓ Para un día en particular, ¿Durante              accidentes?
    qué horas es más probable que               ✓ ¿Cuáles son las cinco principales 
    ocurran accidentes?                             causas de accidentes en la ciudad? 
 ✓ ¿Hay más accidentes entre semana                 ¿Qué tipo de vehículos están 
    que durante los fines de semana?                involucrados?
4.  Contexto analítico
Se le proporciona un archivo .CSV                ✓ Extraer datos adicionales del 
(almacenado en la carpeta datos) que                municipio almacenados en un 
contiene detalles sobre cada accidente,             archivo JSON
por ejemplo, fecha, hora, ubicación del          ✓ Leer, transformar y preparar datos 
accidente, motivo del accidente, tipos de           para su visualización.
vehículos involucrados, recuento de 
lesiones y muertes. El delimitador en el         ✓ Realizar análisis y construir 
archivo .CSV dado es “;” en lugar del               visualizaciones de los datos para 
predeterminado “,” . Debe realizar las              identificar patrones.
siguientes tareas con los datos:
                Datos - columnas de 
                                    interés
       1. BOROUGH: el municipio en el que        9. NUMBER OF (CYCLISTS, 
           ocurrió el accidente                     MOTORISTS, PEDESTRIANS) 
       2. COLLISION_ID: un identificador            INJURED: Lesión 
           único para esta colisión              10.NUMBER OF (CYCLISTS, 
       3. CONTRIBUTING FACTOR                       MOTORISTS, PEDESTRIANS) 
           VEHICLE (1, 2, 3, 4, 5): Motivos         DEATHS: Categoría muerte 
           del accidente                         11.ON STREET NAME: Calle donde 
       4. CROSS STREET NAME: Calle                  ocurrió el accidente
           transversal más cercana al lugar      12.VEHICLE TYPE CODE(1, 2, 3, 4, 
           del accidente                            5): Tipos de vehículos involucrados 
       5. DATE: Fecha del accidente                 en el accidente
       6. TIME: Hora del accidente               13.ZIP CODE: código postal del lugar 
       7. LATITUDE: Latitud del accidente           del accidente
       8. LONGITUDE: Longitud del 
           accidente
Datos - columnas de 
 interés
5.  Exploratory Data 
Analysis
Notamos que agosto de 2019 es el mes       Sin tener en cuenta este mes, la menor 
con el menor número de accidentes, esto    cantidad de accidentes parece ocurrir en 
probablemente se deba al hecho de que      febrero. Este es el mes del año con las 
no hay un registro completo para este      temperaturas más bajas y las condiciones 
mes: max(df['DATE']) =                     de la carretera pueden complicarse un 
Timestamp('2019-08-24 00:00:00'). Es       poco. Por lo tanto, las personas pueden 
decir, falta toda la última semana de      preferir usar el transporte público y evitar 
datos.                                     salir tanto como sea posible debido al 
                                          clima frío. Esto contrasta con el verano, 
                                          donde las personas tienden a salir y 
                                          pueden ocurrir más accidentes.
     Accidentes por mes
                REEMPLAZAR 
                 POR VIDEO
 Distribución de horas con 
      mayor cantidad de 
            accidentes
                REEMPLAZAR 
                 POR VIDEO
También podemos estratificar nuestros 
datos por género antes de realizar las 
visualizaciones anteriores:
Hay relativamente menos accidentes 
los fines de semana.
Podemos ver que Brooklyn y Queens 
tienen un número muy alto de 
accidentes en relación con los otros 
tres condados. Pero, ¿Qué tal por milla 
cuadrada? ������
Al mirar el parámetro 
accident_per_sq_mi, Manhattan 
encabeza la lista por un amplio 
margen. Esto muestra claramente que 
aunque Brooklyn y Queens tienen más 
accidentes totales, Manhattan tiene 
una concentración mucho mayor de 
accidentes.
6.  Data Engineering 
(Obtención de insights)
Podemos ver que en 
todos los distritos el 
recuento de accidentes 
es más alto 
aproximadamente entre 
las 2 y las 6 p.m. Pero 
en Manhattan y el 
Bronx, puede ver que no 
hay tanto aumento 
relativo durante estas 
horas como en Brooklyn 
o Queens. Además, 
Staten Island tiene el 
menor número total de 
accidentes.
5.  Desarrollo del 
algoritmo-método
Muertes por vehículo
9 y 10.  Interpretación de 
resultados y conclusiones
  1. El gráfico de líneas que trazamos           3 Podemos ver que Brooklyn y 
     muestra claramente que no hay una               Queens tienen un número muy alto 
     tendencia alcista obvia en los                  de accidentes en relación con los 
     accidentes a lo largo del tiempo.               otros tres condados.
  2. Hay relativamente menos accidentes          4 Al mirar el parámetro 
     los fines de semana que entre                   accident_per_sq_mi, Manhattan 
     semana.                                         encabeza la lista por un amplio 
                                                     margen. Esto muestra claramente 
                                                     que aunque Brooklyn y Queens 
                                                     tienen más accidentes totales, 
                                                     Manhattan tiene una concentración 
                                                     mucho mayor de accidentes.
9 y 10.  Interpretación de 
resultados y conclusiones
 5 El recuento de accidentes es más alto     Causas principales de accidentes
     aproximadamente entre las 2 y las 6 
     p.m. Pero en Manhattan y el Bronx,        a. Driver Inattention/Distraction
     puede ver que no hay tanto aumento        b. Failure to Yield Right-of-Way
     relativo durante estas horas como en      c. Following Too Closely
     Brooklyn o Queens. Además, Staten         d. Backing Unsafely
     Island tiene el menor número total de     e. Passing Too Close
     accidentes.
 6 Podemos ver que Sedan y Station 
     Wagon / Sport Utility Vehicle son 
     claros ganadores por causar el mayor 
     número de accidentes, y que esto no 
     difiere entre los condados.
                     4
     Estructurando un 
       Proyecto de DS 
Deberás entregar el cuarto avance de tu proyecto final. Continuaremos hablando sobre 
            (parte II)
lo trabajado en el desafío Estructurando un proyecto DS-Parte I. Crearás un 
  notebook donde se resuelvan los siguientes apartados: i) Abstracto, ii) 
Preguntas/hipótesis, iii) EDA, iv) recomendaciones con base en insights observados y v) 
tener definido en el notebook las secciones: Objetivo, Contexto Comercial, Problema 
   Comercial Contexto analítico y Exploratory Data Analysis (EDA)
Recordemos
                                     Generamos hipótesis de interés
    Desafío anterior:                Creamos visualizaciones y 
Estructurando un proyecto             resúmenes numéricos
       DS- Parte 1
                                    Encontramos patrones de interés
                                    Exploratory Data Analysis (EDA)
                                                  ������
       DESAFÍO 
       ENTREGABLE
Estructurando un 
Proyecto de DS (parte II)
                                                           Formato
Consigna                                                     ✓ Entregar un archivo con formato .ipynb. 
Deberás complementar la parte I con:                              Debe tener el nombre 
  1.  Abstracto con motivación y audiencia                        “ProyectoDS_ParteII_+Apellido.ipynb”
  2.  Preguntas/Hipótesis que queremos resolver 
      mediante el análisis de datos
  3.  Análisis Exploratorio de Datos (EDA)                 Sugerencias
  4.  Con base en las visualizaciones y                      ✓ Preparar el código y probar los 
      resúmenes numéricos generados del desafío                   resultados con subconjuntos del 
      anterior dar recomendaciones basados en                     conjunto original.
      los insights observados.                               ✓ Link video explicativo
  5.  Para esta oportunidad deberás tener 
      avances en los apartados: Definición de              Aspectos a incluir:
      objetivo, Contexto comercial, Problema                 ✓ El código debe estar hecho en un 
      Comercial, Contexto analítico, Exploración 
      de datos (EDA)                                              notebook y debe estar probado.
CLASE N°17
Glosario
Feature Engineering: proceso mediante              Sistemas de recomendación: 
el cual se hace la limpieza y                      entendimiento del comportamiento de 
estructuración de datos lidiando con               consumo de compradores para generar 
nulos, atípicos, outliers, categorías no           productos, ofertas y descuentos 
deseadas. Suele ser una de las etapas              personalizados basados en tendencias. 
donde más se debe invertir tiempo para             Estos sistemas son muy comunes en 
que los datos que salgan sean de buena             plataformas como Mercadolibre, Amazon, 
calidad                                            Ebay, Alibaba
Analítica predictiva: toma de decisiones           Clicksteams: sistema de recomendación 
en el negocio con base en información              usado por amazon que sigue la secuencia 
histórica para mejorar márgenes de                 de clicks y tiempos en pantalla de 
productividad. Comprende todo lo que se            potenciales compradores para mostrar 
conoce como procesos de Bussiness                  resultados personalizados
Intelligence y Decision Making
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
MATERIAL AMPLIADO
Recursos multimedia
Links de interés
✓ Jornadas de Data Mining y Business I
ntelligence – Maestría en Ciencia de 
Datos
- | UNIVERSIDAD AUSTRAL 
✓ Predicción de Cancelaciones de Rese
rvas: Caso de Éxito Medplaya
- | cleverdata.io 
✓ Amazon utiliza Big Data & Análisis Pr
edictivo para recomendar futuras ve
ntas
| diplomadosonline.com
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Casos de éxito: Andreni, Medpalaya, Amazon
      ✓ Ejemplo en vivo: fase por fase
Opina y valora 
esta clase
Muchas 
gracias.
           ¿Sabías que 
     premiamos a nuestros 
            estudiantes 
       por su dedicación? 
           Conoce los beneficios del Top 10
Esta clase va a ser
grabad
  a
      Clase 22. Data Science 
     Retomando 
      impulso…
Temario
            21                  22                  23
          Stack            Retomando              Data 
      Tecnológico II         Impulso           Acquisition I
       ✓ Sistema in-       ✓ Fases de un       ✓ Adquisición de 
         house               proyecto de DS      datos 
       ✓ Cloud             ✓ Numpy y Pandas    ✓ Lectura de 
         Computing                               datos con 
                           ✓ Visualización       Pandas
       ✓ Fundamentos       ✓ Estadística       ✓
         del Big Data                            Hojas de 
                             Descriptiva         cálculo
       ✓ ETL                                   ✓
                                                 Pyspark
Objetivos de la clase
         Repasar las herramientas fundamentales para 
         el curso que comienza
         Realizar un recuento de los temas vistos en 
         Data Science I a modo de nivelación
         Proveer a los estudiantes el compendio de 
         notebooks del curso Data Science I 
MAPA DE CONCEPTOS
                     Repaso Data Science-I
 Fases de        Numpy y        Visualización   Estadística 
proyecto de DS    Pandas                         Descriptiva
Fases iniciales  Numpy           Matplotlib      Tipos de 
                                                 variables
                                                Medidas de 
Fases finales    Pandas          Seaborn      tendencia central 
                                                y dispersión
Repaso: 
Fases de un proyecto DS
Fases iniciales de un 
proyecto DS
   1                2               3                 4                5
Definición de     Contexto         Problema            Data       Exploratory Data 
objetivo       Comercial       Comercial         Acquisition    Analysis (EDA)
Fases finales de un 
proyecto DS
  6               7               8                 9               10
Data        Selección del  Desarrollo del     Validación y    Conclusiones
Wrangling        algoritmo       algoritmo        despliegue
(Munging)        apropiado
1. Definir el                                2. Contexto 
objetivo                                     comercial
Son las resultados que se planean             Se puede considerar como 
alcanzar al finalizar el proyecto. En         una breve introducción al 
proyectos de DS es muy                        problema de estudio 
importante delimitar el alcance de            teniendo en cuenta 
tal manera que lo que se propone              contexto, agentes y causas 
sea alcanzable y satisfaga los                involucradas en el problema 
requerimientos deseados.                      de interés. 
3. Problema                                 4. Data Acquisition 
comercial
En esta fase de debe generar                Es el proceso de 
una pregunta problema a                     recopilación de datos tipo: 
resolver, debe ser acotada y                First, Second o Third Party. 
debe guardar relación con el                Se debe identificar, 
objetivo que se quiere                      comprender y evaluar qué 
alcanzar                                    tipos de datos permitirán 
                                           resolver el problema
5. Exploratory Data                            6. Data Wrangling 
Analysis (EDA)                                 (Munging)
Proceso crítico donde se realizan               Se conoce también como 
análisis iniciales sobre los datos              Data Cleaning, Data Munging 
para descubrir patrones,                        o Data Remediation. Es un 
detectar anomalías, probar                      proceso para transformar los 
hipótesis y verificar suposiciones              datos de su forma original a 
con la ayuda de estadísticas y                  un formato que sea más fácil 
representaciones gráficas                       de acceder y analizar
  7. Selección del                               8. Desarrollo del 
        algoritmo                                       algoritmo
En esta etapa se define qué algoritmo se      Luego de elegirse el algoritmo se deben 
va  a  usar  (Machine  Learning,  Deep        chequear  parámetros  e  hiperparametros 
Learning, Optimización). Para el caso de      de  forma  que  el  algoritmo  funciones  en 
Machine Learning podemos hacer uso de         las mejores condiciones.
Aprendizaje Supervisado, No supervisado 
o por refuerzo
    9. Validación y                                  10. Conclusiones
        despliegue
La validación es el proceso mediante el           Desarrollo  de  insights,  consideraciones, 
cual  se  mide  la  efectividad  de  un           recomendaciones,      y   limitaciones    de 
algoritmo  (Cross  validation,  Train/test,       acuerdo a los resultados obtenidos. 
LOCV).
El despliegue es el método mediante el 
cual se integra un modelo a un entorno 
de  producción  con  el  fin  de  tomar 
decisiones comerciales basadas en datos
Ejemplo
         1. Definir el                          2. Contexto comercial
             objetivo
Diseñar un sistema que permita detectar                Trabaja  en  el  equipo  comercial  de 
patrones      entre     consumidores       para        detección de fraudes para una compañía 
identificar   posibles     fraudes  en  una            de  seguros  y  su  función  consiste  en 
compañía de seguros.                                   determinar  qué  reclamos  presentados 
                                                       deben aprobarse.
      3. Problema                            4. Data Acquisition 
         comercial
¿Existen  patrones  particulares  en  los      Datos no etiquetados proporcionados por 
grupos  de  reclamos  presentados  que         el  departamento  de  contabilidad  en 
puedan ser indicativos de fraude?              formato  .csv  con  4  variables  (edad, 
                                              género,  ingreso  y  reclamos)  y  3000 
                                              instancias  cubriendo  el  periodo  2000-
                                              2010
5. Exploratory Data                                  6. Data Wrangling 
     Analysis (EDA)                                         (Munging)
Se  realizan  gráficos  de  distribución  de       Se realiza limpieza de nulos, duplicados y 
salarios   y   reclamos.  Diagramas  de            atípicos. También se hace un proceso de 
dispersión  entre  edad  vs  salario  y            estandarización      de     las     variables 
reclamos  vs  salario  para  entender              numéricas y selección de variables 
posibles  asociaciones  con  características 
particulares
  7. Selección del                            8. Desarrollo del 
        algoritmo                                    algoritmo
Como se cuenta con datos no etiquetados      Se  realiza  el  proceso  de  entrenamiento 
se  elige  el  Aprendizaje  no  supervisado  con  la  matriz  de  diseño  X  utilizando  la 
(Algoritmo:  k-means),  debido  a  que  se   librería  Scikit  Learn,  específicamente  la 
eliminaron atípicos y se cuenta con pocos    función KMeans en Python. 
datos
    9. Validación y                                10. Conclusiones
        despliegue
El  algoritmo  es  sensible  al  número  de       Descubrimiento  de  grupo  problemático 
grupos  elegidos  (método  del  codo  para        con  características  particulares(  bajos 
selección de valor óptimo de k), métrica          ingresos,  altos  reclamos  y  edades  entre 
usada         (distancia       euclidiana)e       25-45 años)
inicialización  de  clusters  por  medio  del 
método (kmeans++)
El despliegue se realiza por medio de un 
dashboard con actualización quincenal 
Para pensar
Teniendo en cuenta el proyecto que 
realizaron en Data Science I. ¿Hasta qué 
etapa de desarrollo consiguieron llegar? 
¿Cuál consideran que es la etapa más 
importante de la secuencia de trabajo?
Contesta la encuesta de Zoom 
Repaso:
Numpy y Pandas
 Numpy
Numpy
NumPy es un proyecto de código abierto que tiene                  NUMeric    PYthon
                                                                 al
como objetivo permitir la computación numérica con                 Potente estructura de 
Python.  Fue  creado  en  2005,  basándose  en  el                       datos
trabajo  inicial  de  las  bibliotecas  Numeric  y 
Numarray.                                                         Implementa matrices y 
                                                                      matrices 
NumPy siempre será un software 100% de código                      multidimensionales
abierto, de uso gratuito para todos y publicado bajo 
los términos liberales de la licencia BSD modificada
                                                                    Estructuras que 
                                                                  garantizan cálculos 
                                                                 eficientes con matrices
Fuente: https://numpy.org/
Arrays como estructura 
fundamental
✔ Estructura de datos que permite 
almacenar información de un mismo 
tipo (típicamente números)
✔ Es bastante eficiente a la hora de 
realizar operaciones ya que hacen 
uso de operaciones vectorizadas.
✔ Los np.arrays pueden almacenar 
vectores (1D), matrices (2D) o 
tensores (3D o más dimensiones)
Tipos de datos y 
propiedades de arrays
                           Propiedades de los numpy array
Tipos de datos posibles en numpy array
Indexación de arrays
                                                                 Selección          Sintaxis
                                                                 Un elemento       array[posición]
                                                                Varios elementos   array[inicio:fin]
                                                                 consecutivos
                                                                 Elementos en     array[[p1,p2,....,p
                                                                orden cualquiera        n]]
  Importante: Los índices pueden ser negativos o
  positivos 
                                              Operaciones 
                                                  básicas
              Reshape                          Concatenación                           Splitting
    Permite modificar la dimensión       Permite concatenar arrays por        Permite separar arrays por una 
    de  un  array  (siempre  que  el     una    dimensión     específica      dimensión  específica  siempre 
    producto  de  las  dimensiones       siempre     y    cuando     las      y  cuando  las  dimensiones  lo 
    coincidan)                           dimensiones lo permitan              permitan
             np.reshape()                      np.concatenate()                         np.split()
Ejemplo en vivo
Utilizaremos el notebook llamado 
Numpy.ipynb dentro de la carpeta de 
clase para repasar los conceptos 
fundamentales de Numpy (Creación y 
propiedades de arrays, indexación, 
reshape, concatenación y split)
Pandas
Pandas
                                                                  Python Data Analysis 
Pandas es una librería que facilita la manipulación                     Library 
de  datos  en  Python  por  medio  de  estructuras  y                  Limpieza de datos
métodos para el manejo de grandes volúmenes de 
datos.
Permite el acople con la librería NumPy y altamente                   Manipulación de datos
compatible  con  diferentes  tipos  de  formatos  de 
datos (csv, txt, sql, json, xls, xlsx, odf)
                                                                       Alto performance, 
                                                                    visualización y agrupación 
                                                                           de datos
Fuente: https://pandas.pydata.org/
Series       DataFrames
 Lectura de datos con Pandas
      df= pd.read_csv('pokemon_data.csv',sep=',')              df= pd.read_csv('pokemon_data.txt',delimiter='\t')
 Lectura de datos con Pandas
                                                                  url = 
                                                                  'https://raw.githubusercontent.com/JJTorresDS/stoc
         df= pd.read_excel('pokemon_data.xlsx')                   ks-ds-edu/main/stocks.csv'
                                                                  df = pd.read_csv(url, index_col=0)
 Lectura de datos con Pandas
         df= pd.read_sql('archivo_base_datos.sql')                         df = pd.read_json('file_d.json')
      Características                         Numpy                                       Pandas
      Homogeneidad            Los arrays son elementos                   Los DataFrames de pandas permiten 
                              homogéneos (mismo tipo de data)            tener datos heterogéneos
        Mutabilidad           Los arrays son mutables                    Los DataFrames son mutables
          Acceso              Se puede acceder a los arrays              Se puede acceder a los Dataframes 
                              usando posiciones enteras                  usando enteros e índices
        Flexibilidad          Los arrays no tienen la flexibilidad de    DataFrames son mucho más flexibles 
                              manejar datos dinámicos secuencias         para manipular tipos de datos de 
                              y mezcla de tipos de datos                 diversos orígenes con múltiples 
                                                                         funciones
      Tipos de datos          Están diseñados para trabajar con          Están diseñados para trabajar con datos 
                              datos numéricos                            tabulares
          Tiempos             Mucho más tiempo de carga y                Menor tiempo de carga y procesamiento
                              procesamiento
Ejemplo en vivo
Utilizaremos el notebook llamado 
Pandas.ipynb dentro de la carpeta de 
clase para repasar los conceptos 
fundamentales de Pandas (Creación y 
manipulación de series y Dataframes, 
filtros, lecturas de archivos en formato csv, 
txt, xlsx, json)
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Repaso:
Visualización
Matplotlib
Matplotlib
Es una librería básica en Python para la generación 
de visualizaciones con alto nivel de personalización.                     Fácil y extensible
Utiliza herramientas de la GUI de Python como PyQt, 
TKinter y WxPython. Además hace uso de Numpy
                                                                        Open Source y de alto 
Es la librería que sirve como fundamento para otras                             nivel
como Seaborn, Bokeh, Pygal y Plotly.
                                                                        Orientado a objetos y 
                                                                               portable
Fuente: https://matplotlib.org/
Interfaces Matplotlib
                   ✔ La  interfaz  orientada  a  estados  es 
                     similar a la interfaz gráfica de MATLAB, 
                     donde se pueden modificar atributos del 
                     gráfico  y  ejes  y  no  permite  reutilizar 
                     objetos.
                   ✔ La interfaz orientada a objetos es mucho 
                     mejor  alternativa  si  deseamos  crear 
                     varios  gráficos  al  tiempo.  Las  figuras 
                     pueden ser guardadas como un objeto y 
                     pueden ser utilizadas posteriormente o 
                     ser  exportadas en un formato deseado 
                     (pdf, jpg, png, jpeg, tiff, svg)
   1. Gráfico de líneas                                                          2. Gráfico de puntos
                                                                            fig, ax = plt.subplots()
                                                                            ax.scatter(alturas, pesos, alpha=0.7)
    fig, ax = plt.subplots()                                                ax.set_title('Altura vs. Peso de 50 alumnos')
    ax.plot([0, 1, 2, 3, 4, 5, 6], [1, 5, 2, 4, 8, 9, 2])                   ax.set_xlabel('Altura (cm.)')
                                                                            ax.set_ylabel('Peso (kg.)')
            3. Gráfico de                                                        4. Histograma
                     barras
                                                                     fig, ax = plt.subplots(figsize=(8, 4))
   fig, ax = plt.subplots(figsize=(8,4))                             ax.hist(df_lluvias.values.flatten(), bins=10)
   p_acumuladas = df_lluvias.sum()                                   ax.set_title('Histograma de prec.')
   ax.bar(df_lluvias.columns, p_acumuladas)                          ax.set_xlabel('Intervalos de prec. (mm.)')
                                                                     ax.set_ylabel('Frecuencia absoluta')
                5. Boxplot                                                             6. Piechart
                                                                         cars = ['AUDI', 'BMW', 'FORD', 'TESLA', 'JAGUAR', 
      fig, ax = plt.subplots(figsize=(8, 4))                             'MERCEDES']
      ax.boxplot(df_lluvias.T)                                           data = [23, 17, 35, 29, 12, 41]
      ax.set_title('Boxplot de prec.')
      ax.set_xlabel('Meses')                                             fig,ax = plt.subplots(figsize =(10, 7))
      ax.set_ylabel('Precipitacione (mm)')                               ax.pie(data, labels = cars)
Seaborn
Seaborn
Es  una  librería  con  un  conjunto  de  funciones 
internas   para    realizar   mapeo  semántico  y                          Funciones simples
agregación estadística .
Permite  generar  visualizaciones  con  excelentes 
acabados  estéticos  sin  tener  que  utilizar  mucho                    Open Source y de alto 
código.                                                                          nivel
Proporciona una interfaz de alto nivel para realizar                     Orientado a objetos con 
gráficos estadísticos atractivos e informativos.                             estilos y temas 
                                                                               elaborados
Fuente: https://seaborn.pydata.org/
Tipos de funciones 
Seaborn
                                    Funciones Figure-
                                    level
    Funciones Axes-
    Level
1. Gráfico de líneas                               2. Gráfico de puntos
flights = sns.load_dataset("flights")          tips = sns.load_dataset("tips")
may_flights = flights.query("month=='May'")    sns.scatterplot(data=tips, x="total_bill", 
sns.lineplot(data=may_flights, x="year",       y="tip", hue="time")
y="passengers")
     3. Gráfico de                                  4. Histograma
           barras
tips = sns.load_dataset("tips")            penguins = sns.load_dataset("penguins")
# Axis-level                               # Axis-level
ax = sns.barplot(x="day",                  sns.histplot(data=penguins, 
y="total_bill", data=tips)                 x="flipper_length_mm")
         5. Boxplot                                         6. Piechart
                                                 data=[data1, data2, data3, data4]
 penguins = sns.load_dataset("penguins")         labels=["label1",”label2”, “label3”, 
 ax = sns.boxplot(x="day", y="total_bill",       “label4”]
 data=tips)                                      colors= sns.color_palette("pastel")[0:5]
                                                 plt.pie(data=data, labels= labels,colors= colors, 
                                                 autopct= "%.0f%%")
Matplotlib vs 
Seaborn
     Características                      Matplotlib                                   Seaborn
      Funcionalidad          Gráficos básicos                         Temas fascinantes. Compila datos a 
                                                                      gráficos
         Sintaxis            Sintaxis larga y compleja e.g            Sintaxis simple y fácil de entender e.g 
                             matplotlib.pyplot.bar(x_axis, y_axis)    seaborn.barplot(x_axis,y_axis)
    Múltiples figuras        Se pueden tener figuras múltiples        Puede tener más problemas de 
                             simultáneamente                          memorias
       Flexibilidad          Altamente customizable y robusto         Evita superposición de temas 
  DataFrames y Arrays        Funciona eficientemente y trata a        Más funcional y organizado y trata 
                             figuras y ejes como objetos              datasets como unidad simple
      Casos de uso           Gráficas diversas usando Numpy y         Versión extendida de Matplotlib con el 
                             Pandas                                   uso de Numpy y Pandas
Otras librerías para 
visualizaciones
Otras librerías
Ejemplo en vivo
Utilizaremos el notebook llamado 
Matplotlib.ipynb y Seaborn.ipynb 
dentro de la carpeta de clase para repasar 
los conceptos fundamentales de las 
librerías básicas para visualizaciones
Repaso: 
Estadística Descriptiva
Tipos de variables
                                            Categorías 
                          Nominal           mutuamente 
                                           exclusivas sin un 
                                           orden implícito
           Cualitativas                     Categorías 
                          Ordinal           mutuamente 
                                          exclusivas con un 
                                           orden implícito
Tipos de 
variables 
                                          Variables numéricas 
                          Discretas       que sólo admiten 
                                          números enteros
          Cuantitativas
                                          Variables numéricas 
                          Continuas         que admiten 
                                          números reales 
Medidas de tendencia 
central
Media: representa el 
promedio de los datos, 
también existe la media 
recortada
Mediana y cuartiles: 
representan medidas de 
localización en la distribución 
de la variable númerica
Moda: representa el valor de 
mayor frecuencia. No existe 
en todas las ocasiones
Medidas de dispersión
Varianza: es una medida que cuantifica la variabilidad de los datos, su principal 
desventaja es que se encuentra en unidades al cuadrado lo que hace que la 
interpretación sea compleja
Desviación estándar: es la raíz cuadrada de la varianza tiene la ventaja de que se 
puede interpretar como un valor de dispersión alrededor de la media.
Rango intercuartílico: medida robusta de dispersión definida como el Q3-Q1 la cual es 
resistente a atípicos y permite comprender mejor la distribución y variabilidad de los 
datos.
Ejemplo en vivo
Utilizaremos el notebook llamado Estadística 
Descriptiva.ipynb dentro de la carpeta de 
clase y repasaremos conceptos como: 
pruebas de hipótesis, p-valor, intervalos de 
confianza, cálculo de estadísticas descriptivas 
básicas e inferencia estadística.
  Accediendo a notebooks 
       del Curso DS-I
Realizaremos una copia de todos los notebooks 
disponibles del Curso de DS-I en nuestras cuentas 
           personales
        Duración: 10-15 min
ACTIVIDAD EN CLASE
Accediendo a 
notebooks del Curso 
Descripción de la actividad. 
DS-I
Se propone que los estudiantes puedan descargar los 
archivos en formato .ipynb disponibles para cada clase, 
pueden hacer una copia en sus cuentas de google.
En la carpeta de clase encontrarán dos subcarpetas: 
una llamada Notebooks a repasar donde trabajaremos 
conceptos de las librerías Pandas, Matplotlib y Seaborn, 
fundamentales para desarrollar un correcto EDA. En la 
subcarpeta Datos se encuentra la data para trabajar
Tiempo estimado 20-25min
CLASE N°22 
Glosario
Numpy: librería de Python que nos permite         Series: estructuras 1D en Pandas que 
trabajar con matrices y vectores de forma         almacenan vectores con índice, nombre y 
sencilla y potente                                valores
Array: estructura fundamental en Numpy que        DataFrames: estructuras 2D (filas x 
solo permite un solo tipo de dato haciéndolo      columnas) que son la generalización de Series 
eficiente para operaciones de alta complejidad,   en Pandas
pueden ser de 1D (vectores), 2D (matrices) o 
3D (tensores)                                     Matplotlib y Seaborn: Librerías gráficas de 
                                                 Python que permiten elaborar gráficos de alta 
Pandas: librería fundamental que nos permite      calidad con diferentes propósitos.
trabajar con archivos planos (.csv, .txt, xlsx) en 
Python                                            Estadística Descriptiva: Rama de la 
                                                 estadística que se encarga de descubrir 
                                                 patrones en los datos por medio de resúmenes 
                                                 numéricos y gráficos 
    ¡Atención!
Recuerda instalar Python para la próxima clase.
    Ver Guía de Instalación
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
 MATERIAL AMPLIADO
Recursos multimedia
Numpy y Pandas
✓ Numpy | Numpy team |          Estadística inferencial
 numpy library                 ✓ What is inferential Statistics? | Essa 
✓ Pandas | Pandas team |            W | Inferential Statistics
 Pandas library
Matplotlib y Seaborn
✓ Matplotlib | Matplotlib team | 
 Matplotlib
✓ Seaborn | Seaborn team | Seaborn
Estadística Descriptiva
✓ Descriptive Statistics | Adam 
Disponible en nuestro repositorio.
 Hayes, Andy Smith, Michael 
 Logan | Estadística Descriptiva
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Repaso de fases de un proyecto de DS
      ✓ Repaso de Numpy y Pandas
      ✓ Repaso de visualizaciones (Matplotlib + Seaborn) 
        y EDA
      ✓ Repaso de estadística descriptiva
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 21. DATA SCIENCE
Stack Tecnológico II
Temario
            20                 21                  22
          Stack               Stack            Retomando 
      tecnológico I       Tecnológico II        impulso 
                          ✓ Sistema in-       ✓ Fases de un 
       ✓ Bases de            house               proyecto de DS 
          Datos                               ✓ Numpy y Pandas
                          ✓ Cloud 
       ✓ Lenguajes           Computing        ✓
          DS                                     Visualización
                          ✓ Fundamentos       ✓ Estadística 
       ✓ Visualización       del Big Data
          de datos                               Descriptiva
                          ✓ ETL
Objetivos de la clase
         Explorar las potencialidades de Cloud 
         Computing.
         Identificar conceptos de Big Data y sus 
         herramientas.
         Interpretar el proceso de ETL y su aplicación 
         dentro del mundo de la Ciencia de Datos.
MAPA DE CONCEPTOS                            Definición
                                             Tipos
                        Cloud 
                        Cloud 
                        Computing
                        Computing
  Stack Tecnológico                          Ventajas y 
  II                                         desventajas
                                             Proveedores
                                             Fundamentos
                                             impulsores
                        Big Data
                        Big Data
                                             Herramientas
                                             Definición
                        ETL
                        ETL                  On premise
                                             On cloud
Sistemas
in-house
Servidor In-House
                    ✓ Un servidor In-house es aquel que 
                      almacena y mantiene en el espacio 
                      físico en un entorno local. 
                    ✓ Por lo tanto, no hay necesidad de una 
                      conexión a Internet para acceder a 
                      sus datos. 
                    ✓ Permite el control físico sobre su 
                      copia de seguridad en todo momento.
                    ✓ Pueden ser viables para pequeñas 
                      empresas 
 Ventajas y desventajas In-
                       Ventajas                                          Desventajas
 House
    Seguridad. Manejo de datos sensibles sin            Requiere de grandes inversiones iniciales para 
    terceros                                            infraestructura y hardware
    Sin tantos gastos fijos (pago a terceros por        Gastos variables por consumo y espacio 
    servicio)                                           además de mantenimiento
    Posibilidad de autogestión dentro de la             Tiene una carga máxima (capacidad de 
    compañía.                                           procesamiento)
    No requiere de conexión a Internet (no es           Requiere de personal capacitado para resolver 
    limitante velocidad de conexión)                    problemas
    Hay control sobre la infraestructura (control de    Pierde flexibilidad (acceso remoto) además de 
    acuerdo a las necesidades)                          ser susceptible a perdida de datos (daños 
                                                        estructura)
Fuente:  Kharb L. et al. (2018. A Comprehensive Study of Security in Cloud Computing) 
Resumen: Ventajas in-
Customización: Con nuestra propia 
infraestructura de redes y de datos podemos 
house
obtener una mayor customización de 
recursos por ejemplo en base a los 
requerimientos de la organización.
Conocimiento: El conocimiento de los sistemas y 
datos reside únicamente in-house. Esto puede 
ser preferible para compañías con 
requerimientos muy específicos de 
seguridad.
Sin costos mensuales: Usualmente, los planes de 
deployment on-premise se estructuran en planes 
anuales o multi-anuales, potencialmente 
eliminando la necesidad de costos 
mensuales.
Resumen: Desventajas in-
Elasticidad:  Falta de elasticidad para el uso de 
más recursos en caso de ser necesario dado un 
house
pico de demanda. 
Costos de Infraestructura:  Los costos de 
infraestructura para montar un datacenter 
propio son mucho más superiores que si 
adquirimos una licencia con algún proveedor de 
servicios Cloud.
Personal altamente capacitado:  Si quisiéramos 
tener nuestra infraestructura In – House 
deberemos contar con personal altamente 
capacitado en temas de Seguridad e 
Infraestructura informática.
Cloud
Computing
Cómputo en la nube
Cloud Computing es una tendencia en alza 
especialmente dentro del mundo de la Analítica 
de Datos y como Data Scientist resulta realmente 
muy importante que podamos entender qué es el 
Cloud, sus características y cuáles son los 
diferentes servicios que podemos utilizar en la 
nube para complementar nuestro Stack 
Tecnológico como Científicos de Datos.
Arquitectura Cliente - 
Servidor
                          Claves de la arquitectura
                          → Cliente
                          → Servidor
¿Qué es el Cloud 
Computing?
                                 Nueva tecnología que nos permite acceder 
                              mediante un sistema remoto, al software, al 
                                      procesamiento de datos y al 
                                     almacenamiento de archivos.
                                            Tipos disponibles
                                      Nube pública      Nube privada
Beneficios de Cloud 
Computing 
Existen muchas ventajas, 
mencionaremos las más 
en Big Data
relevantes:
✔ Abaratamiento de costos
✔ Inmediatez
✔ Capacidad de proceso
✔ Concurrencia
✔ Seguridad
 “El Cloud Computing, permite la 
 aceleración y gestión de procesos 
 computacionales, haciéndolos más 
 eficientes”
                                       -   Observatorio Nacional de Telecomunicaciones y de la 
                                                                                                                   SI
   Ventajas y desventajas 
                         Ventajas                                         Desventajas
   Cloud
     Pocos costos operacionales (proveedor se encarga   Sistemas de seguridad que tienen políticas y 
     de mantenimiento, equipos y red de servidores)     términos a veces desconocidos
     Hardware y software de punta (CPU, GPU), IA, ML,   Esquemas de costos complejos basados en 
     DL                                                 consumo y servicios ocupados
     Acceso rápido por conexión con alto Gbps y         Existen costos grandes cuando se transfiere la 
     muchos puntos de intercambio                       información de una organización a otra
     Permite el acceso remoto y conectividad rápida     En algunos casos el soporte al cliente no es 
                                                        óptimo
     Infraestructura más segura y sistemas de backup    Requiere de conexiones estables e internet 
     regulares                                          constante para acceso remoto
Fuente:  Kharb L. et al. (2018. A Comprehensive Study of Security in Cloud 
Computing) 
 Resumen: Ventajas cloud
  Escalabilidad:   uno     puede     construir   muy 
  rápidamente  una  nueva  infraestructura  con 
  aplicaciones y alta escalabilidad. La carga sobre 
  la  aplicación  crece  horizontalmente  por  las 
  provisiones de servidores.
  Agilidad:  Desarrollo  rápido  de  aplicaciones  por 
  tecnología  de  punta  hace  que sea  un  sistema 
  ágil y eficiente. 
  Accesibilidad y back-ups: Se tienen sistemas que 
  permiten  alta  concurrencia  y  accesibilidad 
  rápida.  Además  se  tiene  la  posibilidad  de 
  generar back-ups de manera recurrente.
   Resumen: Desventajas 
    Costos variables:  Los contratos con proveedores 
   cloud
    suelen ser bastante rígidos y el consumo junto 
    con el almacenamiento físico puede ser variable
    Control  de  Infraestructura:    No  se  tiene  la 
    posibilidad de controlar la infraestructura a 
    la    conveniencia  del  momento  según  las 
    necesidades lo cual genera restricciones.
    Conexión continua:  Se requiere de sistemas de 
    conexión  a  internet  de  buena  calidad  y 
    estables para que no se generen problemas 
Tipos de nubes y 
servicios
Modelos de Servicios en la 
Nube
✔ Infraestructura como servicio ������ IaaS
✔ Plataforma como servicio ������ PaaS
✔ Software como servicio ������ SaaS
✔ Storage as a Service ������ (STaaS)
✔ Data as a Service ������ (DaaS)
✔ Functions as Service ������ (FaaS)
Tipos de nubes
          Nube Privada               Nube Pública
           En mi propio              En Azure u otro 
         centro de datos              proveedor de 
                                        servicios
Proveedores
Tipos de proveedores
Como en todo servicio en cuanto a 
servicios en la nube se tienen                  Miremos primero los principales líderes 
proveedores líderes del mercado como:           del mercado…
Amazon Web Services, Google Cloud y 
Microsoft Azure.
Sin embargo existen otros que son los 
retadores en el mercado como: Alibaba 
Cloud, Oracle, IBM, Tencent Cloud entre 
otros.
Amazon Web 
Services
Quizás es una de las plataformas de servicios en la 
nube más conocidas, proporciona una variedad de 
servicios de infraestructura, tales como 
almacenamiento, redes, bases de datos, servicios de 
aplicaciones, potencia de cómputo, mensajería, 
inteligencia artificial, servicios móviles, seguridad, 
identidad y conformidad, entre otros. 
Principal ventaja → la experiencia en el mercado 
y el servicio de atención al cliente que ofrece ������
Microsoft Azure
Microsoft Azure, proporciona una gama de servicios 
en la nube, incluidos servicios vinculados con la 
computación, analítica, almacenamiento y redes. 
Los usuarios pueden elegir entre estos servicios 
para desarrollar y escalar nuevas aplicaciones, o 
ejecutar aplicaciones existentes. 
Principal ventaja →  la rapidez ⏱
Google Cloud 
Platform
Google Cloud Platform, también conocida por la 
nomenclatura GCP, es una suite que contiene 
diversos servicios que funcionan en la misma 
infraestructura que utiliza Google de manera 
interna, por ejemplo con servicios como Youtube o 
Google Search. 
Principal ventaja → la seguridad ⏱ 
               Otros 
               ✔ Dentro de los otros proveedores uno de los más 
               proveedores
                 completos es Alibab Cloud que ha tomado 
REEMPLAZAR        renombre en los últimos años con servicios de 
POR IMAGEN        Web hosting, Servidores virtuales y capacidad de 
                 cálculo comparable con AWS, Azure y GCP
               ✔ Otros que también ofrecen beneficios similares 
                 son IBM cloud y Oracle, la única debilidad de 
                 Oracle es que no tiene sistemas de Shared Web 
                 Hosting. 
               ✔ El que menos adeptos tiene es TencentCloud por 
                 ser un sistema bastante reciente.
¿Cuál elegir?
La elección deberá realizarse en función de las 
necesidades computacionales de la 
organización y acompañado por la asesoría de un 
profesional capacitado en la temática. Quizás, una 
buena recomendación antes de tomar una decisión 
definitiva al respecto, sería tomar una prueba 
gratuita de cada una de las herramientas, para 
vivenciar de esa manera la experiencia de utilizar 
cada una de ellas. 
Para pensar
¿Qué proveedor han utilizado en sus 
trabajos?
¿Cuál de ellos conoces?
Contesta mediante el chat de Zoom 
         ☕
       Break
       ¡10 minutos y 
        volvemos!
          ¡Lanzamos la
          Bolsa de 
          Empleos!
         Un espacio para seguir potenciando tu carrera y 
         que tengas más oportunidades de inserción 
         laboral.
         Podrás encontrar la Bolsa de Empleos en el menú 
         izquierdo de la plataforma.
         Te invitamos a conocerla y ¡postularte a tu futuro 
         trabajo!
           Conócela
Fundamentos del
Big Data
Fundamentos 
generales
Colección de grandes volúmenes de datos,                          REEMPLAZAR 
complejos y muy difíciles de procesar, a través de                 POR IMAGEN
herramientas de gestión y procesamiento de datos 
tradicionales.
               Impulsores 
 REEMPLAZAR    Sin dudas existen múltiples drivers del Big Data, 
 POR IMAGEN    podemos mencionar:
               ✔ IoT (Internet of Things)
               ✔ Redes Sociales facilitando interacción
               ✔ El internet
               ✔ Tics, Instrumentación, Sensores, Tecnología
               ✔ Cloud Computing
Las 3 V del Big Data
                        Volume
                          n
                        Big 
                        Data
               Velocidad        Variedad
Principales 
herramientas
Hadoop
Framework implementado en Java que         ¿Para qué sirve? 
permite el almacenamiento y                     → Almacenar archivos de manera 
procesamiento distribuido de grandes       distribuida. 
conjuntos de datos estructurados, semi-         → Procesar archivos de manera 
estructurados y no estructurados. Está     distribuida.
diseñado para trabajar en clústers y tiene 
una alta tolerancia a fallas. 
Apache Sqoop
Herramienta que está dentro de proyecto 
Apache, y además forma parte del 
ecosistema Hadoop. Sqoop nos permite la 
transferencia de datos entre un RDBMS 
como MySQL o oracle y HDFS.
Hive
Software que facilita leer, escribir y              ������ MR es un modelo de programación 
manejar largos conjunto de datos, que               utilizado en Hadoop que facilita el trabajo 
residen en un almacenamiento                        con grandes volúmenes de datos 
distribuido, usando SQL.                            distribuidos a través de múltiples 
                                                    máquinas dentro de un clúster.
Cluster Computing
Proceso de compartir las tareas de computación 
entre varias computadoras y esas computadoras o         1. Motor de búsqueda de 
máquinas forman el clúster (sistema distribuido            Google 
con las redes).                                         2. La simulación de 
Diferentes preferencias arquitectónicas, como              terremotos
clústeres de equilibrio de carga, clústeres de alta     3. Simulación de yacimientos 
disponibilidad (HA) y clústeres de alto rendimiento        de petróleo 
(HP).                                                   4. Sistema de pronóstico del 
Algunas de las ventajas son la velocidad de                tiempo.
procesamiento, la rentabilidad, la 
escalabilidad y la alta disponibilidad de 
recursos.
Spark
Apache Spark es un framework de            Sus propias características:
propósito general para el procesamiento      ✔ Proyecto de código libre
de datos en un clúster.                      ✔ Framework de procesamiento 
                                               unificado (Batch y Streaming)
                                            ✔ Escalable y tolerante a fallos
                                            ✔ Potente motor de procesamiento de 
                                               datos masivos en memoria (in-
                                               memory processing) sobre un cluster 
                                               (Spark está integrado con Hadoop, 
                                               pero no depende de él
Hadoop vs Spark
                                   Hadoop                    Spark
  Almacenamiento                   Solo disco             Memoria disco
  Operaciones                    Map y Reduce       Map, Reduce, Join,Sample, 
                                                              etc.
  Modelo de ejecución               Batch                    Batch, 
                                                       interactivo,streaming
  Entornos de programación           Java              Scala, Java, R ,Python
Para pensar
¿Para qué contextos te parece más útil el 
Cloud Computing y en cuáles no? 
Contesta mediante el chat de Zoom 
Servicios en la nube 
y Cloud Computing
     Duración: 15 minutos
ACTIVIDAD EN CLASE
Servicios en la 
nube y Cloud 
Descripción de la actividad. 
Computing
Se propone elegir alguno de los siguientes proveedores 
en la nube:
1. Google Cloud
2. Amazon Web Services (AWS)
3. Microsoft Azure
Identificar 3 fortalezas y debilidades. Además encontrar 
cuáles son los principales usos más comunes para los 
sistemas cloud en la actualidad.
 ETL
Extraer, transformar 
y cargar
Extraer, transformar y cargar (ETL, Extract, Transform, Load) es el proceso de 
compilación de datos a partir de un número ilimitado de fuentes, su posterior 
        organización y su carga en un repositorio.
Principales 
herramientas
On premise
Lanzada en el 2006, es una plataforma de integración completa de ETL que cubre los 
requisitos de integración de datos. Además, también ofrece un entorno gráfico para 
construir, gestionar y mantener procesos de integración de datos en sistemas de 
             inteligencia empresarial.
On premise
Ofrece una variedad de productos para realizar la integración de datos y los procesos de 
ETL. Es open source y realmente muy potente, ya que funciona bien con integraciones 
          para Google Big Query o Redshift.
On premise
Proporciona una herramienta de diseño visual, que simplifica significativamente la 
construcción, ejecución y mantenimiento de los procesos de integración de 
datos empresariales. También, cuenta con una potente interfaz fácil de usar y amigable 
               con el usuario.
On premise
Componente de Microsoft SQL Server utilizado para la extracción, transformación y carga 
de datos (ETL). De las más reconocidas y utilizadas actualmente en el mercado. 
On cloud
Azure Data Factory es un servicio serverless de ETL el cual nos permite realizar procesos 
de data integration, data transformation y data load. A su vez ofrece una Interfaz gráfica 
       muy intuitiva para la aplicación y creación de los ETL.
On cloud
AWS Glue:
Servicio administrado del cloud AWS que nos permite hacer procesos ETL 
de una manera visual para extraer o volcar datos a AWS.
On cloud
Data Flow nos permite desarrollar ETL desde cero utilizando un pipeline o flujos de datos, 
también nos ayuda a realizar la ingesta de la información en tiempo real o en Batch de 
         acuerdo al requerimiento de la empresa.
Cuestionario final
¡Ya estamos terminando! ¿Testeamos 
lo que aprendimos? ⏱⏱
Te compartimos a través del chat de 
Zoom / chat de la plataforma el PIN para 
sumarte al Kahoot.
Duración: 10 minutos
Actividad colaborativa
Socialización y consulta de Proyectos - 
II
Avanzaremos resolviendo consultas 
asociadas al proyecto del curso mediado 
por tutores
Realizaremos la actividad en grupos de 3-4 
personas 
Duración: 15 minutos
     ACTIVIDAD COLABORATIVA
Acuerdos
Presencia                                       Apertura al aprendizaje
✓ Participar y “estar” en la clase, que          ✓ Siempre, pero siempre puedes 
    tu alrededor no te distraiga                    seguir aprendiendo. Compartir el 
                                                    conocimiento es válido, la 
Escucha activa                                       construcción colaborativa es la 
                                                    propuesta.
✓ Escuchar más allá de lo que la 
    persona está expresando 
    directamente                               Todas las voces
                                                 ✓ Escuchar a todos, todos podemos 
                                                    reflexionar. Dejar el espacio para 
                                                    que todos podamos participar.
    ACTIVIDAD COLABORATIVA
Socialización y consulta 
de Proyectos - II
Consigna: Esta actividad colaborativa 
interactuamos con compañeros y tutores       Se propone resolver consultas 
con eje central el proyecto del curso.       metodológicas o de forma acerca del 
                                           proyecto final. 
                                           Identificar fortalezas y debilidades que 
                                           pueden servir como punto de mejora para 
                                           la entrega del proyecto.
                                           Se propone que los estudiantes muestren 
                                           los avances a los tutores y compañeros 
                                           para recibir retroalimentación de su 
                                           proyecto.
NOTA: usaremos los breakouts rooms. El tutor/a tendrá el rol de facilitador/a.
         Primera entrega 
      de tu Proyecto final
   Debes entregar el Análisis de datos con Python
  correspondiente a la primera entrega de tu proyecto 
                          final.
  Crearás la notebook de un análisis exploratorio de datos sobre un problema a 
 resolver para una industria, negocio o proyecto personal. La notebook contendrá 
 un informe que detalle tus hipótesis primarias y secundarias, el código utilizado 
               para probarlas y su posible resolución.
Recordemos…
                  Aplicamos Feature Selection
                  Elegimos un algoritmo de 
                  Machine Learning 
                  Evaluamos el desempeño del 
                  algoritmo elegido
 Clase 19
Estructurando un 
proyecto de DS-III Obtención de conclusiones con 
                  base en resultados obtenidos
    ENTREGA DEL PROYECTO 
    FINAL
Análisis de datos con 
Python
Objetivos generales                          Formato
✓ Estructurar un problema en función         ✓ Link a repositorio de Github o 
   de múltiples, pero simples                   documento de Jupyter.
   preguntas/hipótesis a responder           ✓ El archivo debe tener el nombre 
✓ Analizar datos tabulares (e.g excel,          “PrimeraEntrega+Apellido.ipynb”
   csv, etc)  usando Python
✓ Utilizar modelos de Machine              Sugerencias
   Learning con Python
                                             ✓ La entrega es individual y se realiza 
                                                a través de la plataforma.
     ENTREGA DEL PROYECTO 
     FINAL
Primera entrega
El modelo de Jupyter Notebook debe contener
 ✓ Abstracto con motivación y audiencia: Descripción de 
     alto nivel de lo que lo motiva a analizar los datos                 Video explicati
     elegidos y qué tipo de audiencia se podría beneficiar de            vo
     este análisis
 ✓ Preguntas/Hipótesis que queremos responder mediante 
     el análisis de datos
 ✓ Estructura acorde.
 ✓ Análisis Exploratorio de datos (EDA): Análisis descriptivo 
     de los datos mediante visualizaciones mediante el 
     análisis de datos
¿Preguntas?
           Resumen 
       de la clase hoy
      ✓ Herramientas de Cloud Computing, de Big Data y 
        ETL
Opina y valora 
esta clase
Muchas 
gracias.
Esta clase va a ser
grabad
  a
      Clase 42. DATA SCIENCE
     Algoritmos de 
     clasificación y 
       Regresión
Temario
               41                      42                     43
         Algoritmos de           Algoritmos de           Algoritmos de 
          clasificación          clasificación y          Agrupación I
                                   Regresión
                               ✓ SVM                     ✓ K means
        ✓ KNN                  ✓ Ejemplos clasificación 
        ✓ Random Forest           errónea                ✓ DBSCAN
        ✓ Regresión            ✓ Regresión Lineal y 
                                  Múltiple
           Logística
                               ✓ Optimización de 
                                  hiperparámetros
Objetivos de la clase
         Profundizar en el Aprendizaje Supervisado 
         Aplicar el algoritmo de SVM
         Aplicar los modelos de Regresión
MAPA DE CONCEPTOS
                        Ejemplos de 
                        clasificación 
                         errónea
                     Algoritmos de              Regresión 
   SVM               clasificación y            Lineal y 
                       regresión                Múltiple
                        Optimización 
                         de hiper 
                        parámetros
Support Vector 
Machine:
  SVM
Definición
Máquinas de soporte vectorial 
SVM por sus siglas en inglés 
(Support Vector Machines), es un 
algoritmo que se puede usar tanto 
para regresión como para problemas 
de clasificación. Es un algoritmo que 
se fundamenta en la construcción de 
hiperplanos de segmentación.
Ejemplo aplicado
Máquinas de soporte vectorial 
 1) Imaginemos que tenemos dos grupos               2) Pero qué pasa si tenemos un punto muy 
 (Verde y Rojo). La línea naranja en este caso      cerca del límite de decisión
 funciona como punto límite para decisión
Máquinas de soporte vectorial 
                                                       Podemos hacer esta 
                                                       clasificación de una mejor 
 3) En este caso lo podríamos clasificar como          forma?
 verde pero no tiene mucho sentido en este 
 caso
Podemos enfocarnos en las observaciones       Para ello podemos usar el valor del punto 
cerca a los límites de los dos grupos         medio entre esos dos puntos
Máquinas de soporte vectorial 
TERMINOLOGÍA
                     Para recordar el margen en teoría 
                     debe ser igual en ambos lados del 
                     threshold para que se tenga una 
                     buena clasificación 
La distancia más corta entre las 
observaciones y el threshold se conoce como Pero no siempre funciona bien! 
margen esto se conoce como Maximal 
Margin Classifier        OUTLIERS
                     PORQUE EN ESTE CASO NO 
                     FUNCIONA?
Máquinas de soporte vectorial 
En este caso esa metodología no             Elegir una zona límite que no sea sensible 
funciona tan bien y es porque los           a outliers se conoce como Bias/Variance 
Maximal Margin Classifiers son muy          Tradeoff y el límite se conoce como Soft 
sensibles a OUTLIERS                        margin
                                                 Support Vector Classifier viene 
                                                 del hecho que se usó un soft 
                                                 margin por medio de support 
                                                 vectors
Máquinas de soporte vectorial 
Máquinas de soporte vectorial 
                 Las observaciones que 
                 quedan dentro del soft 
                 margin quedan mal 
                 clasificadas.
Máquinas de soporte vectorial 
Máquinas de soporte vectorial 
                       Cuando hay 4 o más 
                       dimensiones la máquina 
                       de soporte vectorial es un 
                       hiperplano así como en 
                       el caso 1D, 2D o 3D
Máquinas de soporte vectorial 
                     Las máquinas de soporte vectorial 
                     pueden manejar outliers y problemas de 
                     malas clasificaciones
Ejemplo: Qué pasaría si tuviéramos este caso en el cual los puntos verdes 
representan pacientes recuperados y los rojo no recuperados (El medicamente solo 
funciona en las dosis apropiadas)
Máquinas de soporte vectorial 
En estos casos Maximal Margin Classifiers no funciona tan bien y hablamos de SVM 
(Support Vector Machines)
                               1) Empezar con 
                                data en una 
                                dimensión baja 
                                (1D)
                               2) Mover los datos 
                                a una dimensión 
                                mayor (e.g 2D)
                               3) Encontrar el SVC 
                                que separe los 
                                datos
Ejemplos de 
clasificación errónea
Clasificaciones erróneas
Vamos a suponer que los puntos azules corresponden a la clase «azul» y los 
puntos rojos a la clase «rojo».
Ahora vamos a intentar dibujar una línea 
que separe los puntos azules de los rojos. 
De esta forma, cuando haya un punto nuevo, intentaremos poder 
determinar qué color va a tener, dependiendo del lado de la línea en el 
que se encuentre.
     Algunos ejemplos…
                       En el contexto de los algoritmos de 
                       clasificación podemos tener 
                       muchos falsos positivos y 
                       negativos, es por eso que debemos 
                       tener mucho cuidado a la hora de 
                       usar cualquier algoritmo
                       A continuación veremos algunos 
                       ejemplos de formas equivocadas de 
                       clasificar
Clasificaciones erróneas
En la siguiente figura, podemos decir que lo 
que esté a la izquierda de la línea, es azul y 
lo que esté a la derecha, es rojo. 
Sin embargo, el punto nuevo abajo a la 
izquierda es clasificado como azul 
aunque en realidad debería clasificarse 
como rojo.
Clasificaciones erróneas
Podemos decir que cualquier punto que esté 
por arriba de la línea establecida será azul y 
cualquier otro punto que esté por debajo de 
la línea será rojo. 
Sin embargo, el nuevo punto a la 
derecha, ha sido incorrectamente 
clasificado como azul, cuando debería 
ser rojo.
Clasificaciones erróneas
La línea que mejor distingue las 
zonas de los puntos azules de la 
zona de los puntos rojos es la que 
maximiza el margen entre ambos.
SVM es una técnica de machine 
learning que encuentra la mejor 
separación posible entre clases. 
Para recordar…
Resulta importante mencionar, que 
normalmente los problemas de aprendizaje 
automático tienen muchas dimensiones, por 
lo tanto en vez de encontrar la línea óptima, el 
SVM encuentra el hiperplano que maximiza 
el margen de separación entre clases 
Regresión Lineal 
Simple y Múltiple
Regresión Lineal 
Simple
Regresión Lineal simple
A diferencia de los modelos anteriores, 
es un modelo estadístico que trata 
de explicar la relación que existe entre 
una variable dependiente 
(variable respuesta) y una variable 
independiente (explicativa) 
Regresión Lineal simple
El modelo de regresión lineal está dado por la 
siguiente expresión:
    y=������+������X +������
������= intercepto (valor que toma Y cuando X 
vale 0)
������= es la pendiente (indica cómo cambia Y al 
incrementar X en una unidad)
������= representa el error aleatorio con una 
distribución normal (0,������)
Regresión Lineal simple
La estimación de ������ y ������ se hace por medio del 
metodo de minimos cuadrados, donde se 
busca minimizar la suma de cuadrados de los 
errores dada por:
Regresión Lineal simple
Interpretación del coeficiente ������
Tenemos tres casos posibles:
1. ������ =0 para cualquier valor de X la variable 
Y es constante (no cambia)
2. ������ >0 indica que al aumentar el valor de 
X, también aumenta el valor de Y
3. ������<0 indica que al aumentar el valor de X, 
el valor de Y disminuye
Regresión Lineal simple
Coeficiente de correlación R: Es una 
medida que trata de medir la dependencia 
lineal que existe entre dos variables. Y su 
cuadrado se determina coeficiente de 
determinación R^2
El coeficiente de determinación cuantifica el 
porcentaje de variabilidad que puede explicar 
X de Y, por ejemplo si R^2= 0.45, indica que 
45% de la variabilidad de Y es explicada por X
Propiedades del coeficiente de 
Regresión Lineal simple
correlación
1. No tiene dimensión y siempre está entre 
[-1,1]
2. Si las variables son independientes 
entonces R=0, pero lo inverso no 
siempre es cierto
3. Si existe relación lineal perfecta R=1 o -1 
(relación inversa perfecta)
4. Si R>0 indica una relación directa lineal 
de X en Y
5. Si R<0 indica una relacion inversa de X 
en Y
Regresión Lineal simple
Supuestos
1. Independencia: los residuales deben 
ser independientes entre sí 
2. Homocedasticidad: significa varianzas 
iguales, para cada valor de X la varianza 
de los residuales debe ser la misma 
3. Normalidad: para cada valor de X, los 
residuales tienen distribución normal con 
media cero 
Regresión Lineal 
Múltiple
Regresión Lineal simple
Similar al modelo estadístico de 
Regresión lineal simple donde trata 
de explicar la relación que existe entre 
una variable dependiente 
(variable respuesta) y unas 
variables independientes 
 (explicativas) 
Regresión Lineal simple
El modelo de regresión lineal múltiple está 
dado por la siguiente expresión:
 y=������+������1X1+... +������nXn+������
������= intercepto (valor que toma Y cuando X 
vale 0)
������i= es la pendiente de cada variable 
independiente (i= 1,2,.....,n)
������= representa el error aleatorio con una 
distribución normal (0,������)
Regresión Lineal simple
Este modelo al igual que el de regresión lineal 
simple tiene los mismos supuestos y se 
puede cuantificar su desempeño de la misma 
forma (utilizando el coeficiente de 
determinación (R^2)
De igual forma el la pendiente de cada 
variable independiente puede ser o no 
significativa y se necesita verificar 
individualmente. 
Optimización de 
hiperparametros
Definición
Hiperparametros
Los hiperparametros son variables que 
rigen el proceso de entrenamiento (e.g en 
una red neuronal las capas ocultas serían un 
ejemplo). Estas variables no están 
directamente relacionadas con los datos 
de entrenamiento, sino que son de 
configuración y por ende son constantes 
durante cualquier entrenamiento
Hiperparametros
Además de los hiperparametros tenemos dos 
otros conceptos importantes que controlan el 
entrenamiento de cualquier modelo:
1. Datos de entrada: colección de 
instancias con las características relevantes 
para el problema de interés. Se usan en el 
entrenamiento para configurar el modelo con 
el fin de que pueda realizar predicciones 
sobre nuevas instancias 
Hiperparametros
Además de los hiperparametros tenemos dos 
otros conceptos importantes que controlan el 
entrenamiento de cualquier modelo:
2. Parámetros: son las variables que usan 
los modelos para ajustarse a los datos (e.g los 
nodos en una red neuronal y sus pesos). Los 
parámetros son formalmente el modelo ya 
que son los que dan las características 
Hypertuning de 
parámetros
Hypertuning de Parámetros
Dentro de este contexto, resulta 
importante entender que la optimización 
de hiper-parámetros, se realiza 
normalmente mediante la utilización de un 
proceso de búsqueda cuyo objetivo 
consiste en encontrar la mejor selección 
de valores para un conjunto finito de hiper-
parámetros con el objetivo de generar el 
 mejor modelo posible.
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Ejemplo en vivo
Miraremos ejemplos de aplicación dentro 
de la carpeta de clase para los algoritmos: 
SVM, Regresión Lineal + múltiple. Además 
exploraremos cómo podemos hacer el 
hypertuning de parámetros para un modelo 
de Regresión.
Elaborando un algoritmo de 
     regresión
Utilizaremos lo aprendido en clase para crear un 
modelo de regresión y validar variables relevantes
     Duración: 15-20 mins
ACTIVIDAD EN CLASE
Elaborando un 
algoritmo de 
En esta oportunidad nos reuniremos en grupos de máximo 
regresión
4 personas.
1. Elegir 4 variables independientes que consideren 
útiles para predecir los “costos” de nuevos clientes
2. Realizar el Encoding de las variables independientes 
(una persona hace el código y comparte, los demás 
ayudan dando instrucciones, etc.) para generar 
matriz para el modelo
3. Elegir uno de los modelos aprendidos en clase y 
entrenarlo
4. Generar una predicción de costos sobre uno de los 
estudiantes del grupo 
                                1
                                0
             Entrenando un 
     algoritmo de Machine 
                    Learning
     Deberás entregar el décimo avance de tu proyecto final. Continuaremos hablando 
     sobre lo trabajado en la segunda pre entrega del proyecto final. Crearás un 
     notebook donde trabajarás sobre los datos elegidos en la primera y segunda pre 
     entrega del proyecto final. Posteriormente, realizarás las etapas de: i) Encoding, ii) 
     Ingeniería  de  atributos  y  iii)  Entrenamiento  de  un  modelo  de  Machine  Learning 
     Supervisado  (Clasificación  o  Regresión)  o  no  supervisado  dependiendo  de  la 
     pregunta problema.
    DESAFÍO 
    ENTREGABLE
Entrenando un algoritmo de Machine 
Learning
Consigna                                       Formato
 ✓ Utilizar una fuente de datos para           ✓ Se debe entregar un Jupyter notebook con 
    resolver problemas de clasificación o          el nombre: 
    regresión.                                     “Desafio_AlgoritmoML_MVP_+Nombre
 ✓ Realizar los procesos de Encoding,              _ +Apellido.ipynb”.
    Feature Engineering y entrenamiento       Sugerencias
    de un modelo de Machine Learning           ✓ Se pueden utilizar fuentes de datos 
    (Clasificación o Regresión)                    conocidas en sitios como Kaggle o UCI
Aspectos a incluir                              ✓ Se recomienda elegir datasets curados 
 ✓ Notebook donde se detallen todos los            para que la mayor parte del tiempo se 
    pasos seguidos                                 utilice para el entrenamiento de modelos 
                                                   y no en limpieza de datos
Ejemplo                                        Explicación del desafío
 ✓ Ejemplo Desafío Entrenamiento ML,           ✓ ¡Click aquí!
¿Quieres saber más?
Te dejamos material 
ampliado de la clase
 MATERIAL AMPLIADO
Recursos multimedia
Algoritmos de regresión
✓ Algoritmos de regresión | Scikit-Learn | Enlace
Disponible en nuestro repositorio.
¿Preguntas?
CLASE N°42
Glosario
SVM: algoritmo de aprendizaje               Parámetros: son las variables que usan 
supervisado que permite resolver            los modelos para ajustarse a los datos. Se 
problemas de clasificación o regresión      definen como la estructura interna del 
haciendo uso de hiperplanos generando       modelos ya que son las características del 
regiones de separación con un amplio        mismo
margen
                                           Hiperparametros: variables que rigen el 
Regresión lineal: técnica estadística       proceso de entrenamiento, no están 
que permite encontrar la asociación lineal  directamente relacionadas con los datos de 
entre una variable dependiente (Y) y        entrenamiento sino que son de 
una/varias variable(s) independiente(s)     configuración y por ende son constantes 
llamadas X’s, puede ser simple (1 variable  durante cualquier entrenamiento 
independiente) o múltiple (más de 1 
variable independiente)
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Clasificación con SVM
      ✓ Ejemplos de clasificación errónea
      ✓ Regresión Lineal simple y Múltiple
      ✓ Optimización de hiper parámetros 
Opina y valora 
esta clase
Esta clase va a ser
grabad
  a
      Clase 33. DATA SCIENCE
   Visualizaciones 
   efectivas y Data 
     Storytelling
   Temario
                                  32                                         33                                         34
                      Introducción a la                         Visualizaciones                                 GIS y datos 
                       visualización de                        efectivas y Data                                  espaciales
                                datos                              Storytelling
                       ✓    Historia de las                      ✓ Beneficios del                            ✓ GIS y datos 
                            visualizaciones                           Storytelling y                              espaciales
                       ✓    Principios generales del                  componentes
                            diseño analítico                     ✓ Visualizaciones de                        ✓ Mapas y 
                                                                      datos                                       animaciones de 
                       ✓    Buenas prácticas                                                                      Python
                       ✓    Gestalt                              ✓ Elevator Pitch
                                                                                                             ✓ Uso de la librería 
                       ✓    Visualizaciones                      ✓ Interfaz de usuario                            Plotly
                            engañosas
                                                                 ✓ Principios de 
                                                                      usabilidad
Objetivos de la clase
         Profundizar en la gramática de los gráficos
         Reconocer la utilidad y beneficios del 
         Storytelling
         Implementar los principios de usuario
 MAPA DE CONCEPTOS
                          Beneficios del 
                          storytelling y 
                          componentes
                      Visualizaciones 
Visualización de      efectivas y Data            Principios de 
    datos                                          usabilidad
                        Storytelling
                          Elevator Pitch e 
                           Interfaz de 
                            usuario
Beneficios del StoryTelling 
y componentes
Storytelling
¿Qué es el Storytelling?
Desde el inicio de los tiempos, el ser humano 
ha usado las historias para comunicarse con 
los demás y poder expresar sus pensamientos e 
ideas. 
Por lo tanto el Storytelling, es el arte de 
contar una historia que genere impacto 
mediante palabras, imágenes o sonidos. 
Se busca crear una “atmósfera mágica” a 
través de nuestro relato.
¿Qué es el Storytelling?
El StoryTelling, es el proceso de traducir 
los  análisis  de  datos  a  términos 
simples, con el objetivo para influir en 
una  decisión  o  acción  en  particular,  la 
cual suele ser de tipo comercial. 
Para pensar
¿Cuáles te parecen que son los beneficios del 
Storytelling en relación a Data Science?
Contesta en el Chat de ZOOM
Contexto 
Storytelling
Storytelling en la era 
digital
El storytelling, se ha arraigado sin dudas en 
diferentes  disciplinas.  La  aparición  de 
Internet,  la  revolución  de  las  redes 
sociales  y  la  evolución  de  las  TIC,  han 
posibilitado a los individuos el compartir sus 
historias  en  nuevos  formatos  (como  el 
podcast,  el  vídeo  o  el  blog)  y  recibir 
retroalimentación constante.
Storytelling en la era 
digital
 Cuando hablamos de Storytelling en la         Hoy el término se asocia con 
 era digital, estamos hablando de la           diferentes funciones como ser 
 aplicación de los métodos y                   por ejemplo: visualizaciones de 
 técnicas de storytelling más                  datos, infografías, tableros, 
 tradicionales pero ahora dentro               ciencia de datos, etc. 
 del mundo y contexto digital.  
¿Por qué hacemos 
visualizaciones?
✔ Comunicar patrones interesantes.
✔ Alentar el descubrimiento y exploración de un conjunto de datos.
✔ Detectar patrones en nuestros datos.
✔ Entender nuestros modelos.
Para pensar
¿Qué usos de Storytelling conoces?
Contesta en el Chat de ZOOM
Beneficios del 
Storytelling
Beneficios del StoryTelling
      Conexión emocional con la                    Recordabilidad
              audiencia                  Gracias a un relato con impacto, es muy 
       Sin dudas contar una historia   probable que podemos quedar en la mente 
     permite empatizar con nuestro       de nuestra audiencia, aún luego de que 
   público objetivo y generar un mayor    quizás haya pasado un largo tiempo.
            engagement. 
Beneficios del StoryTelling
                  Confianza                                 Engagement
           Nuestra historia humaniza            Efectivamente, al empatizar generamos 
         nuestro relato, y eso termina          un vínculo con nuestra audiencia que 
        impactando en la confianza de           puede tornarse en una relación a largo 
           nuestro público objetivo.                            plazo. 
Componentes del 
Storytelling
Componentes del 
StoryTelling
Componentes del 
StoryTelling
A partir de los datos que consideremos 
relevantes, esta técnica permite 
convertirlos en una historia que nos 
conecte con nuestro público mediante 
formatos visuales potentes. Resulta 
importante mencionar, que para 
generar un impacto, hay que 
trabajar estas 3 de forma conjunta.
Visualización de 
datos
            Repaso…
Se refiere a la presentación de datos en un 
formato ilustrado o gráfico. Permite observar el 
análisis de datos, de manera visual, para captar 
conceptos difíciles o identificar patrones.
Gracias a la visualización de datos, las personas 
encargadas de tomar decisiones por ejemplo 
dentro de una empresa, ven significativamente 
facilitada su labor.
Elementos de una buena 
visualización
Se necesita de al menos los 
siguientes componentes:
✔ Información
✔ Historia
✔ Objetivo
✔ Forma visual
PARA RECORDAR
Retomando lo visto anteriormente , el 
paso más importante para crear una 
visualización de impacto es saber y 
definir qué queremos decir. También, es 
imprescindible que nuestra visualización 
tenga un propósito claro y establecido 
que nos permita conectar con la 
audiencia desde una perspectiva humana
Preguntas básicas del 
StoryTelling
Traductor de datos
Muchas empresas comienzan a incorporar 
este rol para hacer entendible los 
descubrimientos basados en datos por 
medio del Storytelling. 
El traductor tiene que preguntarse cómo 
crear una historia interesante para que la 
audiencia la entienda.
Para eso puede utilizar por ejemplo la regla de la triple W (Why, Who, What), 
que abordaremos más adelante.
Estructura del relato
Elementos de un relato
  Inicio o         Nudo         Desenlace
planteamiento
Elementos de un relato
Aquí conocemos a los personajes, nos situamos en el espacio/tiempo, la idea principal 
detrás del planteamiento es lograr que “todo el mundo hable el mismo idioma”.
Elementos de un relato
Es cuando el protagonista del relato experimenta el conflicto, es decir, cuando se 
rompe el estado de equilibrio del planteamiento y por el que el protagonista deberá 
 empezar a usar sus recursos para poder superar los problemas.
Elementos de un relato
El protagonista ya no será el mismo después de haber sufrido el nudo explicado en la 
    trama. Se recupera el equilibrio inicial que se había truncado.
Construcción
Elementos de un relato
      WHY                             WHO                            WHA
                                                                       T
  Debemos empezar            Tenemos que pensar a quién    Definimos qué vamos a 
 definiendo cuál es el           va dirigida nuestra        presentar para que se 
objetivo y el propósito        presentación. Por ejemplo:     adapte a los criterios 
de nuestra presentación         nivel estratégico, táctico u   anteriores. Una buena 
por ejemplo:  persuadir,              operativo.            opción es una lluvia de 
 motivar, explicar, etc.                                            ideas.
Recomendaciones 
para contar buena historia
Claridad y concisión: 
Buscamos resumir el volumen de datos ofreciendo 
una lectura simplificada, mediante un lenguaje 
conciso y claro. Además se requiere ser conciso a 
la hora de transmitir las ideas.
Recomendaciones 
para contar buena historia
Insights relevantes. 
Identificar la información importante 
del data storytelling es fundamental, igual 
que determinar el objetivo que 
perseguimos: queremos inspirar a nuestro 
público, divertirlo, informar sobre un 
producto por ejemplo.
Recomendaciones 
para contar buena historia
Texto e imágenes. 
Si pensamos en nuestro buyer 
persona podremos elegir imágenes de 
alta carga informativa que les resulten 
útiles y, a la vez, den soporte a la 
información de manera atractiva.
Recomendaciones 
para contar buena historia
Ponernos en la piel del oyente. 
Por más que parezca una obviedad muchas 
veces nos olvidamos de estos aspectos. 
Necesitamos empatizar y generar 
engagement. Podemos hacer uso también 
de técnicas como por ejemplo el Design 
Thinking Pensamiento de Diseño.
Errores típicos al contar 
una historia DS
✔ No usar señales visuales en la narración de 
datos.
✔ Contar la historia sin contexto 
✔ Transmitir  falta  de  confianza  (lo  cual  se 
evidencia en visualizaciones poco elaboradas 
y resultados sin respaldo) 
✔ No conocer bien la audiencia
✔ Hablar sin un propósito claro
Errores típicos al contar 
una historia DS
✔ Las    ideas   presentadas  no    estén 
   interconectadas
✔ Recordar  que  los  algoritmos  no  toman 
   decisiones
✔ No se prioriza el storytelling en la forma de 
   presentar los resultados
✔ Se plasman varias historias que hacen que la 
   principal pierda su peso
✔ No llevar los datos y resultados obtenidos al 
   plano del mundo real
PARA RECORDAR
Preguntas que jamás debemos 
olvidarnos
●¿Responde a los objetivos iniciales de análisis?
●¿Hemos seleccionado los tipos de gráficos más 
adecuados en base al análisis que queremos 
realizar?
●¿Puedo comprender la visualización rápida sin 
necesidad de información adicional?
Errores típicos en el Diseño
✔ Abuso de colores en los gráficos.
✔ No seleccionar el método de codificación 
adecuado para la visualización.
✔ Demasiada información.
✔ Muy poca o nula información.
✔ Uso de visualizaciones poco explicativas que 
al final no terminan explicando demasiado
Elevator Pitch
Elevator Pitch para
Storytelling
Sabemos que contar una historia frente a gente desconocida o colegas de trabajo, 
muchas veces suele ser difícil. Por eso, podríamos apoyarnos de la técnica del Elevator 
Pitch, la cual en definitiva es un tipo de discurso que tiene como objetivo vender bien 
algo: una idea, nuestra empresa o incluso a nosotros mismos. 
También se puede definir como una técnica a la hora de hacer la presentación breve de un 
proyecto para que resulte convincente.
Elevator Pitch para
Storytelling
Su nombre es un juego de palabras que significa: 
“el discurso del ascensor” y simboliza muy bien 
su  principal  característica:  una  conversación 
breve  como  las  que  tienen  lugar  en  los 
ascensores. 
Por lo tanto, tenemos prácticamente muy poco tiempo para llamar la atención de nuestro 
público objetivo y lograr que se interese en nuestra historia.
Elevator Pitch para
Storytelling
Para crear un elevator pitch efectivo, tenemos que asegurarnos que nuestro mensaje 
responda las siguientes preguntas:
✔ ¿Quién eres y a qué te dedicas? Estas cuestiones, de preferencia, 
debemos responderlas en los primeros 20 segundos. 
✔ ¿Qué necesidad resuelves y qué soluciones ofreces? Esto tiene que 
quedar claro del segundo 20 al 40. 
✔ ¿Por qué eres la persona indicada para el trabajo? Tu cierre no debe 
tardar más de 10 segundos para lograr un elevator pitch menor a 60 
segundos.
Elevator Pitch para
Storytelling
Tenemos que definir claramente:
                    ✔ ¿Qué queremos lograr?.
                    ✔ ¿Qué  ofrecemos  o  cuál  es  nuestra 
                      propuesta de valor?.
                    ✔ Ser breves y concisos.
                    ✔ Ordenar nuestro mensaje.
                    ✔ Ensayar  continuamente  para  lograr 
                      una mayor soltura.
         ☕
       Break
     ¡10 minutos y 
     volvemos!
Interfaz de usuario
Principios de usabilidad
Podemos identificar este elemento en el marco de UX, resulta importante mencionar, que 
UX pertenece al mundo del Customer Experience o también conocido como CX.
CX= Customer 
Experience
UX= User experience
UI= User Interface
Experiencia del cliente (CX)
Se define como la suma de todas las 
interacciones que tiene un cliente con 
una empresa. Ponen en el centro del foco 
la figura del consumidor en todo el 
recorrido de un producto o servicio. La 
manera de relacionarse con el consumidor, 
está compuesta por múltiples canales: 
físico, telefónico, virtual, etc. 
Experiencia de usuario 
(UX)
¡Ahora sí! Hablemos de UX. 
Se trata de todo lo que una persona percibe/ 
siente/ experimenta al interactuar con una 
empresa, servicio o producto.
Interfaz de usuario (UI)
Otro concepto que no podemos no mencionar es el de 
UI. Si bien este concepto quizás inicialmente aplique 
más al mundo del diseño y la programación, también 
resulta interesante poder mencionarlo de manera 
breve.  Ya que es la traducción visual de todos los 
elementos de la app con los que el usuario interactúa 
directamente. 
Interfaz de usuario (UI)
En otras palabras, es un compendio de 
todo lo que el usuario recibe como 
feedback visual. Incluye por tanto el 
diseño de los botones y campos de 
formularios, el uso de la paleta de color y 
aspectos vinculados con las percepciones 
auditivas.
Diseño                  Diseño 
  UX                      UI
 Agregar al carrito                   Agregar al 
                              carrito
UX decidir cuál va ser    UI decide como se verá 
  el Call To Action       este botón para que sea 
                         accesible, ¿qué color, que 
                            iconografía, que 
                             tipografía?
          UX                                                                                                                                UI
                     IInvnveesstitiggaaccii             Entrevista
                                                                                                                                                                                                                                        Colores, 
                             óónn  //                          s y                          Pruebas                                                                                                                                      Colores, 
                                                                                                                                                                    Diseño                         Diseño de                         Tipografía, 
                     EEsstataddíísstiticcaa             Definición                                de                                                                 Diseño                         Diseño de                         Tipografía, 
                                                                                                                                                                     Visual                          Interfaz                         Ilustracion
                               ss                               de                         Usuarios                                                                   Visual                          Interfaz                         Ilustracion
                                                                                                                                                                                                                                      es, Iconos.
                                                         Personas                                                                                                                                                                      es, Iconos.
                                                                                                                                                                                                   Definición 
                      Redacción                         Redacción                         Diseño de                                                                                                 Definición 
                                                                                                                                                                      Estilo                               de 
                       de Textos                         de textos                        interacció                                                                   Estilo                               de 
                                                                                                                                                                     Visual                        Estructura
                                                                                                   n                                                                  Visual                        Estructura
                                                                                                                                                                                                            s
                                                                                                                                                                                                             s
                      Arquitectu
                           ra de                        Usabilidad                       Prototipad
                      Informació                                                                   o
                               n
Principios de 
usabilidad
Leyes de Nielsen
Jakob Nielsen, quien es considerado el padre de la Usabilidad, 
planteó 10 Principios de Usabilidad para el correcto Diseño de 
una Interfaz de Usuario, los mismos son:
1. Visibilidad del estado del sistema: La web o aplicación 
debe mostrar en todo momento al usuario qué está pasando y en 
qué punto de la navegación se encuentra.
2. Adecuación entre el sistema y el mundo real: El sistema 
debe hablar con el mismo lenguaje que los usuarios.
Leyes de Nielsen
3.  Libertad  y  control  por  el  usuario:  Los  usuarios  deben 
poder volver fácilmente a un estado anterior. Es conveniente dar 
las opciones de "deshacer" y "rehacer". 
4.  Consistencia  y  estándares:  Se  debe  seguir  y  repetir 
algunos patrones para no confundir a los usuarios.
5. Prevención de errores: Es mejor prevenir los errores que 
generar mensajes una vez se produzcan.
Leyes de Nielsen
6.  Reconocer  mejor  que  recordar:  Hay  que  intentar  en  la 
medida de lo posible mostrar objetos, acciones y opciones para 
minimizar el uso de memoria del usuario.
7. Flexibilidad y eficiencia de uso: Es importante personalizar 
las acciones frecuentes. 
8. Estética y diseño minimalista: Intentar simplificar, eliminar 
el  contenido  irrelevante  para  que  el  usuario  sólo  se  fije  su 
atención en lo realmente  es importante.
Leyes de Nielsen
9.  Ayudar  a  los  usuarios  a  reconocer,  diagnosticar  y 
solucionar los errores: Los mensajes de error deben expresar 
claramente cuál ha sido la causa del problema.
10.  Ayuda  y  documentación:  En  algunos  casos  puede  ser 
necesario que el usuario necesite ayuda, por lo que debe estar 
siempre disponible una sección de Ayuda. 
                      8
   Desafío entregable: 
      Data Storytelling
  Deberás  entregar  el  octavo  avance  de  tu  proyecto  final.  Continuaremos 
  hablando sobre lo trabajado en el desafío “Data Wrangling”. Crearás un 
  notebook donde se desarrolle una narrativa que permita dar respuesta a las 
  preguntas/hipótesis formuladas para el proyecto final.
       Recordemos…
                 Extrajimos datos de interés
                 Comenzamos el proceso de 
                 limpieza y estructuración 
                 Desarrollamos algunas 
                 hipótesis 
  Clase 29
Desafío entregable
Data Wrangling     Data Wrangling/Munging
    DESAFÍO 
    ENTREGABLE
Data Storytelling
Consigna                                     Formato
✓ Iniciar el proceso de Data                 ✓ Se espera un notebook en 
   Storytelling respondiendo las                formato .ipynb. Dicho notebook debe 
   preguntas que se quieran responder           tener el siguiente nombre: 
                                                “Data_StoryTelling+Apellido.ipynb”
                                                .
Aspectos a incluir                           Sugerencias
✓ Notebook con código y estructura           ✓ Utilizar las herramientas vistas en el 
   eficiente                                    curso
Ejemplo                                        ✓ Manejo de duplicados nulos y análisis 
✓ Data StoryTelling                             exploratorio
                                             ✓ Comenzar por preguntas de alto nivel y 
                                                luego más específicas
                                           Explicación del desafío
                                             ✓ ¡Click aquí!
CLASE N°33
Glosario
Storytelling: es el arte de contar una            Preguntas básicas de Storytelling: 
                                                 ¿Para qué creo esta historia? ¿Qué quiero 
historia que genere impacto                       transmitir? ¿Cómo representar los datos? 
mediante palabras, imágenes o                     ¿Cómo la va a recibir la audiencia?
sonidos. Se busca crear una “atmósfera 
mágica” a través de nuestro relato.               Elementos de un relato: se necesita de 
Componentes de Storytelling:                      un inicio, nudo y desenlace. Además 
narrativa, visualización y datos. Narrativa       debemos tener presente el ¿Por qué? 
+ Data (Explicación), Visualización + Data        ¿Quién? y ¿Qué queremos transmitir?.
(Informar), Narrativa + Visualización 
(Atracción)                                       Elevator Pitch: forma de transmitir una 
                                                 idea en corto y tiempo de y forma 
Elementos de una buena                            apropiada
visualización:. Información, historia, 
objetivo y forma visual
¿Preguntas?
Opina y valora 
esta clase
Muchas 
gracias.
           Resumen 
       de la clase hoy
      ✓ Beneficios de Storytelling y componentes
      ✓ Visualización de datos
      ✓ Elevator Pitch
      ✓ Interfaz de usuario
      ✓ Principios de usabilidad`]

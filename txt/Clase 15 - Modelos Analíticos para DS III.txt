    Esta clase va a ser
        grabad
          a
              Clase 15. DATA SCIENCE
         Modelos Anal√≠ticos 
              para DS III
      Temario
                       14                      15                     16
                    Modelos                Modelos               Estudios de 
                anal√≠ticos para         anal√≠ticos para           casos de  
                      DS II                  DS III                modelos 
                                                                 anal√≠ticos I
                  ‚úì Recapitulaci√≥n                               ‚úì Casos de 
                                        ‚úì Modelo anal√≠tico          √©xitos con 
                  ‚úì Aprendizaje                                     ciencias de 
                     supervisado        ‚úì Reglas de 
                                            asociaci√≥n              datos
                  ‚úì Clasificaci√≥n
                                        ‚úì Reducci√≥n de           ‚úì Armado de 
                  ‚úì KNN                                             presentaci√≥n 
                                            dimensionalidad         ejecutiva
                  ‚úì Regresi√≥n
    Objetivos de la clase
                Profundizar en el Tipo de Aprendizaje No 
                Supervisado.
                Entender los algoritmos de Clustering y 
                Reglas de Asociaci√≥n.
                Conocer la Reducci√≥n de la Dimensionalidad y 
                PCA.
        MAPA DE CONCEPTOS
                                    Caracter√≠stica
                                       s
      Modelos      Aprendizaje 
    Anal√≠ticos para   No            Diferencias       Clustering
     Ciencia de    Supervisado
      Datos III                                       Reglas de 
                                                      Asociaci√≥n
                                    Algoritmos      Reducci√≥n de la 
                                                     Dimensionalida
                                                         d
                                                        PCA
       Repaso clase anterior
         En la clase de Modelos Anal√≠ticos para DS II,         En esta sesi√≥n, nos centraremos 
         hemos estudiado el Tipo de Aprendizaje                exclusivamente en el Aprendizaje de Tipo 
         Supervisado tanto para problemas de                   No Supervisado, caracter√≠sticas, 
         Clasificaci√≥n como as√≠ tambi√©n de                     aplicaciones y particularidades. 
         Regresi√≥n.                                            ¬°Empecemos!ÌøÅÌ†º
     Modelo anal√≠tico
    Aprendizaje no supervisado
     ‚úì Es una subcategor√≠a del aprendizaje 
       autom√°tico y la IA. 
     ‚úì Se define por el uso de conjuntos de datos 
       no etiquetados para entrenar algoritmos 
       que encuentren patrones ocultos. 
     ‚úì El aprendizaje no supervisado ayuda a las 
       organizaciones a resolver una variedad de 
       problemas del mundo real a gran escala, 
       por ejemplo en sistemas de recomendaci√≥n 
       como Amazon con base en Clustering.
         Diferencias
         ‚úì Sus  m√©todos  no  se  pueden  aplicar       ‚úì Permiten      realizar   tareas    de 
            directamente  a  un  problema  de              procesamiento  m√°s  complejas. 
            regresi√≥n  o  clasificaci√≥n,  porque  no       Puede  ser  m√°s  impredecible  en 
            tenemos idea de cu√°les deber√≠an ser            comparaci√≥n  con  otros  m√©todos  de 
            los valores de los datos de salida. ÌøÅÌ†º         aprendizaje naturales.
            Recordemos  que  la  variable  es          ‚úì Se utilizan para agrupar los datos 
            desconocida.                                   no   estructurados     seg√∫n  sus 
         ‚úì Puede  utilizarse  para  descubrir  la          similitudes y patrones distintos en 
            estructura  subyacente  de  los                el conjunto de datos. 
            datos.
                En 
       resumen‚Ä¶ Ì≥ùÌ†Ω 
      ‚úì No hay una variable objetivo (variable de salida). 
      ‚úì No hay variables que ayudan a predecir a la variable 
        de salida. 
      ‚úì Todas las variables tienen la misma importancia. 
      ‚úì Se busca la interdependencia de las variables.
     ¬øC√≥mo funciona?
       ¬øC√≥mo funciona? 
      ‚úì Funcionan con datos no etiquetados. 
          Su   prop√≥sito   es   naturalmente    la 
          exploraci√≥n. 
      ‚úì Si  el  Aprendizaje  Supervisado  funciona 
          bajo  reglas  claramente  definidas,  el 
          Aprendizaje       no      Supervisado 
          funciona bajo condiciones en las que 
          los  resultados  son  desconocidos  y 
          por lo tanto, es necesario definirlos en el 
          proceso.
       Est√°n acostumbrados a
         ‚úì Explorar la estructura de la informaci√≥n y     En   otras   palabras,  describe    la 
             detectar patrones distintos.                 informaci√≥n  o  nuestro  dataset 
                                                          identificando  las  relaciones  entre 
         ‚úì Extraer ideas valiosas.                        los features
         ‚úì Aumentar la eficacia del proceso de toma 
             de  decisiones  en  base  a  los  patrones 
             detectados.
                                       APRENDIZAJE                        APRENDIZAJE NO 
                                      SUPERVISADO                           SUPERVISADO
    Entrenamiento             Utiliza datos etiquetados                  Utiliza datos no etiquetados
    Retroalimentaci           Retroalimentaci√≥n directa               No tiene retroalimentaci√≥n directa
           √≥n
         Datos           Se proporcionan de entrada y salida           Se proporcionan s√≥lo de entrada
        Objetivo             Predecir con nuevos datos             Encontrar patrones ocultos de los datos
      Supervisi√≥n             Necesita ser supervisado                     No necesita supervisi√≥n
     Clasificaci√≥n     Problemas de Clasificaci√≥n y Regresi√≥n      Problemas de Clustering y Asociaciones
       Usabilidad Cuando conocemos la entrada y las salidas        Cuando solo tenemos datos de entrada
       Resultado                  Resultado preciso                             Menos preciso
     Relaci√≥n con         No est√° cerca de la verdadera IA          Est√° m√°s cerca de la verdadera IA, ya 
           AI                                                      que aprende de manera similar a como 
                                                                                   un ni√±o
       Algoritmos
     Algoritmos
     Los problemas de aprendizaje no 
     supervisado se clasifican principalmente 
     en dos categor√≠as: 
      ‚úì Cluster (donde tenemos algoritmos 
        como k-means. clustering jer√°rquico, 
        modelos de mixturas gaussianas, o 
        algoritmo basados en densidad como 
        DBSCAN) 
      ‚úì Reducci√≥n de dimensionalidad (como 
        PCA, ICA, An√°lisis Factorial)
         ¬øQu√© tipo de problemas 
         resuelve?
        Problemas de Clustering:                      Problemas de Reducci√≥n 
        Asignaci√≥n de individuos/objetos a            Dimensionalidad: 
        grupos homog√©neos asegurando m√≠nima           Cuyo prop√≥sito es reducir el n√∫mero de 
        varianza intra-cluster y m√°xima varianza      features (variables) por medio de 
        inter-cluster intentando descubrir la         feature selection (selecci√≥n existente) o 
        estructura oculta de los objetos              feature extraction (combinaci√≥n de 
                                                      datos originales).
     Clustering vs. Reducci√≥n 
     de dimensionalidad
     Usamos clustering cuando 
     queremos agrupar observaciones 
     con base en ciertos features 
     (reducci√≥n de n√∫mero de 
     observaciones a K grupos), 
     mientras que reducci√≥n de 
     dimensionalidad se usa cuando 
     se tienen muchas dimensiones y 
     queremos comprender las 
     relaciones existentes en menos 
     dimensiones.
    M√∫ltiples algoritmos del 
    Aprendizaje No 
    Supervisado.
     Algunos de los m√°s populares son:
     ‚úì Clustering o Agrupamiento
     ‚úì Reglas de Asociaci√≥n
     ‚úì Algoritmos de Reducci√≥n de la 
       Dimensionalidad
    Clustering
    Tambi√©n conocidas como agrupamiento o segmentaci√≥n tienen como principal funci√≥n 
    encontrar una estructura o un patr√≥n en una colecci√≥n de datos no clasificados. 
    ÌøÅÌ†º
    Es decir, Intentan encontrar grupos en los datos que compartan atributos en 
    com√∫n Ì≥ùÌ†Ω 
      T√©cnicas para 
      codificaci√≥n de 
       categor√≠as
    One Hot Encoding
    Toman variables num√©ricas para medir la 
    distancia. Sin embargo, existe la manera 
    de trabajar con variables categ√≥ricas 
    haci√©ndolas variables dummy, a trav√©s 
    del uso de la t√©cnica de transformaci√≥n de 
    datos One Hot Encoding (OHE).
    Label Encoder
    Sin embargo One Hot Encoding es la √∫nica 
    t√©cnica para transformar variables 
    categ√≥ricas, existe una alternativa para 
    reducir el problema de 
    multidimensionalidad cuando tenemos 
    muchas categor√≠as en una variable a 
    trav√©s del uso de la t√©cnica de 
    transformaci√≥n de datos Label Encoder 
    (LE).
     One Hot Encoding vs Label 
       1) La variable categ√≥rica no es ordinal               1) La variable categ√≥rica es ordinal 
     Encoder
           (como los pa√≠ses anteriores)                         (como Jr. kg, Sr. kg)
       2) La  cantidad  de  categor√≠as  es                   2) El n√∫mero de categor√≠as es bastante 
           peque√±a  para  evitar  problemas  de                 grande ya que la codificaci√≥n one-hot 
           multicolinealidad y overfit.                         puede llevar a un alto consumo de 
                                                                memoria.
           Manzan     Poll  Br√≥co   Calor√≠a                      Nombre       Categor√≠   Calor√≠a
           a          o     li      s                            de la        a          s
                                                                 comida
           1          0     0       95
                                                                 Manzana      1          95
           0          1     0       231
                                                                 Pollo        2          231
           0          0     1       50
                                                                 Br√≥coli      3          50
    Tipos de algoritmos 
       Clustering
     Clustering
     Particiones      Hierarchical           Density             Grid              Model
       K-means          Aglomerativo/        DBSCAN            Wavecluster     GMM (Gaussian 
                          Divisivo                                              Mixture Model)
     PAM (Partition   BIRCH (Balanced        OPTICS              STING 
   around medoids)    Iterative Reduced)                                          COBWEB
        CLARA           ROCK (Robust         DBCLASD            CLIQUE            CLASSIT
   (Clustering Large     Clustering 
     Applications)       Algorithm)
     FCM (Fuzzy C-    CURE (Clustering       DENCLUE            OptiGrid           SOMs
       Means)              using 
                       representatives)
     Fuente: Adaptado de Mehta V et al. (2020).
              Algoritmos
                      Algoritmos                    Tiempo                 Accuracy              Manejo 
                                                                                                outliers
                  Affinity Propagation               Medio                   Medio                  si
               Agglomerative Clustering               Alto                    Alto                  Si
                         BIRCH                       Medio                    Alto                  Si
                        DBSCAN                        Alto                    Bajo                  No
                        K-means                      Medio                    Bajo                  No
                   Mini-Batch K-Means                 Alto                   Medio                  No
                       Mean Shift                    Medio                    Bajo                  Si
                         OPTICS                       Alto                   Medio                  No
                   Spectral Clustering               Medio                   Medio                  No
  Fuente: Adaptado de Rujasari P et al. (2010). En negrita, los algoritmos m√°s populares 
       Clustering 
       Jer√°rquico
    Clustering Jer√°rquico
                        En estos algoritmos se generan sucesiones 
                        ordenadas (jerarqu√≠as) de 
                        conglomerados. Puede ser agrupando 
                        cl√∫sters peque√±os en uno m√°s grande o 
                        dividiendo grandes clusters en otros m√°s 
                        peque√±os. 
                        La estructura jer√°rquica es representada en 
                        forma de un √°rbol llamado Dendograma.  
   Ì≥ùÌ†Ω Podemos encontrar 2 clasificaciones adicionales:
    1. Jer√°rquicos 
       aglomerativos (bottom-
     Inicialmente cada instancia es un cl√∫ster. 
     Las  estrategias  aglomerativas  parten  de 
       up)
     un  conjunto  de  elementos  individuales  y 
     van  ‚Äújuntando‚Äù  los  elementos  que 
     m√°s se parezcan hasta quedarse con 
     un  n√∫mero  de  clusters  que  se 
     considere √≥ptimo.
     2.    Jer√°rquicos 
     divisivos (top-down)
      Inicialmente todas las instancias est√°n en un 
      solo cl√∫ster y luego se van dividiendo, tal cual 
      su nombre lo indica. Las estrategias divisivas, 
      parten del conjunto de elementos completos y 
      se van separando en grupos diferentes 
      entre s√≠, hasta quedarse con un n√∫mero 
      de clusters que se considere √≥ptimo.
    Criterios importantes
    Debemos tomar en cuenta dos factores 
    importantes para la formaci√≥n de grupos:
     1. Medida de distancia 
     2. Criterio de enlace (usualmente criterio 
      de Ward)
     3. Existen otros criterios de enlace (Simple, 
      Completo,Promedio y Centroide)
                                from sklearn.cluster import AgglomerativeClustering
                                import scipy.cluster.hierarchy as sch
                                dataset = pd.read_csv('Mall_Customers.csv')
                                X = dataset.iloc[:, [3, 4]].values
     Ejemplo                    plt.figure(figsize=(10,6))
                                dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
                                model = AgglomerativeClustering(n_clusters=5, affinity='euclidean', 
                                linkage='ward')
                                model.fit(X)
                                labels = model.labels_
                                plt.scatter(X[labels==0, 0], X[labels==0, 1], s=50, marker='o', 
                                color='red')
                                plt.scatter(X[labels==1, 0], X[labels==1, 1], s=50, marker='o', 
                                color='blue')
                                plt.scatter(X[labels==2, 0], X[labels==2, 1], s=50, marker='o', 
                                color='green')
                                plt.scatter(X[labels==3, 0], X[labels==3, 1], s=50, marker='o', 
                                color='purple')
                                plt.scatter(X[labels==4, 0], X[labels==4, 1], s=50, marker='o', 
                                color='orange')
                                plt.show()
      Clustering No 
       Jer√°rquico
           Cluster No Jer√°rquico
            Volviendo  a  este  tipo  de  algoritmo,  la 
            cantidad  de  cl√∫steres  √≥ptima  se  define  de 
            antemano, y los registros  se  asignan  a  los 
            cl√∫steres   seg√∫n     su   cercan√≠a.    Existen 
            m√∫ltiples  algoritmos  de  Tipo  No  Jer√°rquico, 
            como  ser  por  ejemplo:  K  ‚Äì  Means  o 
            DBSCAN. 
     K-Means
                                 ¬øQu√© hace este m√©todo?
                                 Se necesita dar los centroides 
                                 iniciales para que el m√©todo 
                                 comience las iteraciones
    Tipos de distancia
    Hay diferentes tipos de distancias con las que 
    se puede medir la similitud/diferencia entre los 
    diferentes registros. 
    Cada distancia puede representar un tipo de 
    problema diferente y tambi√©n puede cambiar 
    sustancialmente el resultado de mi 
    clustering.
  Medidas de similitud y 
 disimilitud
  Medidas de similitud y 
 disimilitud
                                import numpy as np;import pandas as pd
                                from matplotlib import pyplot as plt
    Ejemplo                     from sklearn.datasets import make_blobs
                                from sklearn.cluster import KMeans
                                X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, 
                                random_state=0);wcss = []
                                for i in range(1, 11):
                                   kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, 
                                n_init=10, random_state=0)
                                   kmeans.fit(X)
                                   wcss.append(kmeans.inertia_)
                                plt.plot(range(1, 11), wcss)plt.title('Metodo del 
                                codo');plt.xlabel('Numero de clusters')
                                plt.ylabel('Inercia');plt.show()
                                kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, 
                                n_init=10, random_state=0)
                                pred_y = kmeans.fit_predict(X)
                                plt.scatter(X[:,0], X[:,1])
                                plt.scatter(kmeans.cluster_centers_[:, 0], 
                                kmeans.cluster_centers_[:, 1], s=300, c='red')
                                plt.show()
    Clustering basados 
      en Densidad
     Clustering basados en 
     Son m√©todos de aprendizaje no supervisado que identifican grupos/cl√∫steres, basados 
     en la idea de que un cl√∫ster es un espacio de datos es una regi√≥n contigua de alta 
     Densidad
     densidad de puntos, separada de otros cl√∫steres similares por regiones contiguas de 
     baja densidad de puntos
    Clustering basados en 
    Se tienen en cuenta dos par√°metros importantes:
     ‚úì eps:  Define  la  vecindad  alrededor  de  un 
      
    Densidad
       punto,  si  la  distancia  entre  dos  puntos  es 
       menor o igual a "eps", entonces se consideran 
       vecinos. Si el valor de eps se elige demasiado 
       peque√±o,  gran  parte  de  los  datos  se 
       considerar√°n valores at√≠picos.
     ‚úì MinPts: N√∫mero m√≠nimo de vecinos (puntos) 
       dentro  del  radio  eps.  MinPts  se  pueden 
       calcular del n√∫mero de dimensiones D como 
       MinPts >= D + 1. El valor m√≠nimo de MinPts 
       debe ser de al menos 3.
                               from sklearn.datasets import make_blobs
                               from sklearn.cluster import DBSCAN
                               # Configuracion de datos y parametros
                               num_samples_total = 1000;cluster_centers = [(3,3), (7,7)]
     Ejemplo                   num_classes = len(cluster_centers);epsilon = 1.0;min_samples = 13
                               # Generacion de datos
                               X, y = make_blobs(n_samples = num_samples_total, centers = 
                               cluster_centers, n_features = num_classes, center_box=(0, 1), 
                               cluster_std = 0.5)
                               # DBSCAN
                               db = DBSCAN(eps=epsilon, min_samples=min_samples).fit(X)
                               labels = db.labels_;no_clusters = len(np.unique(labels) )
                               no_noise = np.sum(np.array(labels) == -1, axis=0) # Ruido (Outliers)
                               print('#. clusters estimado: %d' % no_clusters)
                               print('# puntos ruidosos: %d' % no_noise)
                               # Generar figura de datos
                               colors = list(map(lambda x: '#3b4cc0' if x == 1 else '#b40426', 
                               labels))
                               plt.scatter(X[:,0], X[:,1], c=colors, marker="o", picker=True)
                               plt.title('Clasificacion DBSCAN');
                               plt.xlabel('Eje X[0]');plt.ylabel('Eje X[1]');plt.show()
      Actividad colaborativa
   Clustering aplicado a acciones
   Utilizaremos informaci√≥n de precios de acciones 
   para identificar cu√°les son las m√°s parecidas
   Discutiremos en el final de la actividad los 
   resultados obtenidos
   Duraci√≥n: 20 minutos
   Grupos de 3-4 personas
             ACTIVIDAD COLABORATIVA
        Acuerdos
       Presencia                                       Apertura al aprendizaje
        ‚úì Participar y ‚Äúestar‚Äù en la clase, que          ‚úì Siempre, pero siempre puedes 
            tu alrededor no te distraiga                    seguir aprendiendo. Compartir el 
                                                            conocimiento es v√°lido, la 
       Escucha activa                                       construcci√≥n colaborativa es la 
                                                            propuesta.
        ‚úì Escuchar m√°s all√° de lo que la 
            persona est√° expresando 
            directamente                               Todas las voces
                                                         ‚úì Escuchar a todos, todos podemos 
                                                            reflexionar. Dejar el espacio para 
                                                            que todos podamos participar.
            ACTIVIDAD COLABORATIVA
       Clustering aplicado a acciones
       Consigna: 
        1. Importar datos de Acciones Globales       3.    Aplicar la alternativa elegida y 
            (que est√°n hosteados en GITHUB en        establecer conclusiones
            el siguiente enlace Monitoreo de 
            Acciones
        2. Identificar qu√© algoritmo de 
            clustering aplicar√≠a en este caso 
            para identificar qu√© acciones ser√≠an 
            similares: No Jer√°rquico (K-means), 
            Jer√°rquico (Aglomerativo), Densidad 
            (DBSCAN)
       NOTA: usaremos los breakouts rooms. El tutor/a tendr√° el rol de facilitador/a.
                 ‚òï
               Break
               ¬°10 minutos y 
                volvemos!
    Reglas de asociaci√≥n
      ¬øDe qu√© se trata?
       ‚úì Se refiere a la identificaci√≥n de los 
         objetos que se encuentran juntos 
         en un evento o registro dado. Ej. 
         Una transacci√≥n. 
       ‚úì Tambi√©n se conoce como an√°lisis de 
         canasta de mercado, por su aplicaci√≥n 
         en  el  an√°lisis  de  patrones  en  las 
         compras de los supermercados. 
     Para recordar
   Esos patrones pueden ayudar a tomar decisiones tales como qu√© 
   cupones distribuir, cu√°ndo poner un producto a la venta, o c√≥mo 
   colocar los art√≠culos en las g√≥ndolas, etc. 
    Las interpretaciones de las reglas se expresan como ‚ÄúSi el art√≠culo A 
   es parte de una transacci√≥n, entonces el art√≠culo B es tambi√©n parte 
   de la transacci√≥n X‚Äù.
   Las reglas no deben ser interpretadas como una relaci√≥n causal, sino 
   como una asociaci√≥n entre dos o m√°s elementos.
     Veamos algunos 
      ‚úì Si un hombre compra zapatos, el 
     ejemplos...
        10% de las veces tambi√©n compra 
        medias.
      ‚úì Clientes que adquieren un producto 
        l√°cteo  tienden  a  comprar  un 
        producto de panificados.
      ‚úì El 75% de los clientes que compra 
        fideos  y  alg√∫n  tipo  de  salsa, 
        tambi√©n compra queso rallado.
         ¬øC√≥mo funcionan?
         ‚úì Una  regla  de  asociaci√≥n  tiene  un 
            antecedente  (lado  izquierdo)  y  un 
                                                 Ejemplo: {fideos, salsa} ‚Üí {queso rallado} 
            consecuente  (lado  derecho).  Ambos 
            lados  de  la  regla  son  un  conjunto  de 
            elementos.                                   Antecedente          Consecuente
         ‚úì Si  el  conjunto  de  elementos  X  es  el 
            antecedente y conjunto de elementos Y     Itemset: fideos, salsa, queso rallado
            es el consecuente, entonces la regla de 
            asociaci√≥n se escribe como:
                             X ‚Üí Y
                         Ventajas vs. Desventajas
      ‚úî El concepto del algoritmo de Reglas de           ÌøÅÌ†º Se generan muchas reglas con un 
      Asociaci√≥n es realmente muy sencillo.              peque√±o n√∫mero de elementos.
      ‚úîSu implementaci√≥n no requiere gran                ÌøÅÌ†º  Las reglas pueden ser c√≠clicas, es decir, 
      complejidad y suele funcionar bien, es 
      decir, presenta una buena performance.             (A, B) ‚Üí C, (A, C) ‚Üí B y (B, C) ‚Üí A. 
                                                         ÌøÅÌ†º Se necesita filtrar las reglas si 
                                                         determinados art√≠culos son buscados como 
                                                         consecuente.
     Casos de estudio
         Caso I: E-commerce
        ‚úì Cuando      las   personas    compran       ‚úì En  general  los  clientes  tienen  a 
           productos  a  trav√©s  de  nuestro  sitio       comprar productos similares a los que 
           tenemos  la  informaci√≥n  de  cada             adquieren, a esto se le llama sistemas 
           transacci√≥n. ÌøÅÌ†º                                de recomendaci√≥n. ÌøÅÌ†º
        ‚úì Podemos      usar   esta   informaci√≥n      ‚úì Estos sistema generan mayor eficiencia 
           eficientemente    para    incrementar          en las ventas. ÌøÅÌ†º 
           nuestras ventas. ÌøÅÌ†º
                                                      ‚úì Esta  metodolog√≠a  se  conoce  como 
        ‚úì Podr√≠amos investigar que productos son          ‚ÄòAssociation  rule  Mining‚Äô  o  reglas  de 
           los m√°s vendidos. ÌøÅÌ†º                           asociaci√≥n. ÌøÅÌ†º
        ‚úì Se  pueden  lanzar  anuncios        de 
           recomendaci√≥n  cuando  un  usuario         ‚úì En Marketing se conoce como Market 
           compre alg√∫n producto. ÌøÅÌ†º                      Basket Analysis. ÌøÅÌ†º
          Caso II: Tienda de libros
           ‚úì Si  quisi√©ramos  saber  qu√©  tipos  de  g√©neros 
              deben colocarse uno al lado de los de negocios 
              para    lograr   vender    m√°s?     (Econom√≠a, 
              Gastronom√≠a, Viajes, etc)
           ‚úì Si tenemos informaci√≥n hist√≥rica de compras 
              podemos ver en las transacciones que libros 
              compran  junto  a  los  de  negocios  y  as√≠ 
              establecer la estrategia cross-selling
           ‚úì Antes de cualquier conclusi√≥n se debe analizar 
              cuidadosamente      antes   de    implementar 
              medidas
    Reglas de asociaci√≥n 
     vs Miner√≠a para 
    reglas de asociaci√≥n
     Comparaci√≥n
      ‚óè Una  regla  de  asociaci√≥n  simplemente  es  una 
        regla  que  describe  qu√©  productos  de  nuestra 
        tienda ser√°n comprados al tiempo
      ‚óè En  cambio,  la  miner√≠a  para  reglas  de 
        asociaci√≥n  es  una  metodolog√≠a  que  trata  de 
        descubrir las reglas en los datos.
     Reducci√≥n de la 
     dimensionalidad
      Reducci√≥n de la 
       Buscamos reducir la cantidad de features de un dataset, pero reteniendo la mayor 
       cantidad de ‚Äúinformaci√≥n‚Äù posible. 
      dimensionalidad
       Tenemos dos aplicaciones principales con esta t√©cnica
                      1- Eliminar variables                  2- Encontrar grupos
        Caso 1: Eliminar 
        variables
                     Original:                          Reducci√≥n de 1 dimensi√≥n: 
        3 variables para predecir ‚ÄúIngreso‚Äù                     2 variables
     Caso 2: Transformaci√≥n 
     matem√°tica
                        Ejemplo: Proyectar la tierra (esfera en 3D) en 
                        un plano (2D).
                        Si bien por un lado ganamos una mejor 
                        visualizaci√≥n y entendimiento, inevitablemente 
                        vamos a perder informaci√≥n (por deformaci√≥n 
                        del mapa)
                                 En los m√©todos de Reducci√≥n de la 
                                 Dimensionalidad, siempre vamos a 
                                    perder informaci√≥n 
                                 ¬°El objetivo es perder lo menos 
                                       posible! 
        ¬øPara qu√© lo aplicar√≠amos?
          ‚úì Para enfrentar ‚ÄúLa Maldici√≥n de la        ‚úì Compresi√≥n de archivos.
             Dimensionalidad‚Äù es decir, tenemos 
             tantos features que termina siendo algo  ‚úì Detectar features relevantes en 
             negativo para nuestro modelo de ML.         datasets o variables altamente 
                                                         correlacionadas.
          ‚úì Reducir el input en un modelo de 
             regresi√≥n o clasificaci√≥n.
          ‚úì Visualizar mucho mejor nuestros datos. 
     Algoritmos de aplicaci√≥n
     Algunos de los m√°s populares son:
                        ‚úì PCA: Principal Component Analysis.
                        ‚úì Auto-Encoders con Redes Neuronales. 
                        ‚úì MDS: Multidimensional scaling.
                        ‚úì UMAP, entre otros.
    Principal Component 
        Analysis
        ¬øQu√© hace el PCA? 
         ‚úì El m√©todo gira los datos de forma que, 
             desde un punto de vista estad√≠stico, no 
             exista   una  correlaci√≥n  entre  las 
             caracter√≠sticas   rotadas     pero    que 
             conserven  la  mayor  cantidad 
             posible de la varianza de los datos 
             originales. 
         ‚úì Es  decir,       el   PCA  reduce  la 
             dimensionalidad¬†de  un  conjunto  de 
             datos        proyect√°ndose          sobre 
             un¬†subespacio           de         menor 
             dimensionalidad.
       ¬øQu√© hace el                                                              Reducci√≥n
          Por   ejemplo,  datos  con¬†dos  caracter√≠sticas 
       PCA? 
          (dispuestos en un plano) pueden ser proyectados 
          sobre una √∫nica l√≠nea. 
          Por  otro  lado  un  conjunto  de  datos  de  tres 
          caracter√≠sticas  (dispuestos  en  un  espacio  de  tres 
          dimensiones)  pueden  ser  proyectados  en  un  plano 
          (de    dos    dimensiones).     Incluso     los   datos       3 features         2 features
          resultantes en el plano podr√≠an ser reducidos a 
          una √∫nica l√≠nea es decir pasar de 3 dimensiones 
          a 1. Ì∏éÌ†Ω
    USArrests = sm.datasets.get_rdataset("USArrests", "datasets")
    datos = USArrests.data
    # Entrenamiento modelo PCA con escalado de los datos
    pca_pipe = make_pipeline(StandardScaler(), PCA())                               Ejemplo
    pca_pipe.fit(datos)
    # Se extrae el modelo entrenado del pipeline
    modelo_pca = pca_pipe.named_steps['pca']
    import seaborn as sns;sns.set_style("whitegrid")
    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))
    ax.bar(x= np.arange(modelo_pca.n_components_) + 1,height = 
    modelo_pca.explained_variance_ratio_)
    for x, y in zip(np.arange(len(datos.columns)) + 1, 
    modelo_pca.explained_variance_ratio_):
       label = round(y, 2);ax.annotate(label,(x,y),textcoords="offset 
    points",xytext=(0,10),ha='center')
    ax.set_xticks(np.arange(modelo_pca.n_components_) + 1);ax.set_ylim(0, 
    1.1)
    ax.set_title('Porcentaje de varianza explicada por cada componente')
    ax.set_xlabel('Componente principal')
    ax.set_ylabel('Por. varianza explicada');
     Ejemplo
    # Proyecci√≥n de las observaciones de entrenamiento
    proyecciones = pca_pipe.transform(X=datos)
    proyecciones = pd.DataFrame(proyecciones,columns = ['PC1', 'PC2', 'PC3', 'PC4'],index= datos.index)
    proyecciones = np.dot(modelo_pca.components_, scale(datos).T)
    proyecciones = pd.DataFrame(proyecciones, index = ['PC1', 'PC2', 'PC3', 'PC4'])
    proyecciones = proyecciones.transpose().set_index(datos.index)
    plt.figure(figsize=(15,6))
    proyecciones['val']=proyecciones.index
    ax = proyecciones.set_index('PC1')['PC2'].plot(style='o')
    def label_point(x, y, val, ax):
     a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)
     for i, point in a.iterrows():
       ax.text(point['x'], point['y'], str(point['val']))
    label_point(proyecciones.PC1, proyecciones.PC2, proyecciones.val, ax)
    plt.axvline(x=0,color='black');plt.axhline(y=0,color='black')
    plt.title('PC1 vs PC2 estados EU');plt.xlabel('PC1',color='k')
    plt.ylabel('PC2',color='black')
  Ejemplo
        CLASE N¬∞15
        Glosario
        Aprendizaje No Supervisado:                      One Hot Encoding: t√©cnica de discretizaci√≥n 
        subcategor√≠a del aprendizaje autom√°tico y la  de variables categ√≥ricas, bastante √∫til cuando 
        inteligencia artificial que cuenta con NO        se tienen pocas catego≈ïias, se le conoce 
        datos etiquetados para encontrar patrones        tambi√©n como crear variables dummy
        ocultos en los datos                             Label Encoder: t√©cnica de discretizaci√≥n de 
        Problemas de clustering: son aquellos            variables categ√≥ricas, bastante √∫til cuando se 
        donde se busca encontrar grupos similares        tienen muchas categor√≠as. 
        minimizando la varianza inter cluster y          Tipos de algoritmos de clustering: existen 
        maximizando la varianza entre cluster            varias opciones (utilizando particiones, 
        Problema de reducci√≥n de                         jerarqu√≠as, densidad, mapas o modelos) pero las 
        dimensionalidad: son aquellos donde se           t√©cnicas m√°s comunes con K-means 
        busca encontrar proyecciones de las              (particiones), clustering aglomerativo 
        variables originales para entender mejor las     (jer√°rquico) y DBSCAN (densidad).
        asociaciones entre individuos y variables        Reglas de asociaci√≥n: entendimiento de 
                                                         antecedentes y consecuentes analizados por 
                                                         medio de relaciones causales
     ¬øA√∫n quieres conocer 
          m√°s?
     Te recomendamos el 
      siguiente material
         MATERIAL AMPLIADO
     Recursos multimedia
     T√≠tulo
      ‚úì Reducci√≥n de la dimensionalidad. Aprend√© IA  | aprendeia.com 
      ‚úì Reducci√≥n de la dimensionalidad | interactivechaos.com 
      ‚úì La maldici√≥n de la dimensi√≥n en Machine Learning | iartificial.net 
      Disponible en nuestro repositorio.
      ¬øPreguntas?
                         Resumen 
                   de la clase hoy
                  ‚úì Aprendizaje No Supervisado
                  ‚úì Clustering
                  ‚úì Reglas de Asociaci√≥n
                  ‚úì Reducci√≥n de la Dimensionalidad
                  ‚úì PCA
      Opina y valora 
       esta clase
        Muchas 
        gracias.

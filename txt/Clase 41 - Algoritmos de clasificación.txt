    Esta clase va a ser
        grabad
          a
              Clase 41. DATA SCIENCE
             Algoritmos de 
              clasificaci√≥n
       Temario
                        40                       41                       42
                  Introducci√≥n            Algoritmos de              Algoritmos de 
                    a Machine              clasificaci√≥n             clasificaci√≥n y 
                     Learning                                          Regresi√≥n
                ‚úì Marco CRISP-DM y la 
                    fase de ML y                                    ‚úì SVM
                    modelado             ‚úì KNN
                ‚úì Scikit-Learn                                      ‚úì Ejemplos de 
                                         ‚úì Random Forest               clasificaci√≥n err√≥nea
                ‚úì T√©cnicas de Encoding                              ‚úì Regresi√≥n lineal 
                                         ‚úì Regresi√≥n Log√≠stica         simple y m√∫ltiple
                ‚úì Feature Engineering
                                                                    ‚úì Optimizaci√≥n de 
                ‚úì Flujo de trabajo                                     hiperparametros
    Objetivos de la clase
                 Profundizar en el Aprendizaje Supervisado 
                 Identificar el funcionamiento de los modelos 
                 de clasificaci√≥n
         MAPA DE CONCEPTOS
                                    Regresi√≥n 
                                    Log√≠stica
            KNN                 Algoritmos de               Random Forest
                                clasificaci√≥n
            Repaso
                     Les proponemos tomarse unos minutos 
                     para realizar un repaso de los conceptos 
                      aprendidos en Kahoot, ¬øest√°n listos?
                          Profe, puedes compartir el 
                           PIN o link de acceso al 
                                juego
     K-nearest-neighbor:
          KNN
   KNN (vecinos cercanos)
   Puede usarse para clasificar nuevas 
    muestras (valores discretos)¬†o para 
      predecir (regresi√≥n, valores 
          continuos). 
   Sirve esencialmente para clasificar 
   valores, buscando los puntos de datos 
     ‚Äúm√°s similares‚Äù (por cercan√≠a).
     KNN (vecinos cercanos)
      Entonces, supongamos el siguiente 
      escenario: 
                                                2
      Tenemos un Dataset con 2 Features, en      
                                                e
                                                r
                                                u
      el cual cada instancia puede pertenecer   t
                                                a
                                                e
                                                F
      a una de dos clases: ‚ÄúRojo‚Äù o ‚ÄúAzul‚Äù.
                                                      Feature 1
       Para pensar
   Dada una nueva instancia, de la cual no 
   sabemos cu√°l es su clase, vamos a recurrir a sus 
   vecinos cercanos para clasificar ¬øLa clasificamos 
   como rojo o azul?
   Contesta la encuesta de Zoom 
     KNN (vecinos cercanos)
      Si tomamos K=1, solo miraremos al 
      vecino m√°s cercano.                              k = 
                                                       1    ?
                                            2
      Aclaraci√≥n: K es el nro de vecinos     
                                            e
                                            r
                                            u
                                            t
                                            a
                                            e
                                            F
                  Azul
                                                  Feature 1
     KNN (vecinos cercanos)
      Si elegimos otro valor de k, por ejemplo 
      k > 1, nuestra clasificaci√≥n cambiar√°                k = 
                                                           3     ?
      significativamente.
                                               2
                                                
                                               e
                                               r
      Por ejemplo, con k = 3 tenemos dos       u
                                               t
                                               a
      vecinos Rojos y uno Azul.                e
                                               F
      Por lo tanto en base a este escenario, 
      la clasificaci√≥n ser√°: Rojo.                    Feature 1
       Para pensar
   ¬øConoces el algoritmo de Random Forest?
   ¬øAlguna vez lo han utilizado?
   ¬øC√≥mo lo explicar√≠as a una persona que no lo 
   conoce?
   Contesta en el chat de Zoom 
     Random Forest
       Definici√≥n
   Random Forest 
     Random Forest es un tipo de Ensamble en Machine Learning en 
       donde combinaremos diversos √°rboles de decisi√≥n. Pero 
      entonces, ¬øQu√© son los m√©todos de Ensamble en ML? 
     ¬øC√≥mo funciona?
   M√©todos de Ensamble en ML
      Tambi√©n llamados¬†m√©todos¬†combinados, intentan ayudar a 
      mejorar el rendimiento de los modelos de Machine Learning. 
        Este es un proceso mediante el cual se construyen 
      estrat√©gicamente varios modelos de ML para resolver un 
                problema particular.
       PARA RECORDAR
    Importancia de Random 
    Forest
    Por otro lado, resulta importante mencionar, 
    que Random Forest, al igual que el √Årbol de 
    decisi√≥n, son modelos de aprendizaje 
    supervisado com√∫nmente utilizados en 
    problemas de clasificaci√≥n (aunque tambi√©n 
    puede usarse para problemas de regresi√≥n)
   ¬øC√≥mo surge?
     Uno de los problemas que aparec√≠a con la 
    creaci√≥n de un √°rbol de decisi√≥n, es que si le 
     damos la profundidad suficiente, el √°rbol 
     tiende a ‚Äúmemorizar‚Äù las soluciones en 
    vez de generalizar el aprendizaje. Es decir, 
     a padecer de overfitting. La soluci√≥n para 
    evitar esto es la de crear muchos √°rboles y 
        que trabajen en conjunto. 
     Miremos un ejemplo de c√≥mo funciona‚Ä¶
      Ejemplo aplicado
                                    Paso 1: Creaci√≥n de un boostrapped 
       Tenemos estos datos          dataset (Muestra de los datos) con mismo 
                                    tama√±o del original
    Ejemplo aplicado
                      Paso 2: Creamos un √°rbol de decisi√≥n con el 
                      Bootstrapped Dataset pero solo seleccionando 
                      algunas columnas al azar en cada paso
                      Asumiremos 2 variables (columnas) en cada paso
    Ejemplo aplicado
    Paso 3: Volvemos al paso 1 y repetimos el proceso creando nuevos bootstrapped 
    datasets y creando nuevos √°rboles de decisi√≥n (se puede hacer 100 veces, en este 
    caso solo puse 6)
      Ejemplo aplicado
       Imaginemos que tenemos un            Paso 4: Utilizar el modelo en los diferentes 
       nuevo paciente y queremos            √°rboles de decisi√≥n
       saber si padecera ataque 
       cardiaco 
    Ejemplo aplicado
    Para el segundo √°rbol la decisi√≥n es Yes
    Para el tercer √°rbol la decisi√≥n es Yes
    Ejemplo aplicado
    Para el cuarto √°rbol la decisi√≥n es Yes
    Para el quinto √°rbol la decisi√≥n es Yes
     Ejemplo aplicado
        Tenemos como resultado
                                       Al final se busca la mayor 
                                       votaci√≥n y a partir de esto se 
                                       toma la decisi√≥n de la 
                                       clasificaci√≥n final
                                       Bootstrapping los datos y 
                                       agregarlos para tomar 
                                       decisiones se conoce como 
                                       Bagging
       Para pensar
   ¬øConoces el algoritmo de Regresi√≥n Log√≠stica?
   ¬øAlguna vez lo han utilizado?
   Contesta en el chat de Zoom 
    Regresi√≥n Log√≠stica
       Definici√≥n
   Regresi√≥n Log√≠stica
    Como vimos anteriormente, se trata 
    de una t√©cnica de aprendizaje 
   autom√°tico que proviene del campo 
     de la estad√≠stica. A pesar de su 
   nombre, no es un algoritmo, sino que 
    es un m√©todo para problemas de 
     clasificaci√≥n, en los que se 
   obtienen un valor binario entre 0 
            y 1.
       Ejemplos
   Regresi√≥n Log√≠stica: 
       Un problema de clasificaci√≥n es 
   Fraudes
    identificar si una operaci√≥n dada es 
     fraudulenta o no, asoci√°ndolo una 
     etiqueta ‚Äúfraude‚Äù a unos registros y ‚Äúno 
          fraude‚Äù a otros. Ì±åÌ†Ω
       Entonces, la Regresi√≥n Log√≠stica 
     describe y estima la relaci√≥n entre una 
      variable binaria dependiente y las 
        variables independientes.
   Regresi√≥n Log√≠stica: 
     Si la curva va a infinito positivo la 
   Fraudes
     predicci√≥n se convertir√° en 1, y si la 
     curva pasa el infinito negativo, la 
      predicci√≥n se convertir√° en 0.
    Si la salida de la funci√≥n Sigmoide es 
    mayor que 0.5, podemos clasificar el 
    resultado como 1 o SI, y si es menor 
    que 0.5 podemos clasificarlo como 0 o 
             NO. 
   Regresi√≥n Log√≠stica: 
   Fraudes
   Por su parte si el resultado es 0.75, 
   podemos decir en t√©rminos de 
   probabilidad como, hay un 75% de 
   probabilidades de que nuestro producto, 
   por ejemplo en este caso, tenga √©xito en 
   el mercado.
    Regresi√≥n Log√≠stica: Fraude
      Fraude
                             Fraude
     No Fraude              No Fraude
      En este caso una l√≠nea recta no es apropiada, por ende se habla de una 
      funci√≥n que se ajuste a los datos y podemos usar la funci√≥n Log√≠stica
    Regresi√≥n Log√≠stica: Fraude
       Fraude                 Fraude
     No Fraude               No Fraude
    Las X en rojo representan nuevos individuos. Como se puede observar a 
    izquierda al entrar a la curva se tiene una alta probabilidad de cometer 
    fraude. Sin embargo como se observa a la izquierda podemos tener diferentes 
    casos de an√°lisis. 
                 ‚òï
               Break
             ¬°10 minutos y 
             volvemos!
                  ¬°Lanzamos la
                  Bolsa de 
                  Empleos!
                 Un espacio para seguir potenciando tu carrera y 
                 que tengas m√°s oportunidades de inserci√≥n 
                 laboral.
                 Podr√°s encontrar la Bolsa de Empleos en el men√∫ 
                 izquierdo de la plataforma.
                 Te invitamos a conocerla y ¬°postularte a tu futuro 
                 trabajo!
                   Con√≥cela
      Ejemplo en vivo
   Analizaremos el dataset de titanic con los 
   datos train_titanic.csv y test_titanic.csv 
   dentro de la carpeta de clase. Revisaremos 
   c√≥mo el uso del Feature Engineering puede 
   ayudar a mejorar los resultados de un 
   modelo Random Forest
   Elaborando un algoritmo de 
            clasificaci√≥n
     Aplicaremos Feature Engineering y generaremos un 
            modelos de clasificaci√≥n
             Duraci√≥n: 15-20 mins
       ACTIVIDAD EN CLASE
    Elaborando un 
    algoritmo de 
    En esta oportunidad nos reuniremos en grupos de 
    clasificaci√≥n
    m√°ximo 4 personas.
     1. Elegir 4 variables independientes que consideren 
       √∫tiles para predecir el ‚Äúchurn‚Äù o ‚Äúfuga/baja‚Äù de 
       clientes
     2. Realizar el ‚Äúencoding‚Äù de las variables 
       independientes (una persona hace el c√≥digo y 
       comparte, los dem√°s ayudan dando instrucciones, 
       etc) para generar matriz para el modelo
     3. Elegimos uno de los modelos aprendidos en clase 
       (e.g KNN, Random Forest, Reg. Log√≠stica o √°rboles 
       de decisi√≥n) y entrenan un modelo
     4. Crear matriz de confusi√≥n para evaluar 
       performance
      ¬øPreguntas?
       CLASE N¬∞41
       Glosario
       KNN: algoritmo de aprendizaje               Bagging: Boostrapping aggregating por 
       supervisado que permite resolver            sus siglas en ingl√©s, es un mecanismo que 
       problemas de clasificaci√≥n bas√°ndose en     permite reducir el overfit en los modelos 
       distancias o m√©tricas                       basados en √°rboles de decisi√≥n y se 
                                                   fundamenta en que la mayor√≠a hace la 
       Random Forest: algoritmo de                 fuerza 
       aprendizaje supervisado que permite         Regresi√≥n Log√≠stica: algoritmo basado en 
       resolver problemas de clasificaci√≥n         la funci√≥n sigmoide, meramente estad√≠stico 
       utilizando el mecanismo bagging por         que permite resolver problemas de 
       medio del uso de muchos √°rboles de          clasificaci√≥n en el contexto de aprendizaje 
       decisi√≥n a la hora de discernir la          supervisado. Ideal para clasificaci√≥n 
       predicci√≥n final                            binaria.
      Opina y valora 
       esta clase
                        Resumen 
                   de la clase hoy
                  ‚úì KNN
                  ‚úì Random Forest
                  ‚úì Regresion Logistica
        Muchas 
        gracias.
